---
layout: blog
title: "Exterior Algebra Notes: the Interior Product"
footnotes: true
math: true
aside: true
tags: notes
---

*Previously: [matrices]({{ site.baseurl }}{% post_url 2018-10-08-exterior-1 %}) and [inner products]({{ site.baseurl }}{% post_url 2018-10-09-exterior-2 %}) on exterior algebras.*

*Vector spaces are assumed to be finite-dimensional and over $$\bb{R}$$. The grade of a multivector $$\alpha$$ will be written $$\| \alpha \|$$, while its magnitude will be written $$\Vert \alpha \Vert$$. Bold letters like $$\b{u}$$ will refer to (grade-1) vectors, while Greek letters like $$\alpha$$ refer to arbitrary multivectors with grade $$\| \alpha \|$$.*

*For reasons which will be explained a few posts in the future, I use the symbol $$\v$$ for the exterior product instead of the more common $$\^$$.*


------

Previously we looked at how the inner product on an exterior power of a vector space $$\v^k V$$ is actually exactly what you'd expect (unit multivectors like $$\b{x \v y}$$ and $$\b{y \v z}$$ are orthonormal), and gives rise to some easy derivations of well-known vector identities. The inner product of two $$k$$-vectors was computed by summing over the orthonormal basis vectors, like so:

$$\< \b{u} , \b{v} \> \equiv \sum_{\v I \in \v^k V} u^{\v I} v_{\v I}$$

In this article we cover the next common operations of exterior algebra: a far-reaching generalization of the inner product called the **interior product**.

(Disclaimer: this article used to include the Hodge Star also, before I split them up. Some people would object to treating the interior product before the star. I'd object to doing the star first. Whatever; both require discussion.)

<!--more-->

----------

## 1. Intuition for the Interior Product

Personally I find the concept of multivectors and their inner product to be conceptually simple. It's intuitive to me that some type of mathematical object might represent oriented areas, and that it's a vector in a space and has a dot-product-like-operation like any Euclidean vector.

The interior product is not nearly as satisfying. It's used in any treatment of the subject, but it's hard to figure out exactly what it... is. Partly because nothing like it seems to exist in regular vector algebra. In fact, something does: it turns out that the interior product is related to _projection_. 

Recall that, for regular vectors, $$\b{a} \cdot \b{b}$$ is non-zero if they have some overlap; in Euclidean space the result is $$\Vert \b{a} \Vert \Vert \b{b} \Vert \cos \theta_{ab}$$, where $$\theta_{ab}$$ is the angle between them. Likewise, for multivectors of the same grade, $$\alpha \cdot \beta$$ is non-zero if they have some overlapping component. 

The interior product extends this to allow the right argument to have a higher grade than the left, like $$\b{a} \cdot (\b{b} \v \b{c})$$. Now, instead of the result being a _scalar_ representing the overlap, the result is a _vector_. Likewise, for $$\alpha \cdot \beta$$ when $$\| \beta \| > \| \alpha \|$$, the result is a $$(\| \beta \| - \| \alpha \|)$$-vector.

The meaning of this vector is best illustrated by an example. Suppose we have vectors $$\b{a,b,c}$$ where $$\b{a}$$ and $$\b{b}$$ have some overlap:

$$\b{a} \cdot \b{b} = \b{a} \cdot (\b{b}_a + \b{b}_{\perp a}) = \b{a} \cdot \b{b}_a = ab_a$$

And similarly with $$\b{a} \cdot \b{c} = a c_a $$. Then it turns out that:

$$\begin{aligned}
\b{a} \cdot (\b{b} \v \b{c}) &=  (\b{a \cdot b}) \b{c} - (\b{a \cdot c}) \b{b} = ab_a \b{c} - a c_a \b{b}
\end{aligned}$$

The result is clearly orthogonal to $$\b{a}$$: 

$$\b{a} \cdot (ab_a \b{c} - a c_a \b{b}) = a^2 (b_a c_a - c_a b_a) = 0$$

So by taking the interior product of $$\b{a}$$ -- called 'contracting with $$\b{a}$$', we remove all $$\b{a}$$-components from $$\b{b} \v \b{c}$$.

Meanwhile, $$\b{a} \v (\b{b \v c})$$ must be nonzero. What is it?

$$\begin{aligned}
\b{a} \v (ab_a \b{c} - a c_a \b{b}) &= a b_a \b{a} \v \b{c}_{\perp a} - a c_a \b{a} \v \b{b}_{\perp a} \\
&= a^2 (\b{b}_a \v \b{c}_{\perp a} + \b{b}_{\perp a} \v \b{c}_a)
\end{aligned}$$

This isn't equal to $$\b{b} \v \b{c}$$ -- but it's part of it. We can write (noting that the $$\b{b}_a \v \b{c}_a$$ is 0):

$$\begin{aligned}
\b{b} \v \b{c} &= \boxed{ \b{b}_a \v \b{c}_{\perp a} + \b{b}_{\perp a} \v \b{c}_{a}} + \b{b}_{\perp a} \v \b{c}_{\perp a} \\
&= \b{a} \v (\b{a} \cdot (\b{b \v c})) + \b{b}_{\perp a} \v \b{c}_{\perp a}
\end{aligned}$$

So $$\b{a} \cdot (\b{b \v c})$$ can be said to produce "the component of $$\b{b \v c}$$ which contains $$\b{a}$$". There's your intuition. 

This does pick up a scalar multiple of $$\Vert \b{a} \Vert$$ from each operation, so if you want to actually _project_ $$\b{b \v c}$$ onto $$\b{a}$$, you want to use:

$$(\b{b \v c})_a = \frac{1}{\Vert \b{a} \Vert^2} \b{a} \v [\b{a} \cdot (\b{b \v c})]$$

Compare to vector projection:

$$\b{b}_a = \frac{\b{a}}{\Vert \b{a} \Vert^2} \b{a} \cdot \b{b}$$

(This is one of many examples I have come across, in order to generalize an equation to higher-dimensional objects, it seems to most appropriate to upgrade a scalar product to a wedge product. Food for thought.)

And, naturally, we can do this on any multivectors as well (the result will be 0 if $$\beta$$ does not 'fit in' $$\alpha$$, such as if $$\| \beta \| > \| \alpha \|$$):

$$\alpha_{\beta} = \frac{1}{\Vert \beta \Vert^2} \beta \v [\beta \cdot \alpha]$$

One more thing before we actually learn how these are defined. It's possible to write any $$\v^k$$ vector as an antisymmetric $$k$$-tensor; particularly, a bivector like $$\b{b \v c}$$ corresponds to an antisymmetric matrix $$\b{b} \o \b{c} - \b{c} \o \b{b}$$. Then the interior product turns out to just be multiplication by this matrix:

$$\begin{aligned}\b{a} \cdot (\b{b \v c})&\equiv 
\begin{pmatrix} 0 & b_y c_x  - b_x c_y & b_z c_x - b_x c_z  \\ 
b_x c_y - b_y c_x & 0 &  b_z c_y - b_y c_z\\
b_x c_z - b_z c_x & b_y c_z - b_z c_y & 0 \end{pmatrix} \begin{pmatrix} a_x \\ a_y \\ a_z \end{pmatrix}
\\
&= \begin{pmatrix}
(a_y b_y + a_z b_z) c_x - (a_y c_y + a_z b_z) b_x \\
(a_x b_x + a_z b_z) c_y - (a_x c_x + a_z c_z) b_y \\
(a_x b_x + a_y b_y) c_z - (a_x c_x + a_y c_y) b_z
\end{pmatrix} \\
&= (\b{a \cdot b}) \b{c} - (\b{a \cdot c}) \b{b}

\end{aligned}$$

-------------

## 2. A more complete presentation

The [Interior Product](https://en.wikipedia.org/wiki/Interior_product), also called the _contraction_ or _insertion_ operator, is the [adjoint](https://en.wikipedia.org/wiki/Hermitian_adjoint) of the wedge product with respect to the inner product:

$$\boxed{\< \gamma \v \alpha, \beta \> = \< \alpha, \gamma \cdot \beta \>} 
\tag{1}$$

In this sense it 'partially applies' the inner product. $$\gamma \cdot \beta$$ removes $$\| \gamma \|$$ dimensions from $$\beta$$, leaving an object which has the same grade as $$\alpha$$. 

Note that:

* It's not associative: $$a \cdot (b \cdot c) \neq (a \cdot b) \cdot c$$. This is akin to how division or subtraction aren't associative, since $$(c/b)/a \neq c/(b/a)$$. You should think of $$\cdot$$ as a sort of division-like operation in this sense.
* It's not reflexive: $$a \cdot b \neq b \cdot a$$, and if their grades are different one of those is 'undefined'. The interior product subtracts the grade of its left argument from its right, so: $$\un{a}{\alpha} \cdot \un{b}{\beta} = \un{b-a}{\gamma}$$

Why left from right? It's basically arbitrary, and I kinda think right-from left would be more reasonable so that it looks more like division: $$a \cdot b \sim b/a$$. Some people do it both ways and call them left- and right- contraction. We could also try to do it _both_ ways, so whichever grade is lower is subtracted from whichever is higher. That... feels like a different operation to me. We'll discuss it at some point.

More notes:

* If $$\| \alpha \| = \| \beta \|$$, then $$\alpha \cdot \beta = \< \alpha, \beta\>$$. This justifies using the same dot-product notation for both.
* If the left argument is a scalar, it's just scalar multiplication: $$c \cdot \alpha = c\alpha$$.


In general, you can compute interior products via a slight generalization of the method for inner products (now the antisymmetrization _must_ be performed on the right, though):

1. Write the right side (which has the higher grade) as an antisymmetric tensor product using $$\text{Alt}$$.
2. Contract term-by-term until you run out of terms.

Kinda like this:

$$\begin{aligned}
\b{a } \cdot (\b{b \v c}) &= \b{a} \cdot \text{Alt}(\b{b \o c}) \\
&= \b{a} \cdot (\b{b \o c} - \b{c \o b}) \\
&= (\b{a \cdot b}) \b{c} - (\b{a \cdot c}) \b{b} \\
\end{aligned}$$


Here are a bunch more examples:

$$\begin{aligned}
\b{x} \cdot (\b{x \v y}) &= \b{y} \\
\b{x} \cdot (\b{y \v z}) &= 0 \\
\b{x} \cdot (\b{a \v b}) &= a_x \b{b} - b_x \b{a} \\
(\b{x \v y}) \cdot (\b{x \v y}) &= 1 \\
(\b{x \v y}) \cdot (\b{y \v x}) &= -1 \\
(\b{x \v y}) \cdot (\b{y \v z}) &= 0 \\
(\b{x \v y}) \cdot (\b{x \v y \v z}) &= \b{z} \\
(a_{xy} \b{x\v y} + a_{yz} \b{y\v z}) \cdot (b_{yz} \b{y \v z}) &= a_{yz} b_{yz} \\

\b{a} \cdot (\b{b \v c \v d}) &= (\b{a \cdot b}) (\b{c \v d}) -(\b{a \cdot c}) (\b{b \v d}) \\
&+ (\b{a \cdot d}) (\b{b \v c}) \\
(\b{a \v b}) \cdot (\b{c \v d \v e}) &= (\b{a \v b}) \cdot (\b{c \v d}) \b{e} + (\b{a \v b}) \cdot (\b{d \v e}) \b{c}  \\
&+ (\b{a \v b}) \cdot (\b{e \v c}) \b{d} \\
\end{aligned}$$

And a couple in index notation:

$$\begin{aligned}
\b{a} \cdot (\b{b \v c}) &= a_i (b_i c_j - b_j c_i) \b{x}_j \\
(\b{a \v b}) \cdot (\b{c \v d \v e}) &= a_i b_j c_{[i} d_j e_{k]} \b{x}_k
\end{aligned}$$

Note that we antisymmetrize only the right argument, which has more terms. We could certainly define things by antisymmetrizing both sides, but there would be an extract factorial factor, which I prefer to avoid (I made the same point when defining inner products):

$$\begin{aligned}
(\b{a \v b}) \cdot (\b{c \v d \v e}) &= \frac{1}{2!} a_{[i} b_{j]} c_{[i} d_j e_{k]} \b{x}_k
\end{aligned}$$

When you can do it, the easiest way to compute an interior product is to factor the left term out of the right, like this:

$$\alpha \cdot (\alpha \v \beta) = \Vert \alpha \Vert^2 \beta$$

If you can't find a full copy of $$\alpha$$ inside the right argument, the result is 0 anyway.

--------

### Note on Names and Definitions

The interior product, which I pronounce "dot", is so named for symmetry with the 'exterior' product, which I pronounce and write "wedge" because it's easier. I've defined it in terms of the inner product, but it would have totally been possible to define the interior product first and then just define the inner product as the case where both arguments have the same grade. This way just seemed easier.

On Wikipedia the product is written as $$\iota_{\b{x}}$$ (that's an 'iota'), and some other sources use the symbols $$\llcorner$$ or $$\lrcorner$$. I prefer to just use a dot product because things are a lot more compact that way. This does require some caution, though, because we might be used to dot products on vectors being reflexive ($$\alpha \cdot \beta \stackrel{??}{=} \beta \cdot \alpha$$), but that's not defined here except when both arguments have the same grade.

The reason it's sometimes called _contraction_ is because it is contraction -- it's the contraction operation from the tensor algebra. On a tensor like $$T \in A \o A^* \o B$$, the contraction operation folds a pair of vector space and its dual into a scalar, which generalizes the matrix trace. This looks like $$T = T_{ik}^j a^i \o a_j \o a^k \ra \sum_i T_{ik}^i a^k \in B$$. In indexes, a wedge product looks like an antisymmetrization, such as $$F_{ij} = \p_i A_j - \p_j A_i$$, and the contraction looks like it does on any other tensor: $$x \cdot F = x^i \p_i A_j - x^i \p_j A_i$$.

The reason it's sometimes called _insertion_ is from differential forms, which are the most well-known application of exterior algebra in math.[^forms] Differential forms are things like $$dx$$ and $$dx \v dy$$, which are elements of "the exterior algebra on the cotangent space of a differential manifold". They are multilinear functions from tangent vectors to scalars: $$(dx \v dy)(\b{a}, \b{b}) = dx(\b{a}) dy(\b{b}) - dy(\b{a}) dx(\b{b})$$. 'Insertion' refers to inserting one argument into one of these multi-argument functions: $$\b{a} \cdot (dx \v dy) = dx(\b{a}) dy - dy(\b{a}) dx$$. Incidentally, this is exactly the operation of [currying](https://en.wikipedia.org/wiki/Currying) in computer science, and I find it useful to think of interior products as curried inner products.

[^forms]: One of the reasons I'm motivated to exposit about exterior algebra without reference to differential forms is a lingering bitterness over having to learn both at once in college. Differential forms are an application of exterior algebra, but there are others, and everything works without thinking of anything as a derivative.

A lot of sources insist that the interior product have for its left argument an element of $$\v(V^*)$$, the "exterior algebra over the dual space of $$V$$", writing $$\b{x}^* \cdot \b{x} = 1$$ or $$\iota_{\b{x}^*}\b{x} = 1$$. As long as we're in a Euclidean space with $$\b{x}_i \cdot \b{x}_j = 1_{i=j}$$, this distinction is verbose, unnecessary, and distracting, so I'm not doing that. It'll probably matter later.


-----------

### Properties of the Interior Product

Because the interior product is translatable via adjointness into a wedge product, it is independent of choice of coordinate system (for free!), and it obeys very similar properties. Let $$\iota_{\alpha}(\beta) \equiv \alpha \cdot \beta$$ and $$L_{\alpha}(\beta) \equiv \alpha \v \beta$$ (this is the only time I like to use the $$\iota$$ notation). Then:[^interiorsource]

[^interiorsource]: A helpful source for this section: "Linear Algebra via Exterior Products" by Sergei Winitzki.


$$\begin{aligned}
L_{\alpha} &: \v^k V \ra \v^{k + |\alpha|} V \\
L_{c} \alpha &= c \alpha \\
L_{\b{u}} \circ L_{\b{v}} &= - L_{\b{v}} \circ L_{\b{u}} \\
L_{\b{u}} \circ L_{\b{u}} &= 0 \\
L_{\alpha \v \beta} = L_{\alpha} \circ L_{\beta} &= (-1)^{|\alpha| |\beta|} L_{\beta} \circ L_{\alpha}  \\
&= (-1)^{|\alpha| |\beta|} L_{\beta \v \alpha} \\
&\\
\iota_{\alpha} &: \v^k V \ra \v^{k - |\alpha|} V \\
\iota_{c} \alpha &= c \alpha \\
\iota_{\b{u}} \circ \iota_{\b{v}} &= -  \iota_{\b{v}} \circ \iota_{\b{u}} \\
\iota_{\b{u}} \circ \iota_{\b{u}} &= 0 \\
\iota_{\beta \v \alpha} = \iota_{\alpha} \circ \iota_{\beta} &= (-1)^{|\alpha| |\beta|}  \iota_{\beta} \circ \iota_{\alpha} \\
&= (-1)^{|\alpha| |\beta|}  \iota_{\beta \v \alpha}
\end{aligned}$$

Clearly the two have very similar structure.  The one difference is in how they decompose into component operations -- the interior product reverses the order of its factors: $$L_{\alpha \v \beta} = L_{\alpha} L_{\beta}$$, but $$\iota_{\alpha \v \beta} = \iota_{\beta} \iota_{\alpha}$$. But they're _so_ similar that it almost seems like they are the same operation. 

The unifying idea is that the wedge product acts as a [graded derivation](https://en.wikipedia.org/wiki/Derivation_(differential_algebra)#Graded_derivations), in the language of abstract algebra, and the interior product is a graded anti-derivation. 

A derivation is a thing which obeys a product rule like that of calculus: $$d(ab) = (da) b + a (db)$$. In this case, the wedge product does the operation of 'multiplication' and the interior product is the operation of 'derivative', and 

$$\iota_{\b{c}} (\b{a} \v \b{b}) = (\b{c})$$

This means that they each obeys a sort of 'product rule':

$$\begin{aligned}
L_{a} \alpha \v \beta = 
\end{aligned}$$

$$\iota_a L_b + L_b \iota_a = a \cdot b$$

It is tempting to try to 'unify' them into one operation, by treating the interior product as a multivector of 'negative' grade.

------------

# TODO

## 1. is this true? higher-dim?

Interior products and wedge products with vectors collectively anticommute:

$$\begin{aligned}
\iota_{\b{x}} \iota_{\b{y}} &= - \iota_{\b{y}} \iota_{\b{x}}\\
L_{\b{x}} L_{\b{y}} &= - L_{\b{y}} L_{\b{x}} \\
\iota_{\b{x}} L_{\b{y}} &= -L_{\b{y}} \iota_{\b{x}} \\

\end{aligned}$$

There are a lot of times when the interior product seems to act like a wedge product with an element of grade $$-1$$. It is tempting to try to combine $$L$$ and $$\iota$$ into one operation: something like $$\b{x}^* \v \alpha \equiv \b{x} \cdot \alpha$$. But this would immediately break the associativity of $$\v$$: $$\b{x} \cdot (\b{x} \v \b{x}) = 0 \neq (\b{x} \cdot \b{x}) \v \b{x} = \b{x}$$, and I'm not sure how to go about repairing that.

## Jacobi? Relation to derivatives?

The interior product is a [graded antiderivation](https://en.wikipedia.org/wiki/Derivation_(differential_algebra)), in the language of abstract algebra, which means that it distributes over the wedge product in a way which generalizes the product-rule of calculus:

$$\b{x} \cdot (\alpha \v \beta) = (\b{x} \cdot \alpha) \v \beta + (-1)^{|\alpha|} \alpha \v (\b{x} \cdot \beta)$$

This is probably really important. I don't really know yet.

## Examples from physics and math -- nasty tensor operations?

is it ... multiplying derivatives = wedges; interior product = multiplying by x? something like that?

## Examples: some projections

---------


I think that just about covers the interior product. It still feels a bit mysterious to me, but probably not enough to justify its obscurity in geometry, since it is directly involved in some common vector identities. 

There used to be a long section on the Hodge Star / complement operation next, but I decided to split them up because this was _so long_.

{% include ea.html %}