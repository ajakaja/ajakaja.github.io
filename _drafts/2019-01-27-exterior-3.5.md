---
layout: blog
title: "EA Notes 3.5: The Hodge Star"
footnotes: true
math: true
aside: true
tags: notes
---

(This used to be part of #3 but I decided to split them up)

*Previously: [matrices]({{ site.baseurl }}{% post_url 2018-10-08-exterior-1 %}) and [inner products]({{ site.baseurl }}{% post_url 2018-10-09-exterior-2 %}) on exterior algebras.*

*Vector spaces are assumed to be finite-dimensional and over $$\bb{R}$$. The grade of a multivector $$\alpha$$ will be written $$\| \alpha \|$$, while its magnitude will be written $$\Vert \alpha \Vert$$. Bold letters like $$\b{u}$$ will refer to (grade-1) vectors, while Greek letters like $$\alpha$$ refer to arbitrary multivectors with grade $$\| \alpha \|$$.*

*For reasons which will be explained a few posts in the future, I use the symbol $$\v$$ for the exterior product instead of the more common $$\^$$.*

<!--more-->


-----------


## 1. The Hodge Star / Complement

Let $$\omega \in \v^n V$$ be a unit pseudoscalar in $$\v V$$.[^integral] For examples we'll use $$\omega = \b{x \v y \v z} \in \v^3 \bb{R}^3$$. The selection of $$\omega$$ (particularly, the order of its factors) determines a global orientation on $$V$$. Once we have an $$\omega$$, we define the [Hodge Star](https://en.wikipedia.org/wiki/Hodge_star_operator) or _Hodge Dual_ or "complement" operation as the interior product with $$\omega$$:

[^integral]: In some texts, particularly those of Gian-Carlo Rota, this is called the _Integral_. I think that's a terrible name. He also calls multivectors 'extensors'. See for instance his "On the Exterior Calculus of Invariant Theory", which is very useful except for all the bad names.

$$\star \alpha \equiv \alpha \cdot \omega\tag{2}$$


I personally prefer the name 'complement', because it turns out to be closely related to the set complement operation. We'll talk about that in the next post. In the meantime, know that everyone uses one of the Hodge names. 'Dual' is also a good name, though usually that's claimed by the vector space dual.

I don't like $$\star$$ symbol very much either; I'd prefer the overbar notation $$\bar{\alpha}$$, but I have to concede that it's often very useful to write it as a prefix operator when it has complicated arguments -- but it sure does end up involving a lot of parentheses sometimes. So I guess we're sticking with using the star notation. Some people also write $$\ast \alpha$$ or $$\vert \alpha$$ or even $$\text{\textasciitilde} \alpha$$ instead of $$\star \alpha$$.

The complement operation takes a wedge product of basis vectors to the wedge product of all the _other_ basis vectors, with an appropriate sign such that $$\alpha \v \star \alpha = \Vert \alpha \Vert^2 \omega$$. This is then extended linearly to all multivectors. As we will see, in many cases a wedge product with a vector can be translated to an interior product with its dual, or vice versa -- basically, it reverses the two operations. For a simple example, consider that a wedge product $$\b{a} \v \star \b{b}$$ in $$\bb{R}^3$$. The coefficient ends up being their inner product:

$$\begin{aligned}
\b{a} \v \star \b{b} &= (a_x \b{x} + a_y \b{y} + a_z \b{z}) \v ((b_x \b{x} + b_y \b{y} + b_z \b{z}) \cdot \b{x \v y \v z}) \\
&= (a_x \b{x} + a_y \b{y} + a_z \b{z}) \v ((b_x \b{y \v z} + b_y \b{z \v x} + b_z \b{x \v y}) \\
&= (a_x b_x + a_y b_y + a_z b_z) \b{x \v y \v z} \\
&= (\b{a} \cdot \b{b}) \omega
\end{aligned}$$

It turns out that, in general:

$$\tag{3}  \un{k}{\alpha} \v (\star \un{k}{\beta}) = \< \alpha, \beta \> \omega$$

(Sometimes this is used to define the inner product and the interior product afterwards. I'm doing everything in an unusual order.)

Here are some other examples in $$\bb{R}^3$$:

$$\begin{aligned}
\star 1 &= \b{x \v y \v z} \\
\star \b{x} &= \b{y \v z} \\
\star \b{y} &= \b{z \v x} \\
\star \b{z} &= \b{x \v y} \\
\star (\b{x \v y}) &= \b{z} \\
\star (\b{y \v x}) &= -\b{z} \\
\star(a \b{z} + b\b{x}) &= a \b{x \v y} + b\b{y \v z} \\
\star^2 &= I
\end{aligned}$$

Examples in $$\bb{R}^2$$:

$$\begin{aligned}
\star \b{x} &= \b{y} \\
\star \b{y} &= -\b{x} \\
\star^2 &= -I
\end{aligned}$$

In $$\bb{R}^n$$:

$$\begin{aligned}
\star \b{x}_i &= (-1)^{i-1} \b{x}_1 \v \ldots \cancel{\b{x}_i} \ldots \v \b{x}_n \\
\star^2 \b{x}_i &=  (-1)^{i-1} (-1)^{n-i} \b{x}_i \\
&= (-1)^{n-1} \b{x}_i
\end{aligned}$$

$$\star^2$$ can be computed by considering how many terms each element of $$\star^2 \alpha$$ must move past to be rearranged into the starting order. The general form turns out to be:

$$\boxed{\star^2 \alpha = (-1)^{|\alpha||{\star \alpha}|} \alpha}$$

Mostly I prefer to define things without minus signs everywhere, but this one is, unfortunately, unavoidable. We can also write the coefficient as $$(-1)^{\| {\star \alpha} \|} = (-1)^{n - \| \alpha \|}$$, but I prefer it with the star for lucidity.
Clearly $$\star^2 = I$$ is always true for $$n$$ odd, and for $$n$$ even is true if $$k$$ is also even. It turns out that this particular negative-sign factor $$(-1)^{\|\alpha\|\|{\star \alpha}\|} $$ comes up a lot, and we'll often just write $$\star^2 \alpha$$ to abbreviate it. We can also write down the Hodge Star's inverse:

$$\star^{-1} \alpha = (-1)^{|\alpha|| \star \alpha |} (\star \alpha) = \star^3 \alpha$$


Because $$\star$$ is the complement _with respect to $$\omega$$_, it is specifically _not_ basis-independent, but if a linear transformation $$A$$ doesn't change $$\omega$$ (ie $$\det (A) = 1$$, ie it is orthogonal) then $$\star$$ is also unchanged. We think of a choice of $$\omega$$ not as a basis-dependent object but as a geometric object, akin to saying "this is the scale I am using to measure volume", which then transforms according to whatever basis we are working in. We could incorporate the dual vector space $$V^*$$ and its exterior algebra in order to make inner products and complements invariant, but that would make the exposition much more notationally cumbersome, so I want to leave it for later.


------
### A few identities


Some other versions of (3):

$$\begin{aligned}
\un{k}{\alpha} \v (\star \un{k}{\beta}) &= \< \alpha, \beta \> \omega \\
&= \star \< \alpha, \beta \> \\
&= \star \< \beta, \alpha \> \\
&= \beta \v (\star \alpha)
\end{aligned}$$


Intuitively, these express the fact that if $$\< \alpha, \beta\> \neq 0$$, then they are 'parallel' multivectors, and thus $$\star \beta$$ and $$\alpha$$ have no overlapping components, and their wedge product matches up term-by-term. That the signs and scalar factors work out is indicative of the usefulness of the $$\star$$ operation.

Some other forms:

$$\begin{aligned}
\un{k}{\alpha} \cdot \un{k}{\beta} &= \star (\alpha \v (\star \beta)) \\
&= \star(\beta \v (\star \alpha)) \\
\star(\un{k}{\alpha} \cdot \un{k}{\beta}) &=  \alpha \v (\star \beta) \\
&= \beta \v (\star \alpha) \\
\un{k}{\gamma} \v \un{n-k}{\delta} &= \star \< \gamma, \star^{-1} \delta \> \\
&= \star^{-1} \delta \v \star \gamma
\end{aligned}$$

We can use this to show that the complement operation doesn't change the inner product of two $$k$$-vectors:

$$\begin{aligned} \< \star \un{k}{\alpha}, \star \un{k}{\beta} \>  &= \< \star \alpha, \beta \cdot \omega \> \\
&= \< \beta \v \star \alpha, \omega \> \\
&= (-1)^{|\beta| |\star \alpha|} \< \star \alpha \v \beta, \omega \> \\
&=  (-1)^{|\beta| |\star \alpha|} \< \beta, \star^2 \alpha \> \\
&= (-1)^{|\beta| |\star \alpha|} (-1)^{|\alpha| |\star \alpha|}  \< \beta, \alpha \> \\
&= (-1)^{2k |\star \alpha|} \< \beta, \alpha \>\\
&\boxed{= \< \alpha, \beta \>}
\tag{4}
\end{aligned} $$

The complement of a wedge product turns out to be quite useful (remember that $$\iota_{\alpha \v \beta} = \iota_{\beta} \circ \iota_{\alpha}$$):

$$\begin{aligned} 
\star(\alpha \v \beta) &= (\alpha \v \beta) \cdot \omega \\
&= \beta \cdot (\alpha \cdot \omega) \\
&= \boxed{\beta \cdot (\star \alpha)} \\
&= (-1)^{|\alpha| |\beta| } \alpha \cdot (\star \beta)

\tag{5}
\end{aligned}$$

The condition that $$\alpha$$ and $$\beta$$ have no overlapping components translates to the condition that $$\beta$$ is contained in $$\star \alpha$$. We can use this to come up with a general form for the complement of a dot product, which we can massage into various other identities:

$$\begin{aligned} 
\alpha \cdot \beta &= \star( (\star^{-1} \beta) \v \alpha) \\
&= (-1)^{\| \star \beta \| \| \alpha \| } {\star(\alpha \v \star^{-1} \beta)} \\
&= (-1)^{\| \star \beta \| \| \alpha \| } (\star^{-1} \beta) \cdot (\star \alpha) \\
(\star \beta) \cdot (\star \alpha) &= (-1)^{\| \star \beta \| \| \alpha \| } \alpha \cdot (\star^2 \beta)  \\
\end{aligned} $$

There are many more identities that can be made by combining $$\v$$, $$\cdot$$, and $$\star$$, but they quickly become confusing. The next article will go over them all, promise, and in a much more general way. For now, let's prove some new stuff, because that's fun.

-------

## 2. The Cross Product

In $$\bb{R}^3$$, the familiar vector cross product can be defined as:

$$\b{a \times b} = \star(\b{a \v b})$$

As we saw last time:

$$\begin{aligned}
\< \b{a \times b}, \b{c \times d} \> &= \< \star (\b{a \v b}), \star (\b{c \v d}) \> \\
&= \< \b{a \v b}, \b{c \v d} \> \\
&= (\b{a} \cdot \b{c}) (\b{b \cdot d}) - (\b{a \cdot d}) (\b{b \cdot c})
\end{aligned}$$

But now that we have better tools, we can do the more complicated ones: 

Here's the [scalar triple product](https://en.wikipedia.org/wiki/Triple_product#Scalar_triple_product), using identity (5):

$$\begin{aligned}
\b{a \cdot (b \times c)} &=  \b{a} \cdot \star (\b{b \v c}) \\
&= \star(\b{b \v c \v a}) \\
&= \star(\b{a \v b \v c})
\end{aligned}$$

The [vector triple product](https://en.wikipedia.org/wiki/Triple_product#Vector_triple_product), using (5):

$$\begin{aligned}
\b{a \times (b \times c)} &= \star(\b{a} \v \star(\b{b \v c})) \\
&= (-1)^{| \b{a}| |\star(\b{b \v c})|}  \b{a} \cdot \star^2 (\b{b \v c}) \\
&= - \b{a} \cdot (\b{b \v c}) \\
&= (\b{a} \cdot \b{c}) \b{b} - (\b{a} \cdot \b{b}) \b{c}
\end{aligned}$$

The [quadruple product](https://en.wikipedia.org/wiki/Quadruple_product), via the previous two:

$$\begin{aligned}
(\b{a \times b}) \times (\b{c \times d}) &= ((\b{a \times b}) \cdot \b{d}) \b{c} - ((\b{a \times b}) \cdot \b{c}) \b{d} \\
&= \star(\b{a \v b \v d}) \b{c} - \star(\b{a \v b \v c}) \b{d}
\end{aligned}$$

The [Jacobi Identity](https://en.wikipedia.org/wiki/Jacobi_identity):

$$\begin{aligned}
0 &= \b{a \times (b \times c)} + \b{b \times (c \times a)} + \b{c \times (a \times b)} \\

&= -{\star( \b{a} \cdot (\b{b \v c}) + \b{b} \cdot (\b{c \v a}) + \b{c} \cdot (\b{a \v b}))} \\
&= -{\star( (\b{a \cdot b} - \b{b \cdot a}) \b{c} + (\b{b \cdot c - c \cdot b}) \b{a} + (\b{c \cdot a - a \cdot c}) \b{b})} \\
&= 0
\end{aligned}$$

The Jacobi Identity can also be rearranged into the following intriguing form, which we will contend with in a future article (the second line is our previous expansion of $$\b{a} \cdot (\b{b \v c})$$. How could these be equal?):

$$\begin{aligned}
\b{a} \cdot (\b{b \v c}) &= \b{b} \cdot (\b{a \v c}) - \b{c} \cdot (\b{a \v b}) \\
 &\stackrel{??}{=} (\b{b \cdot a}) \b{c} - (\b{c \cdot a}) \b{b}
\end{aligned}$$

---------

## 3. Application: Matrix Inversion

I mentioned that you can define the complement and the inner product in terms of each other, via:

$$\un{k}{\alpha} \cdot \un{k}{\beta} = \star(\alpha \v \star \beta) = \star(\beta \v \star \alpha)$$

This just says that the wedge product between grade-$$1$$ and grade-$$(n-1)$$ vectors looks like a dot product:

$$(a_x \b{x} + a_y \b{y} + a_z \b{z}) \v (b_x \b{y \v z} + b_y \b{z \v x} + b_z \b{x \v y}) = (a_x b_x + a_y b_y + a_z b_z) \omega$$

And in fact, there's a pretty good argument that this is the more natural definition, because it's the only good way to invert matrices.

Let $$A$$ be a square invertible matrix on $$\bb{R}^n$$. We know that the wedge product of all the columns $$\bigwedge A_i = \det(A) \omega$$ is not $$0$$. For each column $$A_i$$, the wedge product of all the _other_ columns $$A^{\v n-1}_j =  (-1)^{j-1} A_1 \v A_2 \v \ldots \cancel{A_j} \ldots \v A_n$$ will have:[^sign]

$$A_i \v A^{\v n-1}_j  = \det(A) 1_{i = j} \omega $$

[^sign]: The sign here is related to the sign of $$\star \b{x}_i$$.



$$A^{\v n-1}_j$$ is an element of $$\v^{n-1} \bb{R}^n$$, but we can write the wedge product with it as a dot product (using $$\alpha \v \beta = \star^{-1} \beta \cdot (\star \alpha))$$ on scalars) to get:

$$\begin{aligned}
\star A_i \v A^{\v n-1}_j  &=  (A_i \cdot \star A^{\v n-1}_j) \\
&= A_i \cdot \star A^{\v n-1}_j \\
&= \star A^{\v n-1}_j \cdot A_i\\
&= \det(A) 1_{i = j}
\end{aligned}$$

Transposing so that the $$\cdot$$ becomes matrix multiplication (which is clunky, but we need to turn our $$i,j$$ labeled vector equation into a matrix equation somehow):

$$(\star A^{\v n-1})^T A = A (\star A^{\v n-1})^T = \det(A) I$$

We recognize $$(\star A^{\v n-1})^T$$ as the adjugate matrix of $$A$$. The inverse is, as usual:

$$A^{-1} = \frac{1}{\det(A)} (\star A^{\v n-1})^T = \frac{\text{adj}(A)}{\det(A)} $$

-------

I think that just about covers basic tools of exteior algebra.

Next time, we zoom out and try to figure out why there is so much structure here, and why it's so complicated despite that.

{% include ea.html %}
