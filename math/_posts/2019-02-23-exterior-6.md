---
layout: blog
title: "Oriented Projective Geometry"
footnotes: true
math: true
aside: true
tags: notes
---

In the previous post, I looked at how some operations related to exterior algebra (wedge / 'join', 'meet', interior product, Hodge Star, geometric product) roughly correspond to set operations (union, intersection, subtraction, complement, symmetric difference), but the analogy wasn't very good. Exterior algebra isn't _exactly_ linearized set algebra. So what is it?

I found another subject that's a better fit: [oriented projective geometry](https://en.wikipedia.org/wiki/Oriented_projective_geometry) (henceforth 'OPG').

<!--more-->

OPG is the mathematics of oriented figures in space -- 'positive' and 'negative' points, lines with directions, planes with orientations, etc, along with operations like "finding the line between two points". It turns out that every operation in OPG is exactly the same as one in exterior algebra, except that in OPG only the signs of scalar coefficients matter (everything is defined up to multiplication by a positive scalar). Put differently, OPG is exterior algebra if you forget about all the scalars.

I read through (most of) "Oriented Projective Geometry" by Jorge Stolfi (1991), which is very readable and interesting, though it could really use more concrete examples. Stolfi was at least aware of the relationship with exterior algebra also, and he cited the same Barnabei/Brini/Rota paper I've been looking at. He also had some better notations and a few ideas which I should have been aware of. It will take way too much space to fully explain projective geometry here, or to do justice to the oriented version, but I will mention the key ideas needed for the comparison to exterior algebra.


-------

## 1. The Geometry

A $$k$$-linear subspace of $$\bb{R}^n$$ is the vector space spanned by $$k$$ linearly-independent basis vectors. The origin, lines through the origin, planes through the origin, etc. Usual stuff.

A $$k$$-*affine* subspace of $$\bb{R}^n$$ is the same thing, but without the requirement that they go through the origin. So the $$0$$-affine spaces are points, the $$1$$-spaces are lines, etc. Much better for doing geometry; nobody wants all their objects to go through the origin. *Un*oriented projective geometry, the much more well-known concept, is the geometry of affine subspaces of $$\bb{R}^n$$.

There is a trick which lets us study affine subspaces using linear subspaces, which we can do linear algebra on:

Imagine, in $$\bb{R}^3$$, a plane $$P$$ at $$z=1$$. Points on this plane have coordinates like $$(x,y,1)$$. For any line in $$\bb{R}^3$$ that isn't parallel to $$P$$, we can identify a point $$P$$ by finding the intersection of the line with $$P$$, which just means dividing by the $$z$$ coordinate. Then we'll do geometry on $$P$$, a 2d plane, but using lines in $$\bb{R}^3$$ as our 'points' on $$P$$, planes in $$\bb{R}^3$$ as 'lines' on $$P$$, etc.

$$(2,3,5)$$ becomes $$(2/5, 3/5, 1)$$, which we will treat as the point $$(2/5, 3/5)$$ in $$\bb{P}^2$$, the 2d (real) projective plane. The tuple $$(2,3,5)$$ is a set of **homogenous coordinates** for the point $$(2/5,3/5)$$ in $$\bb{P}^2$$; of course they are not unique. In order for _every_ line in $$\bb{R}^3$$ to have a 2d analog, it's useful to alsp define 'points at infinity', which correspond to points like $$(2,3,0)$$ -- we can't divide by zero, so this becomes the "point at infinity" in the $$(2,3)$$ direction -- something like $$(2/0, 3/0)$$, if that was allowed.

Likewise, planes through the origin in $$\bb{R}^3$$ intersect $$P$$ in lines, unless they are parallel - in which case we define them as intersecting in the 'line at infinity'.

_Oriented_ projective geometry uses all of this, but now adds the concept of orientation to each object. We call this space $$\bb{O}^n$$[^name]: I feel like $$\bb{O}^n$$ is a better name than Stolfi's $$\b{T}_n$$.
. Its points can point into or out 'of the page'. Its lines have a direction, and planes have an orientation, defined by choosing a normal vector. Conceptually, there are two copies of $$\bb{R}^n$$ _overlaid_, one of positive points and one of negative points; for any point $$p$$, we can denote its negatively-oriented counterpart (called its **antipode**) as $$\neg p$$.

In the homogenous coordinate construction, this corresponds to dividing through only by _positive_ coefficients. $$(2,3,5)$$ is still $$(2/5, 3/5)$$, but $$(-2, -3, -5)$$ is similar to $$(-2, -3, -1)$$. It's a 'negatively oriented point', which we write as $$\neg(2,3)$$. These are the intersections of _oriented_ subspaces in $$\bb{R}^3$$ with the plane $$P$$: we now distinguish between the linear subspace defined by $$(2,3,5)$$ and that defined by $$(-2, -3, -5)$$, even though they would be the same space in regular linear algebra.

Why do any of this? Because it's used everywhere already, mainly. There's a good argument to made that this is the 'true' geometry that is useful in mathematics and physics. We almost-always care about the orientation of objects in applications -- say, when integrating over a surface.




-----


The oriented projective space $$\bb{O}^n$$ (also called a "two-sided space") is a modification of $$\bb{R}^n$$ which associates to each point a sign $$\pm$$ indicating whether it is _positively_ or _negatively_ oriented[^name]. Conceptually, there are two copies of $$\bb{R}^n$$ _overlaid_, one of positive points and one of negative points, which you can think of as "going into" or "going out of" the page. For any point $$p$$, we can denote its negatively-oriented counterpart (called its 'antipode') as $$\neg p$$.


Connecting these two spaces is a "horizon" or "hyperplane at infinity" which causes each direction to 'wrap around': $$+ \infty = \neg (- \infty)$$. It's two points at infinity in $$\bb{O}^1$$, gluing two real lines together. In $$\bb{O}^2$$, it's a circle, gluing the 'borders' of two planes together, etc. The result is that $$\bb{O}^n$$ is topologically equivalent to an $$n$$-sphere.

Examples: $$\bb{O}^0$$ is two points, with opposite orientations. $$\bb{O}^1$$ is two copies of the real line, with the positive infinities of each linked to the negative infinities of the other, resulting in a topological circle:

{% include image.html filename="2019-02-23/O1.png" width="400px" %}


$$\bb{O}^2$$ is two copies of the real plane, which we may picture as being spread over two hemispheres of a sphere, with antipodal points being oppositely oriented (hence the name 'antipode'), and the equator that joins them as the line at infinity.

This model turns out to be useful when dealing with properties of oriented geometric objects, basically because if you are considering the geometric relationships of oriented shapes, like "lines with directions" or "clockwise circles", you are already working in this space even if you don't realize it. It turns out to be a good representation for answering questions like "what side of a line is a point on?" or "is a point contained in figure?" or "what is the intersection of two planes?", and is ubiquitous (in one form or another) in computer graphics and anything else dealing with representing spatial geometry.

----------

### Homogenous Coordinates

The simplest representation of the points of $$\bb{O}^n$$ is as _directions_ in $$\bb{R}^{n+1}$$, which are called [homogenous coordinates](https://en.wikipedia.org/wiki/Homogeneous_coordinates). 

An example to show why you would want to do this: recall that we can define lines in $$\bb{R}^2$$ by an equation like $$(a,b) \cdot \b{x} = c$$. We can adjust this to look like it as a vector by writing $$(a,b,-c) \cdot (\b{x}, 1) = 0$$. Any scalar multiple of $$(\b{x}, 1)$$, where $$\b{x}$$ is on the line, will solve this equation. Since scalar multiples don't change the result, this is a space of _directions_ in $$\bb{R}^{3}$$. Likewise, any scalar multiple of $$(a,b,-c)$$, such as $$(a/c, b/c, -1)$$, will refer to the same line (as long as $$c > 0$$).

So, a point in $$\bb{O}^n$$ is defined as equivalence class of vectors in $$\bb{R}^{n+1}$$, where two vectors are equivalent if they differ by positive scalar factor[^rp]. We use _positive_ scalar multiples because, for instance, $$(a,b,c)$$ and $$(-a,-b,-c)$$ refer to different _directions_ in $$\bb{R}^{3}$$, and thus (by definition) to different points in $$\bb{O}^2$$.

These refer to the same point in $$\bb{O}^2$$, assuming all three $$(w,x,y)$$ are positive:

[^rp]: Regular projective geometry omits the requirement the factor be positive. The result is an unorientable space with unintuitive properties. Nevertheless, it is ubiquitous in higher mathematics. I suspect that was an accident, just like the ubiquity of $$\bb{C}$$.

$$(w,x,y) \sim (2w,2x,2y) \sim (w/y, x/y, 1) \sim (1, x/w, y/w) \sim (w/x, 1, y/x)$$

As long as $$w \neq 0$$, this is still the same point:

$$(w,x,y) \sim (\sgn(w), x / | w |, y / |w|)$$

These are all different points, though:

$$(w,x,y) \nsim (w,x,-y) \nsim (-w, -x, -y) \nsim (w/y, x/y, 0)$$


Clearly the relevant information to define a point in $$\bb{O}^2$$ is two real numbers $$(x,y)$$ and a 'sign flag', which can be $$\{+1, 0, -1\}$$. It's often easiest to just not divide through, when doing calculations, to avoid having to be careful about signs and absolute values. I like to write the sign flag as a $$w$$-coordinate, in the first position, so that it is the first index in any dimension.

$$(\sgn, x,y) \in \bb{O}^2$$

$$\sgn = +1$$ refers to 'positive points', $$\sgn = -1$$ to 'negative points', and $$\sgn = 0$$ to points at infinity in the direction of $$(x,y)$$. This means that $$(0, x,y) \sim (0, x/y, 1)$$ also, which shows that the hyperplane at infinity in $$\bb{O}^n$$ is itself a copy of $$\bb{O}^{n-1}$$ (in fact, any hyperplane is).

To visualize this embedding for, say, $$\bb{O}^1$$: consider the real plane $$\bb{R}^2$$ with vertical lines drawn at $$x=1$$ and $$x=-1$$, which will be the "front" and "back" parts of $$\bb{O}^1$$. Points in $$\bb{O}^1$$ correspond to "directions" in $$\bb{R}^2$$, which are determined by taking vectors $$(x,y)$$ and seeing where they intersect one of the two vertical lines. If $$y > 0$$, the intersection point is $$(1, x/y)$$, on the front line; if $$y < 0$$, the point is $$(-1, x/y)$$ on the "back line". Like this:

{% include image.html filename="2019-02-23/Graph.png" width="400px" %}

The point $$(0, x)$$ maps to $$+\infty$$, which is the same as $$\neg (-\infty)$$, if $$x > 0$$ and $$-\infty$$ if $$x < 0$$. The point $$(0,0) \in \bb{R}^2$$ doesn't correspond to anything in $$\bb{O}^1$$, and is considered undefined or null.

With a bit of work you learn to think in this model. When you study linear algebra you become used to considering every direction from the origin as a unique 'direction', and you intuit that vectors are linearly independent if they lay in the same subspace. In OPG you think of every _point_ as a unique direction (say, from your eyes), and their spans are lines/planes/higher-dimensional analogues (called 'affine subspaces') which contain them, and linear independence is when they aren't contained in the same line/plane/affine subspace.

Basically: when talking about the geometry of shapes, the origin really _isn't special_, and thus we do geometry in a way that doesn't privilege it.

------

### Flats and Simplexes

In OPG an oriented hypersurface is called a 'flat': an oriented point, line, plane, etc. For each flat $$a$$ is there is an _opposite_ flat, written $$\neg a$$, consisting of the same object with the opposite orientation. A flat's _rank_ is how many points are required to define it: a point has rank $$1$$, a line rank $$2$$, etc (it's the dimension of the figure plus one, or the dimension of the figure in the $$\bb{R}^{n+1}$$ model). We'll refer to flats of rank $$k$$ as $$k$$-flats. There are also two "empty" flats of rank $$0$$, called $$\Lambda$$ and $$\neg \Lambda$$, which are like oriented empty sets.

A _simplex_ is a list of points, written $$(abc\ldots)$$. A _proper_ simplex is one whose points are linearly independent, which means that the simplex does not contain any duplicates, copies of the same point with opposite orientations, sets of three collinear points, etc. This is necessary to determine a flat uniquely: two points to determine a line, three for a plane, etc. Linear independence rules out calling $$(a,a,b)$$ a plane or $$(p, \neg p)$$ a line. We'll almost always just say 'simplex' when we mean 'proper simplex' because we don't care about improper simplexes.

A proper $$k$$-simplex has $$k+1$$ points in it, because it determines a $$k$$-flat -- so a $$0$$-simplex is a point, a $$1$$-simplex is a line, etc. This turns out to be a lot less confusing than if a $$k$$-simplex were to have $$k$$-points in it. 

Basically, flats are oriented $$k$$-surfaces and proper simplexes are oriented $$k$$-segments (line segments, triangles, tetrahedrons, etc). Flats are usually the objects of study, but may be referred to by any simplex which defines them, and computations take place with a choice of simplex and should give the same result for any valid choice of simplex for each flat.

The $$\neg$$ symbol reverses orientations of simplexes, and acts sort of like a minus sign. In the vector representation of points, it literally is one: $$\neg(1,x,y) = (-1,-x,-y)$$. Negating a simplex negates the orientation flat it refers to. The same representation can be reached by swapping any two of its points, or negating all of them:

$$\begin{aligned}
(p,q) &= (\neg p, \neg q) \\
&= \neg (q, p) \\
&= \neg (\neg q, \neg p)
\end{aligned}$$

-------

### Operations on Flats

Flats support the operation of _join_ $$\v$$. Join just appends the simplex representations as lists, so long as none of the points lay in the same hyperplane as the others (ie, $$k+1$$ points should _always_ describe a $$k$$-dimensional figure). For example:

$$(p,q) \v (r,s) = \begin{cases} (p,q,r,s) & \if p,q,r,s \text{ linearly independent} \\ 0 & \text{ otherwise} \end{cases}$$

Join is associative and "rank commutative": 

$$\begin{aligned}
p \v (q \v r) &= (p \v q) \v r = p \v q \v r \\
p \v q &= \neg^{rank(p)rank(q)} q \v p
\end{aligned}$$

Which side of $$\bb{O}^n$$ is the 'positive' side may be specified by supplying a _global orientation_ flat $$\Omega$$, also called a 'universe', which is any oriented $$n$$-flat (there are only two choices anyway).

Flats also support the operation of _meet_ $$\^$$ relative to the global orientation, which is an oriented version of intersection. It's given by:

$$\begin{aligned}
\alpha \^ \beta &= \begin{cases} q & \exists\, p, q, r : \\
& p \v q \v r = \Omega \\
& p \v q = \alpha, q \v r = \beta \\
0 & \text{ otherwise} \end{cases}
\end{aligned}$$

Meet is associative and "corank commutative", where the "corank" of a simplex in $$\bb{O}^n$$ is $$corank(p) = rank(\Omega) - rank(p) = (n+1) - rank(p)$$.
:

$$\begin{aligned}
p \^ (q \^ r) &= (p \^ q) \^ r = p \^ q \^ r \\
p \^ q &= \neg^{corank(p) corank(q)} q \^ p \\
\end{aligned}$$

The complement of a flat $$x$$, written $$x^{\perp}$$, is a flat $$y$$ such that $$x \v y = \Omega$$, and of course $$rank(x^{\perp}) = corank(x)$$. If a flat $$x$$ is contained in a flat $$f$$, we can also define the complement with respect to $$f$$ as a flat $$y$$ such that:

$$x^{\perp f} = \begin{cases} y & \exists \, y : x \v y = f \\ 0 & \text{ otherwise} \end{cases}$$

---------

### Summarizing

This is all _exactly the same_ as exterior algebra. Well, up to a positive scalar factor.

Simplexes are multivectors. In OPG they are being used to represent the geometric object spanned by the points, which is some line/plane/etc in space. $$\neg$$ is multiplication by $$-1$$. Joins (written $$\v$$ in OPG) are wedge products (written $$\^$$ in EA). Meets $$\^$$ are meets $$\v$$. $$\Omega$$ is $$\omega$$. Complements are Hodge Stars and interior products. $$corank(p)$$ is $$\| {\star p} \|$$.

So that's fun.

(By the way, so we don't get confused, in this article $$\v$$ will refer to the join/wedge product (what I normally write $$\^$$) and $$\^$$ will refer to the meet.)

Note that it's the vector representation in $$\bb{R}^{n+1}$$ that is isomorphic to EA. For instance the simplex created by $$(2 \b{x}, \b{y})$$ is equivalent to the multivector $$(1,2,0) \v (1,0,1) = (-2,2,-1) \sim (-1, 1, -\frac{1}{2})$$ and not $$(2,0) \v (0,1) = 2 \b{x \v y}$$.

In OPG, vectors describe points and the join of two points (vectors) is a line (bivector). In EA, vectors describe lines, and the join of two points is a plane (bivector). So the EA on a vector space $$\bb{R}^n$$ is equivalent to the OPG $$\bb{O}^{n-1}$$. But the arithmetic is all the same, and we could represent $$k$$-simplexes, rather than by a list of $$k$$ points $$(p,q,\ldots)$$, by a single $$k$$-vector in $$\^^k \bb{R}^n$$ -- although this would almost always require storing more data; multivectors are not at all an efficient way to store information.[^storage]

Some connections to other stuff: what we've called a flat is also called (though the definitions usually don't include orientation) an "affine subspace" or "linear variety", and $$\bb{O}^n$$ is an [affine space](https://en.wikipedia.org/wiki/Affine_space), which is the name for a Euclidean space where we have no special concept of an origin. when we forget about the scalar factors in homogenous coordinates we are dealing with the (oriented version of the) [Projective Space](https://en.wikipedia.org/wiki/Projective_space) $$P(\v \bb{R}^{n+1})$$. The oriented version is less common but necessary if we want properties like the anticommutativity of join to line up with those in exterior algebra. In Projective Geometry's terminology, there is a thing called the Grassmannian of the vector space $$V$$, which is the set of all linear subspaces of $$V$$, and the [Plücker Embedding](https://en.wikipedia.org/wiki/Pl%C3%BCcker_embedding) maps it into $$P(\v V)$$. The components of the multivector under this map are called [Plücker Coordinates](https://en.wikipedia.org/wiki/Pl%C3%BCcker_coordinates) or Grassmann Coordinates (up to a scalar).

[^storage]: $$\v^2 V$$ has $${n \choose 2} = \frac{1}{2}n(n-1)$$ components in general. If you can store a bivector as two vectors, with $$2n$$ components, do it. Storing the full bivector with its $$O(n^2)$$ components is only necessary when you don't have a vector factorization. Though, bivectors and $$(n-1)$$ vectors can always be factored. For anything in between, not necessarily.

--------

## Good Ideas from OPG

There are a few things in Stolfi's book that I think are obviously good and worth importing into my understanding of EA (and some things obviously missing from OPG that EA knows about, but I'm not as concerned with that direction).

-------

**1**. It is useful to define meets with respect to any set, not just the whole vector space. The "meet of $$a$$ and $$b$$ in $$u$$" is a multivector $$y$$ such that there exist multivectors $$x,z$$ with:

$$\begin{aligned}
x \v y \v z &= u \\
x \v y &= a \\
y \v z &= b 
\end{aligned}$$

Then $$y = a \^_u b$$. (This is probably a better way to intuit what the meet does, anyway; I should have maybe written it this way last time...) $$u$$ has to be spanned by $$a$$ and $$b$$, and $$y$$ is their 'overlap'. The regular meet operation is just this generalized meet with respect to the universe/pseudoscalar: $$a \^ b \equiv a \^_{\Omega} b$$. 

The commutativity law now uses coranks "relative to $$u$$":

$$a \^ b = \neg^{corank_u(a) corank_u(b)} b \^ a$$

By the way, $$x^{\perp}$$ is definitely a better notation than $$\star x$$. Though it is helpful to have it as a prefix operator, so maybe we should write it as $$\perp x$$. And capital omega $$\Omega$$, which really looks like "omega" and evokes feelings of "finality" and "totality", is definitely better than lower-case omega $$\omega$$, which just looks like a weak "w". And the name "universe" is definitely better than "pseudoscalar".

-----

**2**. We can also define _joins_ with respect to any flat/multivector. This one is a bit weirder. Basically, if you have two flats (multivectors) $$v \v x$$ and $$v \v y$$, you can define their "join relative to $$v$$" ("relative join"), as:

$$(v \v x) \v_v (v \v y) = v \v x \v y$$

This is the dual operation to the "meet in a flat". We write it as $$\alpha \v_v \beta$$, and we can compute it (with $$v^{-1} = \frac{v}{\| v^2 \|}$$):

$$\alpha \v_v \beta =  (v^{-1} \cdot \alpha) \v \beta =    \alpha \v (v^{-1} \cdot\beta )$$


----------

**3**: Remember how $$\v{\^} V$$ is the "double" algebra, consisting of two exterior algebras over the same basis multivectors space? In fact, this can be made much more general.

Let us write an exterior algebra as a tuple of a vector space and a join operation. The standard exterior algebra is $$(\bb{R}^n, \v)$$. The 'meet' algebra is $$(\^^{n-1} \bb{R}^n, \^)$$. The double algebra refers to both at once, which we may write as $$(\bb{R}^n, \v, \^)$$.

Let $$S_\alpha$$ be the space of multivectors _contained in $$\alpha$$_, for example in $$\bb{R}^3$$, $$S_{\b{x \v y}}$$ is the $$\b{x \v y}$$ plane consisting of all multivectors with no $$\b{z}$$ component. Then $$(S_\alpha, \v)$$ is an exterior algebra for any $$\alpha \in \^ \bb{R}^n$$, isomorphic to $$\^\bb{R}^{\| \alpha \|}$$, and $$(S_\alpha, \v, \^_\alpha)$$ is a double algebra in the same way.

Moreover, given any two multivectors $$\alpha, \beta$$ with $$\beta \in S_{\alpha}$$, we can define an exterior algebra and double algebra "between them", using the relative-join, so $$(S_{\alpha}, \v_{\beta}, \^_{\alpha})$$ is a double algebra also. This strikes me as very cool. But also confusing.

I am not even sure what that would look like. As an example consider the 2-dimensional space spanned by $$(\b{x \v y}, \b{x \v z})$$ inside of the 4-dimensional space spanned by $$\Omega = \b{w \v x \v y \v z}$$, where we take joins relative to $$\b{x}$$: $$(\b{x \v y}) \v_{\b{x}} (\b{x \v z}) = \b{x \v y \v z}$$ and meets relative to $$\Omega$$: $$(\b{x \v y \v z}) \^_{\Omega} (\b{x \v z \v w}) = (\b{x \v z})$$. 

Something I often think about: perhaps one of the transformations that a geometric calculus should be invariant under is the operation of "forgetting about dimensions". In this case, we see that the exterior algebra on $$\bb{R}^4$$ remains an exterior algebra if we just 'forget about $$\b{x}$$' entirely, either by literally projecting every object onto the $$\b{y \^ z \^ w}$$ plane, or -- as we've done here -- by defining operations that just pretend like it isn't there.

----

**4**: OPG points out that it is useful (especially in computer implementations) to distinguish between the $$0$$s in different vector spaces. In EA terms, this means that the $$0 \in \bb{R} \simeq \^^0 V$$ is not the same as $$0 \in \^^{2} V$$; we can distinguish them by writing $$0^k$$ for the element of $$\^^k V$$. I think this is a good idea. $$0^{\v k}$$ would be good also.

------

**5**: OPG has a useful operation I hadn't thought to write down: a boolean function for determining "relative sign" of two complementary flats / multivectors. For two multivectors $$\alpha, \beta \in \v V$$ with $$\| \alpha \| + \| \beta \| = n = \dim V$$, we write:

$$\alpha \diamond \beta = \sgn(\alpha \v \beta)$$

(Recall that $$\sgn$$ gives 0 if its argument is $$0$$. We of course also take it to be $$0$$ if its argument is any $$0^k$$, in the notation of the previous section.)

This is useful for answering questions about the relative orientations of objects. If we have a point $$(a)$$ and a line $$l = (pq)$$ in $$\bb{O}^2$$, then $$(a) \v (pq) \neq 0$$ only if $$a$$ is not on $$l$$, and the orientation of $$(apq)$$ depends on _which_ side of $$l$$ it is on. Therefore:

$$p \diamond l = \begin{cases} +1 & p \text{ is on the left side of } l \\
0 & p \text{ is on } l \\
-1 & p \text{ is on the right side of } l \end{cases}$$

Which is pretty elegant. Although the requirement that $$\| \alpha \| + \| \beta \| = n$$ is a bit awkward. Of course there is no absolute sense of "which side of a line you are on" if you are in a space with $$n > 2$$, but (I believe) you can also compute it with the relative meet $$\^_a$$, which gives the sidedness relative to the orientation of a plane $$a$$. Also of course similar tests exist for sidedness of any flat relative to any other.

Another example of an OPG-style geometric relationship: the OPG's generalized version of "two distinct points are separated by a line" is: two flats $$a,b$$ of the same rank are distinct iff there is a flat $$x$$ of complementary rank in $$\Omega$$ such that $$a \diamond x = - b \diamond x$$. Not sure off the top of my head how you come up with that flat, but the reason it works is pretty obvious.

OPG is elegant and probably worth understanding if you like thinking about this stuff. I'm not sure, but it seems like there are probably a lot more to discover in these ideas.

{% include ea.html %}
