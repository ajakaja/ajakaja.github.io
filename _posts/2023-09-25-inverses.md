---
layout: blog
title: "How To Invert Anything"
tags: math
math: true
footnotes: true
aside: true
---

Here's a notion that I've been bouncing around for a while:

For any generic linear equation like $$ax = b$$ in which multiplication by $$a$$ is a linear operation, solutions are going to be of the form 

$$x = a_{\parallel}^{-1} (b) + a_{\perp}^{-1} (0)$$

regardless of whether $$a$$ is "invertible". $$a_{\parallel}^{-1}$$ is a sort of "parallel inverse" (in some cases called the "pseudoinverse"), which is the invertible _part_ of $$a$$, while $$a_{\perp}^{-1}$$ is an "orthogonal inverse" which produces the nullspace of $$a$$, the objects for which $$ax = 0$$. It's probably not always true that a formula like this works --- for instance it will require modification if the space of solutions is not topologically connected --- but it often does.

This pattern shows up over and over in different fields, but I've never seen it really discussed as a general phenomenon. But really it makes total sense: why shouldn't _any_ operator be invertible, as long as you are willing to have the inverse live be in a larger space and possible become multi-valued?

I thought I would investigate.

<!--more-->

-----------

## 1

### Matrices

Suppose you have a linear system of equations like 

$$A \b{x} = \b{b}$$

If $$A$$ is square and invertible, then the unique solution to this equation is given via left-multiplication by $$A^{-1}$$:

$$\b{x} = A^{-1} \b{b}$$

If $$A$$ is _not_ square and invertible, then the system of equations is either underspecified (has a non-trivial linear subspace of possible solutions) or overspecified (is unsolveable because there are more linearly-independent constraints than degrees of freedom). If it's one of those cases and you're an undergrad, you give up. Whereas if you're a professional you sigh and look up how to solve it or ask Mathematica or NumPy or something, I don't know; the point is that these _can_ be solvable depending on the nature of $$A$$ and $$\b{b}$$ but it's a lot harder.

In fact, the general solution is to use a [Moore-Penrose Pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) $$A^{+}$$ which, instead of the condition $$A A^{-1} = I$$, obeys the weaker condition $$A A^{+} A = A$$. Then the solution, if there is one, is given by

$$\b{x} = A^+ (\b{b}) + (I - A^+ A) \vec{\lambda}$$

where $$\vec{\lambda}$$ is a vector of free parameters corresponding to the number of degrees of freedom in the solution space. The solution ends up being of the form $$\b{x} = \b{x}_{\text{one}} + \b{x}_{\text{free}}$$, where the first term is _one_ solution and the second term produces all the others. The second term is rotating the free parameters $$\vec{\lambda}$$ into the subspace that $$A$$ annihilates, hence why they can take any value; if you plug this back into $$A \b{x} = \b{b}$$ you will notice that the second term $$(I - A^+ A)$$ will cancel out due to the condition on $$A^+$$. Also, if $$A$$ _is_ invertible, then $$A^+ A = I$$ and the second term is zero anyway.

Meanwhile the first term produces a single actual solution vector that satisfies the $$A \b{x} = \b{b}$$ constraint. It's easiest to see what $$A+$$ means in the [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) of $$A$$: it takes the diagonal SVD matrix $$\Sigma = \text{diag}(\sigma_i)$$ and just inverts it into $$\Sigma^{-1} = \text{diag}(1/\sigma_i)^T$$. Although, considering that there's potentially a whole linear subspace of possible solutions, this can't be the only possible value of $$A^+$$. In practice the Moore-Penrose inverse adds some additional constraints to make it unique. We don't care about those for our purpsoes.

Anyway, of the instances of this "generalized inverse" thing, this matrix pseudoinverse is the most refined and easiest to understand. But it seems to make sense in a lot of other settings.


---------

### Scalars

Once again, if an equation of the form $$ax = b$$ can be inverted, it's always going to look like this:

$$x = a_{\parallel}^{-1} (b) + a_{\perp}^{-1}(0)$$

Where the first term is a _single_ solution that satisfies the constraint, and the second term is any displacement 'orthogonal' to $$a$$ within the solution space that has $$a (a_{\perp}^{-1}) = 0$$.[^topology]

[^topology]: I suppose that this form wouldn't work if the solution space was topologically non-trivial, i.e. you can't create every solution by adding an orthogonal term to a solution term. More on that later, maybe.

For instance here is how you invert "scalar multiplication" and divide by zero.

$$\begin{aligned}
ax &= b \\
x &= b/a + \lambda 1_{a=0} 
\end{aligned}$$

$$\lambda$$ is a free parameter and $$1_{a=0}$$ an indicator function. If $$a=0$$ and $$b=0$$ then this works as long as we interpret $$b/a = 0/0$$ to equal, say, $$1$$. If $$a=0$$ but $$b \neq 0$$ then the answer is infinite, which makes sense: there's really no solution to $$0x = b$$ unless $$x$$ is an object that acts like $$b/0$$. So we can write the solutions as:

$$x = \begin{cases}
b/a & a \neq 0 \\
b \times \infty & a = 0, b \neq 0 \\
\text{anything} & a = 0, b = 0 \\
\end{cases}$$

Note that by interpreting division in this way we have managed to divide by zero without being wrong. Also note that this is basically the $$1 \times 1$$ version of the matrix pseudoinverse.

It is interesting, although I won't go into it here, to imagine differentiating between $$1/0$$, $$1/0^2$$, $$1/0^3$$, etc, as "different orders of $$\infty$$ --- or, equivalently, differentiating $$0$$, $$0^2$$, $$0^3$$ as "different orders of zero". Then it would be possible to sometimes solve the $$a=0, b=0$$ cases depending on their relative $$0$$-ness.

-------

### Vectors

Here's the $$1 \times N$$ version, another interesting example.

$$\b{a} \cdot \b{x} = \b{b}$$

Then

$$\begin{aligned}
\b{x} &= \b{a}_{\parallel}^{-1} \cdot \b{b} + \b{a}_{\perp}^{-1}(0) \\
&= \frac{\b{a}}{\| \b{a} \|^2} \cdot \b{b} + \b{a}_{\perp}^{-1}(0)
\end{aligned}$$

The first term is again the "pseudoinverse" of $$\b{a}$$, written $$\b{a}^{+} \b{a}_{\parallel}^{-1} = \frac{\b{a}}{\| \b{a} \|^2}$$. It's like an inverse but only in the subspace in which $$\b{a}$$ is invertible, which for a vector is its 1d subspace. Meanwhile the second term is the "orthogonal inverse" of $$\b{a}$$, which is any element of the $$(N-1)$$-dimensions orthogonal to $$\b{a}$$.

------

### Functions

Here's an example with functions.

$$x f(x) = 1$$

The solution is

$$f(x) = \begin{cases} 
1/x & x \neq 0 \\
\lambda & x = 0
\end{cases}$$

$$\lambda$$ is again a free parameter. We'd kinda like to write these as one equation, and the way we can do it is like this:

$$f(x) = \mathcal{P}(\frac{1}{x}) + \lambda \delta(x)$$

Where $$\mathcal{P}$$ is the [Cauchy Principal Value](https://en.wikipedia.org/wiki/Cauchy_principal_value) and $$\delta(x)$$ is a delta distribution. We can't just include $$\frac{1}{x}$$ directly because it's not defined at $$x=0$$, but the principal value works around this; it doesn't have a value on its own, but if multiplied by $$x$$ it will always give $$1$$.[^principal] (I think! But take with a grain of salt. I don't understand $$\mathcal{P}$$ very well.)

[^principal]: If you try to assign a value to $$\mathcal{P}(\frac{1}{x})$$ by taking $$\int_{-\e}^\e \frac{1}{x} dx$$ you will find that it gives a different value depending on _how_ you take the limit to shrink the integration bounds to $$0$$. So don't try. $$\mathcal{P}$$ only makes sense as an unbound distribution that receives its meaning inside a larger equation, similar to $$\delta(x)$$.<br><br>Also, note the similarity with the [Sokhotskiâ€“Plemelj theorem](https://en.wikipedia.org/wiki/Sokhotski%E2%80%93Plemelj_theorem) which is basically computing $$\lim_{\e \ra 0} \frac{1}{x + i \e}$$.

Interestingly, in order to invert a function at $$0$$ we had to escape into the larger space of distributions. This is another pattern: when we invert non-invertible things, we often have to "leave the space we're in" for a larger space that can contain the solution. In this case we leave "function space" for "distribution space" because $$\frac{1}{x}$$ doesn't make sense at $$x=0$$ for functions.

What if we have more powers of $$x$$?

$$x^n f(x) = 1$$

We should get something like

$$f(x) = \mathcal{P}(\frac{1}{x^n}) + \delta(x) [\lambda_1 + \lambda_2 x + \ldots \lambda_{n-1} x^{n-1} ]$$

Since, by the same argument, all of those terms should go to $$0$$ when $$x=0$$ as well.

------

### Differential Operators

Here's another example:

$$D f = g$$

The classic way to solve this, which is widely used in physics, is via [Green's Functions](https://en.wikipedia.org/wiki/Green%27s_function). First we find the Green's function for $$D$$, called $$G$$, which has

$$D G = \delta$$

The, since $$\delta$$ is the identity element for [convolution](https://en.wikipedia.org/wiki/Convolution) (that is, $$\delta \ast f = f$$), we write down the solution as

$$f = G \ast g$$

Which is a solution because

$$Df = D (G \ast g )= (DG) \ast g = \delta \ast g = g$$

In practice computing $$G = D^{-1} \delta$$ can be hard. Often you can look it up. When you can't the usual technique (at least in physics) is to (sloppily) Fourier-transform the operator and invert it via multiplication, ending up with a form like $$\hat{G} = \frac{1}{\hat{D}(k)}$$. Un-Fourier-transforming that inverse operator is the hard part, and even just dividing by $$k$$ can make distributional objects like $$\delta(x)$$ and $$\mathcal{P}(1/x)$$ show up in the result. But that's okay, really: we're not really surprised that inverting an operation might blow us up into a different space, particularly into a space that isn't forced to assign a value to $$1/0$$. So if you're lucky the final solution can be interpreted as a function, but it might end up with some combination of $$\mathcal{P}$$ and $$\delta$$ and the like in it, and if so you'll be better off leaving it that way.

For instance, the Poisson equation $$\del^2 f(\b{x}) = g(\b{x})$$ is Fourier transformed[^Fourier] into $$(i k)^2 \hat{f}(\b{k}) = \hat{g}(\b{k})$$ so $$\hat{f}(\b{k}) = \frac{1}{(ik)^2} \hat{g}(\b{k})$$ and finally $$f(\b{x}) = \mathcal{F}^{-1} [\frac{1}{(ik)^2} g( \b{k}) ]$$. The inverse Fourier transform is easy, thankfully: it's $$-\frac{1}{r} \ast g$$, giving $$f(\b{x}) \sim \int \frac{1}{\|x - x_0\|} g(\b{x}_0) d\b{x}_0$$, which is just the equation for an electric potential in terms of electric charges.

[^Fourier]: The [trick](https://arxiv.org/abs/1610.09702) to computing Fourier transforms in your head is to replace any $$f(\b{x}, \p_\b{x})$$ with $$\hat{f}(i \p_\b{k}, i \b{k}) \delta(\b{k})$$. You won't be entirely right, due to tricky behavior at $$k = 0$$ and having to figure out what is meant by any $$\p^{-n}$$s, but you'll be really close.

None of that is quite correct, though. Following the previous examples, the solution _should_ be shaped like

$$f = D^{-1}_{\parallel} (g) + D^{-1}_{\perp} (0)$$

The first term $$D^{-1}_{\parallel}$$ is the explicit Green's function solution which gets convolved with $$g$$. The second term is all of the elements in the kernel of $$D$$. Where does it come from? One way is via the Fourier transform of the earlier paragraph. In the Laplacian $$\del^2$$ example above, we have to keep in mind our generalized inverses for the division in Fourier space:

$$\begin{aligned}
\del^2 f(\b{x}) &= g(\b{x}) \\
- k^2 \hat{f}(\b{k}) &= \hat{g}(\b{k}) \\
\hat{f}(\b{k}) &= - \frac{\hat{g}(\b{k})}{k^2}  + \lambda_1 \delta(\b{k}) + \frac{\vec{\lambda}_2 \cdot \b{k}}{k^2} 
\end{aligned}$$

(For the last term, we need something that's $$O(k^{-1})$$, but I suppose it could really have any number of factors of $$\b{k}$$ in the numerator as long as there's one more in the denominator.)

Upon inverse-Fourier-transforming these two extra terms correspond to two types of terms that have $$\del^2 = 0$$. The first term is constants $$\lambda_1$$, while the second gives a linear term like $$\vec{\lambda}_2 \cdot \b{x}$$. Unsurprisingly, the Laplacian annihilates constant and linear terms, as expected. [^argument]

[^argument]: Actually this argument seems to work for higher-dimensional polynomials as well, as long as every term has a $$\frac{1}{k^n}$$ factor in Fourier space... I don't quite know what to do with that idea though. I need to find a better table of Fourier transforms that includes crazy delta-function stuff; Wikipedia is letting me down.

In principle you can do something like this for any $$D f = g$$ differential equation:

1. Write down its Green's function solution term corresponding to $$D^{-1}(g)$$.
2. But _also_ include terms for $$D^{-1}(0)$$ (which you might find easier to compute in Fourier space)
3. and if you do that properly you should be have a closed form for the complete space of solutions.

Of course that doesn't deal with boundary conditions at all. Normally those are encoded into the definition of the Green's function.

None of this had to go through Fourier space, of course, but I wanted to show it that way because it's intriguing how much everything seems to line up: the funny rules for division up above correspond exactly to the funny rules for inverting differential operators down here, if you do it right. I assume that there is some not-yet-buttoned-up "distributional calculus" that combines all of these ideas together in such a clean, intuitive way that's so simple it ought to be the thing we learn in high school instead of Leibniz's calculus. Sadly I in no way have the qualifications or competencies necessary to figure out exactly what it looks like. But I feel like I can perceive the shape of it, a bit, so that's what I'm describing here.

---------

### Final thoughts

I hope I have convinced you that inverting all of these linear operators sorta works the same way, and maybe you also have the hunch that I have: that there's some grand theory that ties all of this together and makes solving equations easy, like high-school-algebra easy, again. We can dream, right?

It feels like this article has gotten long enough, but I'll just mention that there are a few questions I'm curious to explore later.

First: is it better to "homogenize" all of these equations by e.g. converting $$A \b{x} = \b{b}$$ into $$(A, \b{b}) \cdot (\b{x}, 1) = 0$$? In exterior algebra there's a sense in which the solution to $$A \b{x} = \b{b}$$ is cleanly generated by computing 

$$\^^{n} (\b{A}, \b{b}) = (\^^{n-1} \b{A}, \^^n A)$$

The latter term $$\^^n A$$ is the same as $$\det A$$, and when you set this equal to $$(\b{x}, 1)$$ it implies that you should divide the last term through to get $$A^{-1} = \frac{\^^{n-1} \b{A}}{\det A}$$, which is, in fact, the formula for the inverse of a matrix. But I want to go through this more carefully and figure out how to reconcile it with the "pseudoinverse", and especially figure out if a similar operation makes sense for the other types of linear problems I mentioned above.

Second: can we deal with boundary conditions the same way? For $$D f = g$$ it's normal to include a separate boundary condition term that looks like $$B f = 0$$. But in that case maybe we can treat $$(D, B) f = (g, 0)$$ and use the same homogenization procedure to solve it in one shot? Not sure how to pull that off but it would sure be nice for all of these differential equations to be solved by rote algebra.

Third: in solving $$ax = b$$ with $$a^{-1}(b) + a^{-1}(0)$$, there's an assumption that the whole space of inverses is accessible by adding free parameters to any single solution. Presumably this is not always true because the space of solutions may have some non-trivial topology that makes this impossible. (Presumably algebraic geometers have already thought about this to death). Is there possibly an rote-algebra way to make that non-trivial topology like... trivial again? Would be neat.


