---
layout: blog
title: "How To Invert Anything"
tags: math
math: true
footnotes: true
aside: true
---

Here's a notion I've been bouncing around for a while:

For a generic linear equation like $$ax = b$$, solutions seem to always be of the form 

$$x = a_{\parallel}^{-1} (b) + a_{\perp}^{-1} (0)$$

regardless of whether $$a$$ is "invertible". Here $$a_{\parallel}^{-1}$$ is a sort of "parallel inverse", in some cases called the "pseudoinverse", which is the invertible part of $$a$$. $$a_{\perp}^{-1}$$ is the "orthogonal inverse", normally called either the nullspace or kernel of $$a$$ depending what field you're in, but either way it's the objects for which $$ax = 0$$. It's probably not always true that a formula like this works, but it often does.

This pattern shows up over and over in different fields, but I've never seen it really discussed as a general phenomenon. But really, it makes sense: why shouldn't _any_ operator be invertible, as long as you are willing to have the inverse (a) live in a larger space and (b) possibly become multi-valued?

Here are some examples.

<!--more-->

-----------

## 1. The Pattern

### Matrices

Consider a linear system of equations that's represented by matrix multiplication as

$$A \b{x} = \b{b}$$

If $$A$$ is square and invertible, then the unique solution to this equation is given via left-multiplication by $$A^{-1}$$:

$$\b{x} = A^{-1} \b{b}$$

If $$A$$ is _not_ square and invertible, then the system of equations is either underspecified (has a non-trivial linear subspace of possible solutions) or overspecified (is unsolvable because there are more linearly-independent constraints than degrees of freedom). If it's one of those cases and you're an undergrad, you give up. If you're a professional you sigh and look up how to solve it or ask Mathematica or NumPy or something, I don't know, I'm not a professional either; the point is that these _can_ be solvable depending on the nature of $$A$$ and $$\b{b}$$ but it's a lot harder.

The general solution uses the [Moore-Penrose Pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) $$A^{+}$$ which, instead of the condition $$A A^{-1} = I$$, obeys the weaker condition $$A A^{+} A = A$$. Then the solution, if there is one, is given by

$$\b{x} = A^+ (\b{b}) + (I - A^+ A) \vec{\lambda}$$

where $$\vec{\lambda}$$ is a vector of free parameters corresponding to the number of degrees of freedom in the solution space. The solution ends up being of the form $$\b{x} = \b{x}_{\text{one}} + \b{x}_{\text{free}}$$, where the first term is _one_ solution and the second term produces all the others. This is quite cool. The first term produces a single actual solution vector that satisfies the $$A \b{x} = \b{b}$$ constraint. The second term constructs a matrix which rotates the $$\vec{\lambda}$$ free parameters so that they span the nullspace of $$A$$. On plugging back into $$A \b{x} = \b{b}$$, the second term $$(I - A^+ A)$$ will cancel out entirely due to the condition on $$A^+$$. Of course, if $$A$$ _is_ invertible, then $$A^+ A = I$$ and the second term is zero anyway.

It's easiest to see what $$A^+$$ actually is via [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) representation of $$A$$.[^svd] It takes the non-zero entries from the SVD diagonal $$\Sigma = \text{diag}(\sigma_i)$$ and just inverts them into $$\Sigma^{-1} = \text{diag}(1/\sigma_i)^T$$, while leaving the zeroes untouched.

[^svd]: For those not in the know: the SVD has one positive diagonal entry for each subspace it doesn't annihilate, and one zero for each subspace it does annihilate.

Anyway, of the instances of this "generalized inverse" thing, this matrix pseudoinverse is the most refined and easiest to understand. But it seems to make sense in a lot of other settings.


---------

### Scalars

Once again, if an equation of the form $$ax = b$$ can be inverted, it usually looks like this:

$$x = a_{\parallel}^{-1} (b) + a_{\perp}^{-1}(0)$$

The first term is a _single_ solution that satisfies the constraint, and the second term is any displacement 'orthogonal' to $$a$$ within the solution space that has $$a (a_{\perp}^{-1}) = 0$$.

For instance here is how you invert "scalar multiplication", that is, how you divide by zero.

$$\begin{aligned}
ax &= b \\
x &= b/a + \lambda 1_{a=0} 
\end{aligned}$$

$$\lambda$$ is a free parameter and $$1_{a=0}$$ an indicator function. If $$a=0$$ and $$b=0$$ then this works as long as we interpret $$b/a = 0/0$$ to equal, say, $$1$$. If $$a=0$$ but $$b \neq 0$$ then the answer is infinite, which makes sense: there's really no solution to $$0x = b$$ unless $$x$$ is an object that acts like $$b/0$$. So we can write the solutions as:

$$x = \begin{cases}
b/a & a \neq 0 \\
b \times \infty & a = 0, b \neq 0 \\
\text{anything} & a = 0, b = 0 \\
\end{cases}$$

Not that this is especially useful on its own, but it's one of a pattern. In a sense it is the _right_ answer, in that it contains all the information that the true inverse of this operation has to contain --- just, represented in a way we don't really know how to do math with. Also note that this is basically the $$1 \times 1$$ version of the matrix pseudoinverse.



-------

### Vectors

Here's the $$1 \times N$$ version, for a more interesting example.

$$\b{a} \cdot \b{x} = \b{b}$$

Then

$$\begin{aligned}
\b{x} &= \b{a}_{\parallel}^{-1} \cdot \b{b} + \b{a}_{\perp}^{-1}(0) \\
&= \frac{\b{a}}{\| \b{a} \|^2} \cdot \b{b} + \b{a}_{\perp}^{-1}(0)
\end{aligned}$$

The first term is again the "pseudoinverse" of $$\b{a}$$, written $$\b{a}^{+} = \b{a}_{\parallel}^{-1} = \frac{\b{a}}{\| \b{a} \|^2}$$. It's like an inverse, but only in the subspace in which $$\b{a}$$ is invertible, which for a vector is just one-dimensional. The second term is the "orthogonal inverse" of $$\b{a}$$, which is any element of the $$(N-1)$$-dimensional nullspace which is orthogonal to $$\b{a}$$. We could also choose to write that as an $$(n-1)\times(n-1)$$ matrix times a vector of $$(n-1)$$ free parameters, if we wanted to stick with the pseudoinverse form, but I don't mind leaving it like it is.

------

### Functions

Here's an example with functions.

$$x f(x) = 1$$

The solution is

$$f(x) = \begin{cases} 
1/x & x \neq 0 \\
\lambda & x = 0
\end{cases}$$

$$\lambda$$ is again a free parameter. We'd kinda like to write these as one equation, and the way we can do it is like this:

$$f(x) = \mathcal{P}(\frac{1}{x}) + \lambda \delta(x)$$

Where $$\mathcal{P}$$ is the [Cauchy Principal Value](https://en.wikipedia.org/wiki/Cauchy_principal_value) and $$\delta(x)$$ is a delta distribution. Whereas vectors and matrices forced us to "kick up into" a space with free parameters, functions require us to also "kick up into" the space of distributions. Neither $$\P$$ nor $$\d$$ gives a useful value on its own at $$x=0$$, but when used as a distribution (e.g. integrated against a test function) this object really does behave like the solution to $$x f(x) = 1$$. This is a pattern: when we invert non-invertible things, we often have to (a) introduce free parameters and (b) "leave the space we're in" for a larger space that can contain the solution. In this case we leave "function space" for "distribution space" because $$\frac{1}{x}$$ has no value at $$x=0$$.[^step]

[^step]: In the context of any _particular_ problem, $$\lambda$$ may have a definite value. For instance when computing the Fourier transform of $$\p_x \theta(x) = \delta(x)$$ (where $$\theta$$ is the step function), one ends up with $$(ik) \hat{\theta}(k) = 1$$. Upon dividing through by $$(ik)$$ the result is $$\frac{1}{ik} + \pi \delta(k)$$, where $$\pi$$ is the right answer; it's actually $$(2 \pi) (\frac{1}{2})$$, where $$\frac{1}{2}$$ is the average value of $$\theta(x)$$, hence it's the right coefficient at $$k=0$$.

What if we have more powers of $$x$$?

$$x^n f(x) = 1$$

We should get something like

$$f(x) = \mathcal{P}(\frac{1}{x^n}) + \delta(x) [\lambda_1 + \lambda_2 x + \ldots \lambda_{n-1} x^{n-1} ]$$

Since, by the same argument, all of those terms should go to $$0$$ when $$x=0$$ as well.

------

### Differential Operators

Here's another example:

$$D f = g$$

The natural solution is 

$$f = D^{-1}_{\parallel}(g) + D^{-1}_{\perp}(0)$$

What do these terms mean for a differential operator? The second term, again, is an inverse operator that produces the "nullspace" of $$D$$, which is to say, produces solutions to the homogenous equation $$D f = 0$$. These are "free wave" solutions to the operator, which are packaged with a (probably) infinite number off free parameters. In general there is also a set of boundary conditions on the solution $$f$$, so we'll need to pick from these free parameters to satisfy the boundary conditions.

Or, we could solve it using a Green's functions for $$D$$, by computing $$G = D^{-1}_{\parallel}(\delta)$$ and then $$f = G \ast g$$.

Or, we could invert the operator by directly inverting in Fourier space, which will end up being the function inversion from the previous section:

$$\hat{G} = \hat{D}_{\perp}^{-1}(\hat{g}) + \hat{D}_{\parallel}^{-1}(0)$$

The $$\hat{D}_{\parallel}^{-1}(0)$$ term will produce the homogenous solutions to the differential operator when un-Fourier-transformed. 

Example: the Poisson equation $$\del^2 f(\b{x}) = g(\b{x})$$ is Fourier transformed into $$(i k)^2 \hat{f}(\b{k}) = \hat{g}(\b{k})$$ so $$\hat{f}(\b{k}) = \frac{1}{(ik)^2} \hat{g}(\b{k}) + \lambda_1 \delta(k) + O(\frac{1}{k}) \ldots$$.[^radial] The third term can be $$\frac{1}{k} \delta(k)$$, $$\frac{A\b{k}}{k^2}$$, or higher-order tensor combinations as long as the net magnitude is $$O(\frac{1}{k})$$.

[^radial]: If you go to solve this, keep in mind that $$\delta(k) = \frac{\delta(\b{k})}{4 \pi k^2}$$. Also I'm a bit unsure about this form because I don't really know how to the Fourier transform here.

When untransformed we _should_ get

$$\begin{aligned}
f(\b{x}) &= \mathcal{F}^{-1} [\frac{1}{(ik)^2} \hat{g}( \b{k}) + \lambda_1 \delta(k) + O(\frac{1}{k}) \lambda_2 \delta(k)] \\
&= - \int \frac{1}{4 \pi \| r - r_0 \|} g(\b{x}_0) d \b{x}_0 + \text{<the harmonic functions>}
\end{aligned}$$

--------

# 2. Why do we care?

It's suggestive. The dream is that, at some deep level, all of these problems work the same way, and maybe there's a generic approach to solving them that capitalizes on this structure.

Every equation that we solve is an inverse problem of some sort. Finding the roots of a polynomial is inverting $$P(x) = 0$$. Computing an integral $$\int f(x) dx$$ is inverting $$dF(x) = f(x)$$. Solving a differential equation with boundary value constraints is inverting $$\begin{cases} Df = g \\ Bf = 0 \end{cases}$$. Etc. It seems to me like in some sense every type of problem is doing a similar thing which can be seen in a generic way, and I expect that, if done right, a long list of notable theorems and techniques should fall out as fairly rote applications. of the same general principles.
