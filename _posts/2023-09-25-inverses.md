---
layout: blog
title: "How to invert anything"
tags: math
math: true
footnotes: true
aside: true
---

Here's a notion that I've been bouncing around for a while. 

Warning: there isn't an ounce of rigor in this article, and notation is going to be grossly abused.

<!--more-->

-----------

## 1

Suppose you have a linear system of equations like 

$$A \b{x} = \b{b}$$

If $$A$$ is square and invertible, then the unique solution to this equation is given via left-multiplication by $$A^{-1}$$:

$$\b{x} = A^{-1} \b{b}$$

If $$A$$ is _not_ square and invertible, then the system of equations is either underspecified (has a non-trivial linear subspace of possible solutions) or overspecified (is unsolveable because there are more linearly-independent constraints than degrees of freedom). If it's one of those cases and you're an undergrad, you give up. Whereas if you're a professional you sigh and look up how to solve it or ask Mathematica or NumPy or something, I don't know; the point is that these _can_ be solvable depending on the nature of $$A$$ and $$\b{b}$$ but it's a lot harder.

But it doesn't have to be.

The general solution for a matrix uses the [Moore-Penrose Pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) $$A^{+}$$ which, instead of the condition $$A A^{-1} = I$$, obeys the weaker condition $$A A^{+} A = A$$. Then the solution, if there is one, is given by

$$\b{x} = A^+ (\b{b}) + (I - A^+ A) \b{w}$$

where $$\b{w}$$ is a vector of free parameters corresponding to the number of degrees of freedom in the solution space. The solution ends up being of the form $$\b{x} = \b{x}_{\text{one}} + \b{x}_{\text{free}}$$, where the first term is _one_ solution and the second term produces all the others. The second term is rotating the free parameters $$\b{w}$$ into the subspace that $$A$$ annihilates, hence why they can take any value; if you plug this back into $$A \b{x} = \b{b}$$ you will notice that the second term $$(I - A^+ A)$$ will cancel out due to the condition on $$A^+$$. Meanwhile the first term is producing a single actual solution vector that satisfies the $$A \b{x} = \b{b}$$ constraint. $$A^+$$ therefore can't be unique because there's a whole linear subspace of possible solutions.

I'm intrigued by the idea that the "inverse" of a non-invertible matrix $$A$$ has two terms, one which acts like a real inverse and one which has newly-minted free parameters in it. It shows up in a lot of places! Here are a few of them.

---------

### Scalars

If an equation of the form $$ax = b$$ can be inverted, it's always going to look like this:

$$x = a_{\parallel}^{-1} (b) + a_{\perp}^{-1}(0)$$

Where the first term is (abusing notation) a _single_ solution that satisfies the constraint, and the second term is any displacement 'orthogonal' to $$a$$ within the solution space that has $$a (a_{\perp}^{-1}) = 0$$.[^topology]

[^topology]: I suppose that this form wouldn't work if the solution space was topologically non-trivial, i.e. you can't create every solution by adding an orthogonal term to a solution term. More on that later, maybe.

For instance here is how you invert "scalar multiplication":

$$\begin{aligned}
ax &= b \\
x &= b/a + \lambda 1_{a=0} 
\end{aligned}$$

With $$\lambda$$ a free parameter and $$1_{a=0}$$ an indicator function. If $$a=0$$ and $$b=0$$ then this works as long as we interpret $$b/a = 0/0$$ to equal, say, $$1$$. If $$a=0$$ but $$b \neq 0$$ then the answer is infinite, which makes sense: there's really no solution to $$0x = b$$ unless $$x$$ is an object that acts like $$b/0$$. So we can write the solutions as:

$$x = \begin{cases}
b/a & a \neq 0 \\
b \times \infty & a = 0, b \neq 0 \\
\text{anything} & a = 0, b = 0 \\
\end{cases}$$

Note that by interpreting division in this way we have managed to divide by zero without being wrong.

-------

### Vectors

Here's another example:

$$\b{a} \cdot \b{x} = \b{b}$$

Suppose we write $$\b{a}_{\perp}$$ for "all the elements of the $$(n-1)$$-dimensional space of vectors that have $$\b{a} \cdot \b{a}_{\perp} = 0$$". Then

$$\begin{aligned}
\b{x} &= \b{a}^{+} \cdot \b{b} + \b{a}^{-1}(0) \\
&= \frac{\b{a}}{\| \b{a} \|^2} \cdot \b{b} + \b{a}_{\perp}
\end{aligned}$$

The first term is the "pseudoinverse of $$\b{a}$$", written $$\b{a}^{+} = \frac{\b{a}}{\| \b{a} \|^2}$$. I like to call it the "parallel inverse": it's the inverse, but only in the subspace in which $$\b{a}$$ is invertible, which for a vector is the 1d subspace parallel to it. Meanwhile the second part is the "orthogonal inverse of $$\b{a}$$", which is its nullspace or kernel or whatever you want to call it.

This is a general pattern: whenever we invert a linear object there are two terms: a "parallel inverse"/pseudoinverse/whatever, which is its actual inverse _in the subspace it lives in_, and which in that space just looks like division, and an "orthogonal inverse", which is the kernel of the operator but written in such a way that it generates all of the remaining degrees of freedom in the solution space out of a vector of free parameters.

------

### Functions

Here's another example:

$$f(x) x = 1$$

The solution is

$$f(x) = \begin{cases} 
1/x & x \neq 0 \\
1/x + \lambda & x = 0
\end{cases}$$

Which we can write as 

$$f(x) = \mathcal{P}(\frac{1}{x}) + \lambda \delta(x)$$

Where $$\mathcal{P}$$ is the [Cauchy Principal Value](https://en.wikipedia.org/wiki/Cauchy_principal_value) and $$\delta(x)$$ is a delta distribution and $$\lambda$$ is a free parameter. The result is forced to be a distribution rather than a function, and therefore "only makes sense under an integral sign"... but otherwise it works. (I mean, I think it works. I don't really understand $$\mathcal{P}$$ very well. But I'm pretty sure the right way to interpret $$\mathcal{P}(\frac{1}{x})$$ is: it doesn't have a value on its own,[^principal] but if multiplied by $$x$$ under an integral sign it will always give $$1$$.)

[^principal]: If you try to assign a value to $$\mathcal{P}(\frac{1}{x})$$ by taking $$\int_{-\e}^\e \frac{1}{x} dx$$ you will find that it gives a different value depending on _how_ you take the limit to shrink the integration bounds to $$0$$. So don't try. $$\mathcal{P}$$ only makes sense as an unbound distribution that receives its meaning inside a larger equation, similar to $$\delta(x)$$.<br><br>Also, note the similarity with the [Sokhotskiâ€“Plemelj theorem](https://en.wikipedia.org/wiki/Sokhotski%E2%80%93Plemelj_theorem) which is basically computing $$\lim_{\e \ra 0} \frac{1}{x + i \e}$$.

This is another pattern: when we invert non-invertible things, which is to say when we _divide by zero_, we often have to "leave the space we're in" for a larger space that can contain the solution. In this case we leave "function space" for "distribution space" because $$\frac{1}{x}$$ doesn't make sense at $$x=0$$ in function space.

------

### Derivatives

Here's another example:

$$\p F = g$$

We write:

$$f = \p^{-1}(g) + \p^{-1} (0)$$

The first term is interpreted as a pseudoinverse; it should be the antiderivative $$\int g(x) dx$$ of $$g$$ but with no integration constant. The second term is the antiderivative of $$0$$, hence, a constant. Therefore:

$$f = \int g(x) dx + C$$

-------

### Differential Operators

Here's another example:

$$D f = g$$

The classic way to solve this, which is widely used in physics, is via [Green's Functions](https://en.wikipedia.org/wiki/Green%27s_function). First we find the Green's function for $$D$$, called $$G$$, which has

$$D G = \delta$$

The, since $$\delta$$ is the identity element for [convolution](https://en.wikipedia.org/wiki/Convolution) (that is, $$\delta \ast f = f$$), we write down the solution as

$$f = G \ast g$$

Which is a solution because

$$Df = D (G \ast g )= (DG) \ast g = \delta \ast g = g$$

In practice computing $$G = D^{-1} \delta$$ can be hard. Often you can look them up. When you can't, at least in physics, the usual technique is to (sloppily) Fourier-transform the operator and invert it via multiplication.

For instance, the Poisson equation $$\del^2 f(\b{x}) = g(\b{x})$$ is Fourier transformed[^Fourier] into $$(i k)^2 \hat{f}(\b{k}) = \hat{g}(\b{k})$$ so $$\hat{f}(\b{k}) = \frac{1}{(ik)^2} \hat{g}(\b{k})$$ and finally $$f(\b{x}) = \mathcal{F}^{-1} [\frac{1}{(ik)^2} g( \b{k}) ]$$. Now, un-Fourier-transforming the inverse operator in general can be pretty hard, because division-by-$$k$$ is often Fourier-transformed into $$\mathcal{P}(1/x)$$ which isn't a function. But actually it _shouldn't_ be a function, because in general Green's functions and differential equation solutions are distributions and don't need to be functions in the first place.[^diffeq] If you're lucky the final solution can be interpreted as a function, but it might end up with some combination of $$\mathcal{P}$$ and $$\delta$$ and the like in it, and if so you'll be better off leaving it that way.

In the case of $$\del^2 f = g$$, the inverse Fourier transform is easy, thankfully: it's $$\frac{1}{r} \ast g$$, which is to say that the $$f(\b{x}) = \int \frac{1}{\|x - x_0\|} g(\b{x}_0) d\b{x}_0$$, which is just the equation for a potential in terms of charges.

[^Fourier]: The [trick](https://arxiv.org/abs/1610.09702) to computing Fourier transforms in your head is to replace any $$f(\b{x}, \p_\b{x})$$ with $$\hat{f}(i \p_\b{k}, i \b{k}) \delta(\b{k})$$. You won't be entirely right, due to tricky behavior at $$k = 0$$ and having to figure out what is meant by any $$\p^{-n}$$s, but you'll be really close.

[^diffeq]: One of my gripes with formal mathematics: who cares what space the solution is in? Just give me something that solves it! .... Yeah, math and I have different goals.

None of that is quite correct, though. Following the previous examples, the solution _should_ be shaped like

$$f = D^{-1}_{\parallel} (g) + D^{-1}_{\perp} (0)$$

The first term $$D^{-1}_{\parallel}$$ is the explicit Green's function solution which gets convolved with $$g$$. The second term is all of the elements in the kernel of $$D$$. Where does it come from? One way is via the Fourier transform of the earlier paragraph. In the Laplacian $$\del^2$$ example above, we have to keep in mind our generalized inverses for the division in Fourier space:

$$\begin{aligned}
\del^2 f(\b{x}) &= g(\b{x}) \\
- k^2 \hat{f}(\b{k}) &= \hat{g}(\b{k}) \\
\hat{f}(\b{k}) &= - \frac{\hat{g}(\b{k})}{k^2} + \lambda \delta (k) + \ldots
\end{aligned}$$

In fact, since we're inverting multiplication by $$(ik)^2$$ we can have more free parameters on the right. We know that $$k^2 \delta(k)$$ is zero, but also any term like $$k^2 \frac{1}{k} \delta(k)$$ will be zero as well due to having two factors of $$k$$ in the numerator and one in the denominator. More generally, $$(k^2) \vec{\lambda} \cdot \frac{\b{k}}{k^2} \delta(k) = 0$$ works for any vector $$\vec{\lambda}$$. Therefore we have two free terms on the right:

$$\hat{f}(\b{k}) = - \frac{\hat{g}(\b{k})}{k^2} + \lambda_1 \delta(\b{k}) + \frac{\vec{\lambda}_2 \cdot \b{k}}{k^2} \delta(k)$$

Upon inverse-Fourier-transforming these two extra terms correspond to two types of terms that have $$\del^2 = 0$$. The first term is constants $$\lambda_1$$, while the second gives a linear term like $$\vec{\lambda}_2 \cdot \b{x}$$. Unsurprisingly, the Laplacian annihilates constant and linear terms, as expected.[^argument]

[^argument]: Actually this argument seems to work for higher-dimensional polynomials as well, as long as every term has a $$\frac{1}{k^n}$$ factor in Fourier space... I don't quite know what to do with that idea though. I need to find a better table of Fourier transforms that includes crazy delta-function stuff; Wikipedia is letting me down.

In principle you can do something like this for any $$D f = g$$ differential equation:

1. Write down its Green's function solution term corresponding to $$D^{-1}(g)$$.
2. But _also_ include terms for $$D^{-1}(0)$$ (which you might find easier to compute in Fourier space)
3. and if you do that properly you should end up with a closed form for the complete space of solutions, parameterized by some set of free parameters that act like integration constants

Of course this doesn't deal with boundary conditions at all. Normally those are encoded into the definition of the correct Green's function, but they may also impose constraints on the "integration constants" that we add on at the end.

None of this had to go through Fourier space, of course, but I wanted to show it that way because it's intriguing how much everything seems to line up: the funny rules for division up above correspond exactly to the funny rules for inverting differential operators down here, if you do it right. I assume that there is some not-yet-buttoned-up "distributional calculus" that combines all of these ideas together in such a clean, intuitive way that's so simple it ought to be the thing we learn in high school instead of Leibniz's calculus. Sadly I in no way have the qualifications or competencies necessary to figure out exactly what it looks like. But I feel like I can perceive the shape of it, a bit, so that's what I'm describing here.

---------

### Final thoughts

I hope I have convinced you that inverting all of these linear operators sorta works the same way, and maybe you also have the hunch that I have: that there's some grand theory that ties all of this together and makes solving equations easy, like high-school-algebra easy, again. We can dream, right?

It feels like this article has gotten long enough, but I'll just mention that there are a few questions I'm curious to explore later.

First: is it better to "homogenize" all of these equations by e.g. converting $$A \b{x} = \b{b}$$ into $$(A, \b{b}) \cdot (\b{x}, 1) = 0$$? In exterior algebra there's a sense in which the solution to $$A \b{x} = \b{b}$$ is cleanly generated by computing 

$$\^^{n} (A, \b{b}) = (\^^{n-1} A, \^^n A)$$

The latter term $$\^^n A$$ is the same as $$\det A$$, and when you set this equal to $$(\b{x}, 1)$$ it implies that you should divide the last term through to get $$A^{-1} = \frac{\^^{n-1} A}{\det A}$$, which is, in fact, the formula for the inverse of a matrix. But I want to go through this more carefully and figure out how to reconcile it with the "pseudoinverse", and especially figure out if a similar operation makes sense for the other types of linear problems I mentioned above.

Second: can we deal with boundary conditions the same way? For $$D f = g$$ it's normal to include a separate boundary condition term that looks like $$B f = 0$$. But in that case maybe we can treat $$(D, B) f = (g, 0)$$ and use the same homogenization procedure to solve it in one shot? Not sure how to pull that off but it would sure be nice for all of these differential equations to be solved by rote algebra.

Third: in solving $$ax = b$$ with $$a^{-1}(b) + a^{-1}(0)$$, there's an assumption that the whole space of inverses is accessible by adding free parameters to any single solution. Presumably this is not always true because the space of solutions may have some non-trivial topology that makes this impossible. (Presumably algebraic geometers have already thought about this to death). Is there possibly an rote-algebra way to make that non-trivial topology like... trivial again? Would be neat.

