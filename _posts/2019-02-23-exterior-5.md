---
layout: blog
title: "Oriented Projective Geometry"
footnotes: true
math: true
aside: true
tags: notes
---

*Part 5 of a neverending series on exterior algebra. Parts [1]({{ site.baseurl }}{% post_url 2018-10-08-exterior-1 %}), [2]({{ site.baseurl }}{% post_url 2018-10-09-exterior-2 %}), [3]({{ site.baseurl }}{% post_url 2019-01-26-exterior-3 %}), [4]({{ site.baseurl }}{% post_url 2019-02-13-exterior-4 %})*

*Vector spaces are finite-dimensional and over $$\bb{R}$$. The grade of a multivector $$\alpha$$ is written $$\| \alpha \|$$, while its magnitude will be written $$\Vert \alpha \Vert$$.*

*Disclaimer: I am not a mathematician and am basically in way over my head. I can't promise this is correct, but perhaps it is at least interesting.*

-----------

In the previous post, I looked at how some operations related to exterior algebra (wedge / 'join', 'meet', interior product, Hodge Star, geometric product) correspond roughly to set operations (union, intersection, subtraction, complement, symmetric difference), but the analogy wasn't very good. Exterior algebra isn't _exactly_ linearized set algebra. So what is it?

I figured out the answer: it's linearized [oriented projective geometry](https://en.wikipedia.org/wiki/Oriented_projective_geometry) (henceforth 'OPG'). You could also say this the opposite way: OPG is exterior algebra if you forget about all the scalars. Either way they're related. OPG, incidentally, holds the mantle of "the obscure math topic I think is most likely to someday be mainstream". It just needs a really good application to get there.

I read through (most of) "Oriented Projective Geometry" by Jorge Stolfi (1991), which is a great book that I highly recommend, though it could really use more concrete examples. Stolfi was at least aware of the relationship with exterior algebra also, and he cited the same Barnabei/Brini/Rota paper I've been looking at. He also had some better notations and a few ideas which I should have been aware of. It will take way too much space to fully explain projective geometry here, or to do justice to the oriented version, but I will mention the key ideas needed for the comparison to exterior algebra.

<!--more-->

-------

## 1. The Geometry

The oriented projective space $$\bb{O}^n$$ (also called a "two-sided space") is a modification of $$\bb{R}^n$$ which associates to each point a sign $$\pm$$ indicating whether it is _positively_ or _negatively_ oriented[^name]. Conceptually, there are two copies of $$\bb{R}^n$$ _overlaid_, one of positive points and one of negative points, which you can think of as "going into" or "going out of" the page. For any point $$p$$, we can denote its negatively-oriented counterpart (called its 'antipode') as $$\neg p$$.

[^name]: I feel like $$\bb{O}^n$$ is a better name than Stolfi's $$\b{T}_n$$.

Connecting these two spaces is a "horizon" or "hyperplane at infinity" which causes each direction to 'wrap around': $$+ \infty = \neg (- \infty)$$. It's two points at infinity in $$\bb{O}^1$$, gluing two real lines together. In $$\bb{O}^2$$, it's a circle, gluing the 'borders' of two planes together, etc. The result is that $$\bb{O}^n$$ is topologically equivalent to an $$n$$-sphere.

Examples: $$\bb{O}^0$$ is two points, with opposite orientations. $$\bb{O}^1$$ is two copies of the real line, with the positive infinities of each linked to the negative infinities of the other, resulting in a topological circle:

{% include image.html filename="2019-02-23/O1.png" width="400px" %}


$$\bb{O}^2$$ is two copies of the real plane, which we may picture as being spread over two hemispheres of a sphere, with antipodal points being oppositely oriented (hence the name 'antipode'), and the equator that joins them as the line at infinity.

This model turns out to be useful when dealing with properties of oriented geometric objects, basically because if you are considering the geometric relationships of oriented shapes, like "lines with directions" or "clockwise circles", you are already working in this space even if you don't realize it. It turns out to be a good representation for answering questions like "what side of a line is a point on?" or "is a point contained in figure?" or "what is the intersection of two planes?", and is ubiquitous (in one form or another) in computer graphics and anything else dealing with representing spatial geometry.

----------

### Simplexes

The simplest representation of the points of $$\bb{O}^n$$ is as _directions_ in $$\bb{R}^{n+1}$$, which are called [homogenous coordinates](https://en.wikipedia.org/wiki/Homogeneous_coordinates). 

An example to show why you would want to do this: recall that we can define lines in $$\bb{R}^2$$ by an equation like $$(a,b) \cdot \b{x} = c$$. We can adjust this to look like it as a vector by writing $$(a,b,-c) \cdot (\b{x}, 1) = 0$$. Any scalar multiple of $$(\b{x}, 1}$$), where $$\b{x}$$ is on the line, will solve this equation. Since scalar multiples don't change the result, this is a space of _directions_ in $$\bb{R}^{3}$$. Likewise, any scalar multiple of $$(a,b,-c)$$, such as $$(a/c, b/c, -1)$$ (if $$c > 0$$), will refer to the same line.

The idea behind not allowing division by negative numbers is that points, lines, etc maintain _orientations_. It's not hard to imagine why we might want to keep track of the direction of a line, but points are a little less obvious. For a simple argument why you might care about oriented points, consider that a 1d slice of an oriented line seems to be a point that goes 'into' or 'out of' the page, depending on the direction of the line, and this information might be needed if, say, you want a calculation in 2d to hold if you take a 1d slice of the whole system. But there will be better motivations later.

Thus, a point in $$\bb{O}^n$$ is defined as equivalence class of vectors in $$\bb{R}^{n+1}$$, where two vectors are equivalent if they differ by positive scalar factor[^rp]. We use _positive_ scalar multiples because, for instance, $$(a,b,c)$$ and $$(-a,-b,-c)$$ refer to different _directions_ in $$\bb{R}^{3}$$, and thus (by definition) to different points in $$\bb{O}^2$$.

These refer to the same point in $$\bb{O}^2$$, assuming all three $$(w,x,y)$$ are positive:

[^rp]: Regular projective geometry omits the requirement the factor be positive. The result is an unorientable space with unintuitive properties. Nevertheless, it is ubiquitous in higher mathematics. I suspect that was an accident, just like the ubiquity of $$\bb{C}$$.

$$(w,x,y) \sim (2w,2x,2y) \sim (w/y, x/y, 1) \sim (1, x/w, y/w) \sim (w/x, 1, y/x)$$

As long as $$w \neq 0$$, this is still the same point:

$$(w,x,y) \sim (\sgn(w), x / | w |, y / |w|)$$

These are all different points, though:

$$(w,x,y) \nsim (w,x,-y) \nsim (-w, -x, -y) \nsim (w/y, x/y, 0)$$


Clearly the relevant information to define a point in $$\bb{O}^2$$ is two real numbers $$(x,y)$$ and a 'sign flag', which can be $$\{+1, 0, -1\}$$. It's often easiest to just not divide through, when doing calculations, to avoid having to be careful about signs and absolute values. I like to write the sign flag as a $$w$$-coordinate, in the first position, so that it is the first index in any dimension.

$$(\sgn, x,y) \in \bb{O}^2$$

$$\sgn = +1$$ refers to 'positive points', $$\sgn = -1$$ to 'negative points', and $$\sgn = 0$$ to points at infinity in the direction of $$(x,y)$$. This means that $$(0, x,y) \sim (0, x/y, 1)$$ also, which shows that the hyperplane at infinity in $$\bb{O}^n$$ is itself a copy of $$\bb{O}^{n-1}$$ (in fact, any hyperplane is).

To visualize this embedding for, say, $$\bb{O}^1$$: consider the real plane $$\bb{R}^2$$ with vertical lines drawn at $$x=1$$ and $$x=-1$$, which will be the "front" and "back" parts of $$\bb{O}^1$$. Points in $$\bb{O}^1$$ correspond to "directions" in $$\bb{R}^2$$, which are determined by taking vectors $$(x,y)$$ and seeing where they intersect one of the two vertical lines. If $$y > 0$$, the intersection point is $$(1, x/y)$$, on the front line; if $$y < 0$$, the point is $$(-1, x/y)$$ on the "back line". Like this:

{% include image.html filename="2019-02-23/Graph.png" width="400px" %}

The point $$(0, x)$$ maps to $$+\infty$$, which is the same as $$\neg (-\infty)$$, if $$x > 0$$ and $$-\infty$$ if $$x < 0$$. The point $$(0,0) \in \bb{R}^2$$ doesn't correspond to anything in $$\bb{O}^1$$, and is considered undefined or null.

With a bit of work you learn to think in this model. When you study linear algebra you become used to considering every direction from the origin as a unique 'direction', and you intuit that vectors are linearly independent if they lay in the same subspace. In OPG you think of every _point_ as a unique direction (say, from your eyes), and their spans are lines/planes/higher-dimensional analogues (called 'affine subspaces') which contain them, and linear independence is when they aren't contained in the same line/plane/affine subspace.

Basically: when talking about the geometry of shapes, the origin really _isn't special_, and thus we do geometry in a way that doesn't privilege it.

------

### Flats

In OPG an oriented hypersurface is called a 'flat': an oriented point, line, plane, etc. For each flat $$a$$ is there is an _opposite_ flat, written $$\neg a$$, consisting of the same object with the opposite orientation. A flat's _rank_ is how many points are required to define it: a point has rank $$1$$, a line rank $$2$$, etc (it's the dimension of the figure plus one, or the dimension of the figure in the $$\bb{R}^{n+1}$$ model). We'll refer to flats of rank $$k$$ as $$k$$-flats. There are also two "empty" flats of rank $$0$$, called $$\Lambda$$ and $$\neg \Lambda$$, which are like oriented empty sets.

A _simplex_ is a list of points, written $$(abc\ldots)$$ or $$(a,b,c, \ldots)$$. A _proper_ simplex is one whose points are linearly independent, which means that the simplex does not contain any duplicates, nor oppositely-oriented pairs, nor three points in a line, etc. This is necessary to determine a flat uniquely" two points determines a line, three determines a plane, etc so long. Three collinear points does not determine a plane, of course. Nor does $$(pp)$$ determine a line, and $$(p, \neg p)$$ cannot either, because despite referring to two different points, we don't know which way it goes to get from one to the other.

A proper $$k$$-simplex has $$k+1$$ points in it, because it determines a $$k$$-flat -- so a $$0$$-simplex is a point, a $$1$$-simplex is a line, etc. (This turns out to be a lot less confusing than if a $$k$$-simplex were to have $$k$$-points in it.) We'll almost always just say 'simplex' when we mean 'proper simplex' because we don't care about improper simplexes.

Basically, flats are generalized oriented lines and proper simplexes (simplices?) are generalized oriented line segments (triangles, tetrahedrons, etc). Flats are usually the objects of study, but may be referred to by any simplex which defines them, and computations take place with a choice of simplex and should give the same result for any valid choice of simplex for each flat.

The $$\neg$$ symbol reverses orientations of simplexes, and acts sort of like a minus sign. In the vector representation of points, it literally is one: $$\neg(1,x,y) = (-1,-x,-y)$$. Negating a simplex negates the orientation flat it refers to. The same representation can be reached by swapping any two of its points, or negating all of them:

$$\begin{aligned}
(p,q) &= (\neg p, \neg q) \\
&= \neg (q, p) \\
&= \neg (\neg q, \neg p)
\end{aligned}$$

-------

### Operations on Flats

Flats support the operation of _join_ $$\v$$, which is an oriented version of set union[^span]. Join just appends the simplex representations as lists, so long as none of the points lay in the same hyperplane as the others (ie, $$k+1$$ points should _always_ describe a $$k$$-dimensional figure). For example:

[^span]: Unions of the simplex representation, that is. This corresponds to the 'span' operation on subspaces; the union of two subspaces when considered as sets of points is not a useful concept because it's not a subspace at all.

$$(p,q) \v (r,s) = \begin{cases} (p,q,r,s) & \if p,q,r,s \text{ linearly independent} \\ 0 & \text{ otherwise} \end{cases}$$

Join is associative and "rank commutative": 

$$\begin{aligned}
p \v (q \v r) &= (p \v q) \v r = p \v q \v r \\
p \v q &= \neg^{rank(p)rank(q)} q \v p
\end{aligned}$$

Which side of $$\bb{O}^n$$ is the 'positive' side may be specified by supplying a _global orientation_ flat $$\Omega$$, also called a 'universe', which is any oriented $$n$$-flat (there are only two choices anyway).

Flats also support the operation of _meet_ $$\^$$ relative to the global orientation, which is an oriented version of intersection. It's given by:

$$\begin{aligned}
\alpha \^ \beta &= \begin{cases} q & \exists\, p, q, r : \\
& p \v q \v r = \Omega \\
& p \v q = \alpha, q \v r = \beta \\
0 & \text{ otherwise} \end{cases}
\end{aligned}$$

Meet is associative and "corank commutative", where the "corank" of a simplex in $$\bb{O}^n$$ is $$corank(p) = rank(\Omega) - rank(p) = (n+1) - rank(p)$$.
:

$$\begin{aligned}
p \^ (q \^ r) &= (p \^ q) \^ r = p \^ q \^ r \\
p \^ q &= \neg^{corank(p) corank(q)} q \^ p \\
\end{aligned}$$

The complement of a flat $$x$$, written $$x^{\perp}$$, is a flat $$y$$ such that $$x \v y = \Omega$$, and of course $$rank(x^{\perp}) = corank(x)$$. If a flat $$x$$ is contained in a flat $$f$$, we can also define the complement with respect to $$f$$ as a flat $$y$$ such that:

$$x^{\perp f} = \begin{cases} y & \exists \, y : x \v y = f \\ 0 & \text{ otherwise} \end{cases}$$

---------

### Summarizing

This is all _exactly the same_ as exterior algebra.

Simplexes are multivectors. In OPG they are being used to represent the geometric object spanned by the points, which is some line/plane/etc in space. $$\neg$$ is $$-1$$. Joins are joins (wedge products). Meets are meets. $$\Omega$$ is $$\omega$$. Complements are Hodge Stars and interior products. $$corank(p)$$ is $$\| {\star p} \|$$.

So that's fun.

(By the way, so we don't get confused, in this article $$\v$$ will refer to the join/wedge product (what I normally write $$\^$$) and $$\^$$ will refer to the meet.)

Note that it's the vector representation in $$\bb{R}^{n+1}$$ that is isomorphic to EA. For instance the simplex created by $$(2 \b{x}, \b{y})$$ is equivalent to the multivector $$(1,2,0) \v (1,0,1) = (-2,2,-1) \sim (-1, 1, -\frac{1}{2})$$ and not $$(2,0) \v (0,1) = 2 \b{x \v y}$$.

In OPG, vectors describe points and the join of two points (vectors) is a line (bivector). In EA, vectors describe lines, and the join of two points is a plane (bivector). So the EA on a vector space $$\bb{R}^n$$ is 'really' the OPG $$\bb{O}^{n-1}$$. But the arithmetic is all the same, and we could represent $$k$$-simplexes, rather than by a list of $$k$$ points $$(p,q,\ldots)$$, by a single $$k$$-vector in $$\^^k \bb{R}^n$$ -- although this would almost always require storing more data; multivectors are not at all an efficient way to store information.[^storage] Incidentally, this description of a simplex is called its [PlÃ¼cker Coordinates](https://en.wikipedia.org/wiki/Pl%C3%BCcker_coordinates). It is perhaps amusing that they're literally just multivector components with a different name.

[^storage]: $$\v^2 V$$ has $${n \choose 2} = \frac{1}{2}n(n-1)$$ components in general. If you can store a bivector as two vectors, with $$2n$$ components, do it. Storing the full bivector with its $$O(n^2)$$ components is only necessary when you don't have a vector factorization.

--------

## 2. Good Ideas from OPG

There are a few things in Stolfi's book that I think are obviously good and worth importing into my understanding of EA (and some things obviously missing from OPG that EA knows about, but I'm not as concerned with that direction).

-------

**1**. It is useful to define meets with respect to any set, not just the whole vector space. The "meet of $$a$$ and $$b$$ in $$u$$" is a multivector $$y$$ such that there exist multivectors $$x,y$$ with:

$$\begin{aligned}
x \v y \v z &= u \\
x \v y &= a \\
y \v z &= b 
\end{aligned}$$

Then $$y = a \^_u b$$. (This is probably a better way to intuit what the meet does, anyway; I should have maybe written it this way last time...) The regular meet operation is just this generalized meet with respect to the universe/pseudoscalar: $$a \^ b \equiv a \^_{\Omega} b$$.

The commutativity law now uses coranks "relative to $$u$$":

$$a \^ b = \neg^{corank_u(a) corank_u(b)} b \^ a$$

By the way, $$x^{\perp}$$ is definitely a better notation than $$\star x$$. Though it is helpful to have it as a prefix operator, so maybe we should write it as $$\perp x$$. And capital omega $$\Omega$$, which really looks like "omega" and evokes feelings of "finality" and "totality", is definitely better than lower-case omega $$\omega$$, which just looks like a weak "w". And the name "universe" is definitely better than "pseudoscalar".

-----

**2**. We can also define _joins_ with respect to any flat/multivector. This one is a bit weirder. Basically, if you have two flats (multivectors) $$v \v x$$ and $$v \v y$$, you can define their "join relative to $$v$$" ("relative join"), as:

$$(v \v x) \v_v (v \v y) = v \v x \v y$$

This is the dual operation to the "meet in a flat". We write it as $$\alpha \v_v \beta$$, and we can compute it (with $$v^{-1} = \frac{v}{\| v^2 \|}$$):

$$\alpha \v_v \beta =  (v^{-1} \cdot \alpha) \v \beta =    \alpha \v (v^{-1} \cdot\beta )$$


----------

**3**: Remember how $$\v{\^} V$$ is the "double" algebra, consisting of two exterior algebras over the same basis multivectors space? In fact, this can be made much more general.

Let us write an exterior algebra as a tuple of a vector space and a join operation. The standard exterior algebra is $$(\bb{R}^n, \v)$$. The 'meet' algebra is $$(\^^{n-1} \bb{R}^n, \^)$$. The double algebra refers to both at once, which we may write as $$(\bb{R}^n, \v, \^)$$.

Let $$S_\alpha$$ be the space of multivectors _contained in $$\alpha$$_, for example in $$\bb{R}^3$$, $$S_{\b{x \v y}}$$ is the $$\b{x \v y}$$ plane consisting of all multivectors with no $$\b{z}$$ component. Then $$(S_\alpha, \v)$$ is an exterior algebra for any $$\alpha \in \^ \bb{R}^n$$, isomorphic to $$\^\bb{R}^{\| \alpha \|}$$, and $$(S_\alpha, \v, \^_\alpha)$$ is a double algebra in the same way.

Moreover, given any two multivectors $$\alpha, \beta$$ with $$\beta \in S_{\alpha}$$, we can define an exterior algebra and double algebra "between them", using the relative-join, so $$(S_{\alpha}, \v_{\beta}, \^_{\alpha})$$ is a double algebra also. This strikes me as very cool. But also confusing.

I am not even sure what that would look like. As an example consider the 2-dimensional space spanned by $$(\b{x \v y}, \b{x \v z})$$ inside of the 4-dimensional space spanned by $$\Omega = \b{w \v x \v y \v z}$$, where we take joins relative to $$\b{x}$$: $$(\b{x \v y}) \v_{\b{x}} (\b{x \v z}) = \b{x \v y \v z}$$ and meets relative to $$\Omega$$: $$(\b{x \v y \v z}) \^_{\Omega} (\b{x \v z \v w}) = (\b{x \v z})$$. 

Something I often think about: perhaps one of the transformations that a geometric calculus should be invariant under is the operation of "forgetting about dimensions". In this case, we see that the exterior algebra on $$\bb{R}^4$$ remains an exterior algebra if we just 'forget about $$\b{x}$$' entirely, either by literally projecting every object onto the $$\b{y \^ z \^ w}$$ plane, or -- as we've done here -- by defining operations that just pretend like it isn't there.

----

**4**: OPG points out that it is useful (especially in computer implementations) to distinguish between the $$0$$s in different vector spaces. In EA terms, this means that the $$0 \in \bb{R} \simeq \^^0 V$$ is not the same as $$0 \in \^^{2} V$$; we can distinguish them by writing $$0^k$$ for the element of $$\^^k V$$. I think this is a good idea. $$O^{\v k}$$ would be good also.

------

**5**: OPG has a useful operation I hadn't thought to write down: a boolean function for determining "relative sign" of two complementary flats / multivectors. For two multivectors $$\alpha, \beta \in V$$ with $$\| \alpha \| + \| \beta \| = n = \dim V$$, we write:

$$\alpha \diamond \beta = \sgn(\alpha \v \beta)$$

(Recall that $$\sgn$$ gives 0 if its argument is $$0$$. We of course take it to also be $$0$$ if its argument is any $$0^k$$, in the notation of the previous section.)

(I assume this can also be computed with the meet: $$\alpha \diamond \beta = \sgn(\alpha \^ \beta)$$ but I haven't checked.)

This is useful for answering questions about the relative orientations of objects. If we have a point $$(a)$$ and a line $$l = (pq)$$ in $$\bb{O}^2$$, then $$(a) \v (pq) \neq 0$$ only if $$a$$ is not on $$l$$, and the orientation of $$(apq)$$ depends on _which_ side of $$l$$ it is on. Therefore:

$$p \diamond l = \begin{cases} +1 & p \text{ is on the left side of } l \\
0 & p \text{ is on } l \\
-1 & p \text{ is on the right side of } l \end{cases}$$

Which is, if you ask me, 'super elegant'.

This is one of many, many examples where OPG makes geometric questions into simple calculations (and it highlights why OPG is better than plain-old "PG". PG does not contain, on its own, a representation suitable for this kind of question, though the answer can be teased out of things if you're careful -- but what that looks like is just working in the oriented space without realizing it!)

Of course there is no absolute sense of "which side of a line you are on" if you are in a space with $$n > 2$$, but (I believe) you can also compute it with the relative meet $$\^_a$$, which gives the sidedness relative to the orientation of a plane $$a$$. Also of course similar tests exist for sidedness of any flat relative to any other; see the book.

Related: the OPG's generalized version of "two distinct points are separated by a line" is: two flats $$a,b$$ of the same rank are distinct iff there is a flat $$x$$ of complementary rank in $$\Omega$$ such that $$a \diamond x = - b \diamond x$$.

-------

There is a lot else to recommend in OPG, but these are the main ideas. I find that it constantly 'rings true', in connecting a bunch of concepts that seemed to be mysteriously related before (this is also why I like EA).

But also it has a ton of applications. For example here are some calculations.

----------

## 3. The line between two points

An oriented line in $$\bb{O}^n$$ is described by any two points $$a,b$$ on it (in the correct order), via the simplex $$(ab)$$. The value of the simplex as a multivector is (remember the first coordinate is being called $$\b{w}$$):

$$\begin{aligned}(ab) &= (1,\vec{a}) \v (1, \vec{b}) \\
&= \v^2 \begin{pmatrix} 1 & a_x & a_y \\ 1 & b_x & b_y  \end{pmatrix} \\
&= \b{w} \v (b_x - a_x) + \b{w} \v (b_y - a_y) + \vec{a} \v \vec{b} \\
&= \b{w} \v (\vec{b} - \vec{a}) + \vec{a} \v \vec{b}
\end{aligned}$$

This is a bivector $$\in \v^2 \bb{R}^3$$. We could write it also as the dual of a vector:

$$\begin{aligned} (ab) &= \star(\star(\vec{a} \v \vec{b}), \star(\vec{b} - \vec{a})) \\
&= \star(a_x b_y - a_y b_x, a_y - b_y, b_x - a_x)
\end{aligned}$$

By the way, I included the intermediate step of writing the simplex as a matrix because I think this is the best mental image for what is meant by $$(ab)$$. Probably the wedge product $$\v$$ only needs to performed when actually trying to use $$(ab)$$ in an algorithm; its 'resting state', if you will, is just the coordinates of its component points.

To test if another point $$c$$ is on the line $$(ab)$$, we see if it is linearly independent of $$(1,\vec{a})$$ and $$(1,\vec{b})$$:

$$\begin{aligned}(1, \vec{c}) \v (ab) &= (\b{w} + \vec{c}) \v  (\b{w} \v (\vec{b} - \vec{a}) + \vec{a} \v \vec{b}) \\
&= \b{w} \v \vec{a} \v \vec{b} + \vec{c} \v \b{w} \v (\vec{b} - \vec{a}) \\
&= \b{w} \v (\vec{a} \v \vec{b} + \vec{b} \v \vec{c} + \vec{c} \v \vec{a}) \\
0 &= \vec{a} \v \vec{b} + \vec{b} \v \vec{c} + \vec{c} \v \vec{a} \\
\end{aligned}$$

I've written it this way to illustrate what exactly is going on; the classical way of writing this condition is:

$$\det \begin{pmatrix} 1 & a_x & a_y \\ 1 & b_x & b_y \\ 1 & c_x & c_y \end{pmatrix} = 0^{\v 3}$$

Actually, the column of ones here is completely redundant, and we could write the same condition as:

$$\v^2 \begin{pmatrix} a_x & a_y \\ b_x & b_y \\ c_x & c_y \end{pmatrix} = 0^{\v 2}$$

This makes me wonder if, perhaps, the $$\bb{R}^{n+1}$$ representation is unnecessary in general, and maybe results from people not being as familiar with wedge powers as they should be. (It often seem to me that many ideas has been expressed in terms of determinants only because they were the best tool available at the time.)

We can convert the constraint that $$(c) \v (ab) = 0$$ into a typical equation for a line in $$\bb{R}^2$$ like so:

$$\begin{aligned}
(1, \vec{c}) \v (\b{w} \v (\vec{b} - \vec{a}) + \vec{a} \v \vec{b}) &= 0^{\v 3} \\
\b{w} \v \vec{a} \v \vec{b} + \vec{c} \v \b{w} \v (\vec{b} - \vec{a})) &= 0^{\v 3}\\
\vec{c} \v (\vec{b} - \vec{a})  &= \vec{a} \v \vec{b}\\

\vec{c} \cdot \star_{\bb{R}^2}(\vec{b} - \vec{a}) &= \star_{\bb{R}^2}(\vec{a} \v \vec{b})\\
\vec{c} \cdot (a_y - b_y,b_x - a_x) &= a_x b_y - a_y b_x \\
\end{aligned}$$

The third line of this is the best conceptual representation of the constraint for a line in the plane:

$$\vec{c} \v (\vec{b} - \vec{a})  = \vec{a} \v \vec{b}$$

This says that the area of the triangle $$\mathcal{O}ab$$ must be the same as the area of the triangle $$\mathcal{O}c(c + \vec{b} - \vec{a})$$. Since both have a side $$(\vec{b} - \vec{a})$$ parallel to the origin, this means their perpendicular to the origin must be the same length and sign, ie, $$c$$ lies on the line $$(ab)$$.

All of this generously extends to representions for any flat in any dimension and to testing its containment of any other flat.

--------

## 4. Line Intersection

The intersection point $$p$$ of two lines $$(ab)$$ and $$(cd)$$ in the plane is given by the meet, via $$(p) = (ab) \^ (cd)$$. (If they are parallel, this will be a point at infinity in their common direction.) In homogenous coordinates this is:

$$\begin{aligned}
(1, p) &\sim [(1, a) \v (1,b)] \^ [(1,c) \^ (1,d)] \\
&\sim (\vec{a} \v \vec{b}, \vec{b} - \vec{a}) \^ (\vec{c} \v \vec{d},\vec{d} - \vec{c})
\end{aligned}$$

But meets are quite tedious to calculate (lots of complements to take). I don't like using tricks, but a better way is to just work from the fact that this point will satisfy the system of equations given by each line's contraint in $$\bb{R}^2$$:

$$\begin{cases}
p \v (\vec{b} - \vec{a}) = \vec{a} \v \vec{b} \\
p \v (\vec{d} - \vec{c}) = \vec{c} \v \vec{d} \\
\end{cases}$$

From this I can read off what $$p$$ has to be:

$$p = \frac{ [\star (\vec{c} \v \vec{d})](\vec{b} - \vec{a})  -  [\star (\vec{a} \v \vec{b})] (\vec{d} - \vec{c})}{\star[(\vec{b} - \vec{a}) \v (\vec{d} - \vec{c}) ]}$$

Except, the denominator will be $$0$$ if the lines are parallel. Better to keep it in homogenous coordinates (the $$\star$$s still meanin $$\bb{R}^2$$, though):

$$(1,p) \sim (\star[(\vec{b} - \vec{a}) \v (\vec{d} - \vec{c}) ], [\star (\vec{c} \v \vec{d})](\vec{b} - \vec{a})  -  [\star (\vec{a} \v \vec{b})]  (\vec{d} - \vec{c}))$$

It's not pretty, but it works.