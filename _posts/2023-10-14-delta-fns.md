---
layout: blog
title: "Delta Function Miscellanea"
tags: math
math: true
footnotes: true
aside: true
---

I keep needing to remember some of the strange properties of Delta Functions, so here's a reference.

<!--more-->

---------

## 1. Definitional Stuff

**Quibbles about Definitions**

IMO: if all the ways of defining something give rise to the same properties, then that object "exists" and you don't really need to define it in terms of another object. Sure, you can construct a delta (distribution) as a limit of another sequence, but why would you? $$\int \delta(x) f(x) \, dx = f(0)$$ is just as good as everything else.

The most common one in physics is the definition in terms of the Fourier transform:

$$\delta(k) = \frac{1}{2 \pi}\int e^{-ikx} dx$$

But I still prefer to just claim that $$\delta(x)$$ exists without any justification. Those are just implementations of it.

I also don't care at all about the use of the word "function" vs. "generalized function" vs. "distribution". For my purposes, everything is a distribution and demanding a value at a point is (possibly) a mistake.

I'm sure I'm wrong about all my quibbles, but that's okay. I'm just trying to compress this stuff into a very small and easy-to-use part of my brain, and that's only possible if I can ignore all the technical stuff.

-----------

**Fourier Transform Interpretation**

The Fourier transform of $$f(x)$$ is of course given by:

$$\hat{f}(k) = \int f(x) e^{-ikx} dx$$

One interpretation of the Fourier transform equation is something like:

> $$e^{-ikx}$$ is an orthogonal basis for frequency-space functions and we construct $$\hat{f}(k)$$ by projecting $$f(x)$$ onto that basis, computing the projections via an inner product $$\< f(x), e^{-ikx}\>$$.

And that's fine, but I think there's an even better interpretation: 

> $$f$$ is a generic object that doesn't know anything about our choices of bases. The Fourier Transform of $$f(x)$$ is $$f$$ evaluated at $$\hat{k}$$, where $$\hat{k}$$ is a frequency-value rather than a position value, but the two bases live on equal footing and we can treat either as fundamental.

It just so happens that it's implemented as an integral transform. In particular, the transform is kinda like computing $$f \ast \delta(\hat{k})$$, a sort of "convolution-like" operation between objects in different bases. whatever that means. We could imagine imagine expressing both $$f$$ and $$\delta(\hat{k})$$ in a _third_ basis, neither position nor frequency, and that operation should still make sense.

----------

## 2. The Indicator Function $$I_x = x \delta(x)$$

Since $$\int \delta(x) f(x) \, dx = f(0)$$, we could flip this around and say that this is the _definition_ of evaluating $$f$$ at $$0$$. Or, more generally, we could say that integrating against $$\delta(x-y) \, dx$$ is "what it means" to evaluate $$f(y)$$.

This is a bit strange though. Why does evaluation require an integral? Maybe we need to define a new thing, the indicator function, which requires no integral:

$$I_x f = f(x)$$

The definition is

$$I_x = \begin{cases}
1 & y = x \\ 
0 & \text{otherwise}
\end{cases}$$

But that probably masks its distributional character. A better definition is that it's just 

$$I_x = x \delta(x)$$

Whereas $$\delta_x$$ is infinite at the origin and is defined to _integrate_ to $$1$$, the $$I_x$$ function is just required to _equal_ one at the origin. Of course, its integral is $$0$$. It could also be constructed like this:

$$I_x = \lim_{\e \ra 0^{+}} \theta(x - \e) - \theta(x + \e)$$

Compare to the $$\delta$$ version: $$\delta_x = \lim_{\e \ra 0^{+}} \frac{1}{x} [  \theta(x - \e) - \theta(x + \e)] \stackrel{?}{=} \P(\frac{I_x}{x})$$.

By either definition, $$I_x$$ should have to have $$\p_x I_x = \delta(x) - \delta(-x) = 0$$ and $$\p_x (x \delta(x)) = \delta(x) + x \delta'(x) = \delta(x) - \delta(x) = 0$$. Compare to $$\p_x \delta(x) = \delta'(x) = -\frac{\delta(x)}{x}$$. It _sorta_ seems like there is some even _further_ generalization of distributions which could distinguish this derivative from $$0$$, since obviously $$x \delta(x)$$ is not, in fact, constant at $$x = 0$$. It would require that $$\delta(x) \neq \delta(-x)$$ anymore, though, which is definitely tricky.

$$I_x$$ also has this delta-function-like property:

$$\int I_x \frac{f(x)}{x} dx = f(0)$$

in a "principal value" sense, of course. And it's natural to make a whole family $$x^k \delta(x)$$ of these. Whereas $$\delta^{(n)}(x)$$ functions produce derivatives: $$\delta^{(n)} f(x) = (-1)^{n} f^{(n)}(0)$$, I guess these $$x^n \delta(x)$$ functions produce... integrals? But normally (cf contour integration) integrals add up contributions from (a) boundaries and (b) poles (and really poles are a kind of boundary, topologically). These $$x^n \delta(x)$$ terms only add up the constributions from poles. (Well! in a sense, the $$x^{-n} \delta(x) \propto \delta^{(n)}(x)$$ family are doing exactly the same thing except that they're only concerned with the poles at infinity.)

I like this $$I_x$$ object. It seems fundamental. Maybe we should just write $$f(x) = I_x f$$ all the time. And look how nice the Fourier transforms look!

$$\begin{aligned}
\F(I_x f) &\stackrel{?}{=} f \int I_x e^{-ikx} dx \\
&= I_k f \\
&= \hat{f}(k)
\end{aligned}$$

Maybe that's cheating. But there is something elegant about it. There is a certain elegance to the notion that $$\F(x \delta(x)) = k \delta(k)$$. Both are "pairs of important objects in the theory". It has a certain [Parseval](https://en.wikipedia.org/wiki/Parseval%27s_theorem) nature to it. Indeed, really there are three objects in the theory: $$\{ x, \delta(x), \p_x \}$$... which all obey different rules. But in some sense you can construct $$\p_x$$ out of $$\delta(x)/x$$ so maybe it's really just two still.

----------

## 3. Derivatives of $$\delta$$ act like division.

I always end up needing to look this up.

The rule for derivatives of delta functions are most easily found by comparing their Fourier transforms. Since $$\F(\p_x x^n) = \F(n x^{n-1})$$ then:

$$\begin{aligned}
\F(\p_x x^n ) &= \F(n x^{n-1} ) \\
(ik) (i \p_k)^{n} \delta_k &= n (i \p_k)^{n-1} \delta_k \\
- k \delta_k^{(n)} &= n \delta_k^{(n-1)} \\
\end{aligned}$$

So in particular:

$$\begin{aligned}
-x \delta'_x &= \delta_x \\
-x \delta^{(2)}_x &= 2 \delta^{(1)}_x \\
x^2 \delta^{(x)}_x &= 2 \delta_x \\
&\vdots \\
(-x)^n \delta^{(n)}_x &= n! \delta_x \\
\delta^{(n)}_x &= n!  \frac{  \delta_x}{(-x)^n}
\end{aligned}$$

Or, if you prefer:

$$\delta^{(n)}_x f(x) = (n! )\delta_x \frac{ f^{(n)}(x)}{x^n}$$

Etc. It's easy to forget that that $$n!$$ coefficient shows up there!

(If you use these, probably make sure everything has a $$\P$$ around it.)

---------

## 4. Zeroes of $$\delta$$ count poles

I always end up wanting to look this up too.

Since $$\delta(x)$$ integrates $$f(x)$$ to $$f(0)$$ at _every_ zero of the $$\delta$$, we of course have, via $$u = g(x)$$ substitution:

$$\begin{aligned}
\int \delta(g(x)) f(g(x)) \, dg(x) &= \int \delta(u) f(u) \, du \\
&=f(u)_{u = 0} \\
&=f(g^{-1}(0)) \\
\int \delta(g(x)) f(g(x)) \| g'(x) \| dx &= f(g^{-1}(0)) \\
\delta(g(x)) &= \sum_{x_0 \in g^{-1}(0)} \frac{\delta(x - x_0)}{\| g'(x_0) \|} \\
\end{aligned}$$

For instance:

$$\delta(x^2 - a^2) = \frac{\delta(x - a)}{2\| a \|} + \frac{\delta(x + a)}{2\| a \|}$$

Somewhat harder to remember is the multivariable version:

$$\begin{aligned}\int f(g(\b{x})) \delta(g( \b{x})) \| \det \del g(\b{x}) \| d^n \b{x}
&= \int_{g(\bb{R})} \delta(\b{u}) f(\b{u}) d \b{u} \\
\int f(\b{x}) \delta(g (\b{x})) d \b{x} &= \int_{\sigma = g^{-1}(0)} \frac{f(\b{x})}{\| \nabla g(\b{x}) \|} d\sigma(\b{x}) \\
\end{aligned}$$

Where the final integral is in some imaginary coordinates on the zeroes of $$g(\b{x})$$.

In general there is a giant model of "delta functions for surface integrals" which I've never quite wrapped my head around, but intend to tackle in a later article. Basically there's a sense in which every line and surface integral, etc, can be modeled as an appropriate delta function. Wikipedia doesn't talk about it much. There's a couple lines on the delta function page, but there's quite a more for some reason on the page for [Laplacian of the Indicator](https://en.wikipedia.org/wiki/Laplacian_of_the_indicator).


I'd also love to understand the version of this for vector- or tensor-valued functions as well. What goes in the denominator? Some kind of non-scalar object? Weird.


----------

## 5. $$\delta$$ is weirder in other coordinates

I am often reminding myself how $$\delta$$ acts in spherical coordinates.

It is useful to think about $$\delta$$ as being defined like this:

$$\delta(x) = \frac{1_{x =0 }}{dx}$$

In the sense that it is designed to perfectly cancel out $$dx$$ terms in integral. Actually, maybe a better way to write it would be $$\delta(x) = \frac{1_{x =0 }}{dx}$$. It's $$0$$ everywhere, except at the origin where it perfectly cancels $$dx$$.

So $$\delta$$ always transforms like the _inverse_ of how $$dx$$ transforms. If you write $$dg(x) = \| g'(x) \| dx$$, then of course $$\delta$$ transforms as 

$$\delta(g(x)) = \frac{\delta(x - g^{-1}(0))}{\| g'(x) \|}$$

This, at least, makes it easy to figure out what happens in other coordinate systems.

By the way. The notation $$\delta^3(\b{x})$$ customarily means that the function is _separatable_ into all the individual variables: $$\delta^3(\b{x}) = \delta(x) \delta(y) \delta(z)$$. In other coordinate systems this _doesn't_ work!

Here's spherical coordinates:

$$\begin{aligned}
\iiint_{\bb{R}^3} \delta^3(\b{x}) f(\b{x})  d^3 \b{x} &= f(x=0,y=0,z=0) \\
f(r=0, \theta=0, \phi=0)
&= \int_0^{2 pi} \int_{-\pi}^\pi \int_0^\infty \frac{\delta(r, \theta, \phi)}{r^2 \sin \theta} f(r, \theta, \phi) r^2 \sin \theta \, dr \, d \theta \, d \phi \\
&=  \int_{-\pi}^\pi \int_0^\infty \frac{\delta(r, \theta)}{2 \pi r^2 \sin \theta} f(r, \theta, 0) ( 2 \pi r^2 \sin \theta)  \, dr \, d \theta \\
&= \int_0^\infty \frac{\delta(r)}{4 \pi r^2 } f(r, 0, 0) ( 4 \pi r^2 )  \, dr \\
&= f(0,0,0)
\end{aligned}$$

So:

$$\begin{aligned}
\delta(x, y, z) &=  \frac{\delta(r, \theta, \phi)}{r^2 \sin \theta} \\
&= \frac{\delta(r, \theta)}{2 \pi r^2 \sin \theta} \\
&= \frac{\delta(r)}{4 \pi r^2 } 
\end{aligned}$$

There is some trickiness to all this, though. Be careful: the $$r$$ integral is from $$(0, \infty)$$ instead of the conventional $$(-\infty, \infty)$$. Sometimes identities that you're used to working won't work the same way if you are dealing with $$\delta(r)$$ as a result. Also, it's very unusual, but not impossible I suppose, to have functions that have a non-trivial $$\theta$$ dependence as $$r \ra 0$$. I have no idea what that would be mean and I don't know how to handle it with a delta function.

You can also write it in this weird way:

$$\delta(x,y,z) = \frac{\delta(r, \cos \theta, \phi)}{r^2}$$

Where the $$\cos \theta$$ causes the $$\sin \theta$$ in the denominator to disappear.

Here's the polar / cylindrical coordinate version:

$$\delta^2(x, y) = \frac{\delta(r, \theta)}{r} = \frac{\delta(r)}{2 \pi r}$$

Evidently in $$\bb{R}^n$$, the numerators are related to the surface areas of [n-spheres](https://en.wikipedia.org/wiki/N-sphere).

------

## 6. Miscellaneous Breadcrumbs

Things I want to remember but don't have much to say about:

There is a thing called a [Wavefront Set](https://en.wikipedia.org/wiki/Wave_front_set) that comes from the subfield of "microlocal analysis". It allows 'characterizing' singularities in a way that, for instance, would extract which dimensions a delta function product like $$\delta(x) \delta(y)$$ is acting in. 

Among other things, the Wavefront Set allows you to say when multiplying distributions together is well-behaved: as I understand it, they have to not have singularities "in the same directions". $$\delta(x) \delta(x)$$, for instance, is not allowed. (I bet $$(x \delta)^2$$ is, though.) Here's a [nice paper on the subject](https://arxiv.org/abs/1404.1778).

There are further generalizations of functions called [hyperfunctions](https://en.wikipedia.org/wiki/Hyperfunction), which are instead defined in terms of "the difference of two holomorphic functions on a line" (which can be e.g. a pole at the origin). Gut reaction: relies on complex analysis, which sounds annoying.

A [current](https://en.wikipedia.org/wiki/Current_(mathematics)) is a differential-form distribution on a manifold. Some day I'm going to have to learn about those, but for now, nah, I'm good.