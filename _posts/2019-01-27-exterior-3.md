---
layout: blog
title: "EA Notes 3: The Hodge Star and Interior Product"
footnotes: true
math: true
aside: true
tags: notes
---

*Previously: [matrices]({{ site.baseurl }}{% post_url 2018-10-08-exterior-1 %}) and [inner products]({{ site.baseurl }}{% post_url 2018-10-09-exterior-2 %}) on exterior algebras.*

*Vector spaces are assumed to be finite-dimensional and over $$\bb{R}$$. The grade of a multivector $$\alpha$$ will be written $$\| \alpha \|$$, while its magnitude will be written $$\Vert \alpha \Vert$$. Bold letters like $$\b{u}$$ will refer to (grade-1) vectors, while Greek letters like $$\alpha$$ refer to arbitrary multivectors with grade $$\| \alpha \|$$.*

*For reasons which will be explained a few posts in the future, I use the symbol $$\v$$ for the exterior product instead of the more common $$\^$$.*

<!--more-->

More notes on exterior algebra. This time, the Hodge Star operator $$\star \alpha$$ and the interior product $$\alpha \cdot \beta$$.

-----------


## 1. The Hodge Star

In $$\bb{R}^3$$, we can identify each bivector like $$\b{x \v y}$$ with a vector $$\b{z}$$ that is orthogonal to it with the same magnitude. Actually, we can identify it with two directions: $$\b{z}$$ and $$-\b{z}$$ are both orthogonal to $$\b{x \v y}$$. To have a single choice, we need to define a global orientation on the space, which tells us which ordering of $$(\b{x,y,z})$$ is 'standard'.

We pick a global orientation by just... picking one. Normally, that's $$\b{x \v y \v y}$$ because it's in alphabetical order, but this is arbitrary. This is exactly the same as picking whether you want to compute your cross products with the 'right-hand rule' versus the 'left-hand rule'. Right is standard, which identifies the fact that if you fist with your right hand, with thumb pointing up, the curl of your fingers matches the direction of $$\b{x} \ra \b{y}$$ and your thumb matches $$\b{z}$$.

If it seems odd that the universe would care about which hand you do math with, that's because it is. If you avoid ever using cross products and instead stick with the bivectors, you don't have to pick. (This amounts to not picking whether $$\b{x \v y}$$ or $$\b{y \v x}$$ is the 'unit' bivector.)

When we pick a canonical element like this, we write it as $$\omega$$ or $$\omega_V$$ if we need to distinguish which space it's in. So in $$\bb{R}^3$$, the standard choice is $$\omega = \b{x \v y \v z}$$. We call this the 'unit pseudoscalar' or the 'unit volume' or the 'volume element'. (Elements of $$\v^n V$$ in general are called 'pseudoscalars', since they have scalar coefficients but do change under coordinate transformations.)[^integral]

[^integral]: In some texts, particularly those of Gian-Carlo Rota, $$\omega$$ is called the _Integral_. I think that's a terrible name. He also calls multivectors 'extensors'. See for instance his "On the Exterior Calculus of Invariant Theory", which is otherwise very useful except for all the bad names.

Since $${ n \choose k } = { n \choose n-k}$$, $$\v^k V$$ and $$\v^{n-k} V$$ always have the same dimension, and we can use $$\omega$$ to define an isomorphism between them. We define the [Hodge Star](https://en.wikipedia.org/wiki/Hodge_star_operator) of a multivector $$\alpha \in \v^k V$$ to be the multivector $$\star \alpha \in \v^{n-k} V$$ such that, for all $$\beta \in \v^k V$$,

$$\star \alpha \v \beta = \< \alpha, \beta \> \omega \tag{1}$$

Conceptually, $$\star \alpha$$ is "everything except $$\alpha$$" or "the complementary subspace to $$\alpha$$. For instance $$\star \b{x} = \b{y \v z}$$ in $$\bb{R}^3$$.


Note that if you are keeping track of the difference between $$V$$ and its dual $$V*$$, you probably want $$\star$$ to take a multivector to a dual multivector: $$\star: \v^k V \ra \v^{n-k} V^*$$. This follows from the definition in terms of the inner product.

----

_Hodge Star_ is a terrible name. It should be called the "complement" or something that doesn't have so much baggage. It's also called the "Hodge Dual" sometimes; whatever, that's no better.

The notation is also not great. I'd prefer something like $$\bar{\alpha}$$, again keeping with the 'complement' idea. But I'll concede that it's useful to have it as a prefix operator, because sometimes it takes complicated arguments that would have to fit under the bar. Some people also write $$\ast \alpha$$ or $$\vert \alpha$$ or even ~$$\alpha$$ instead.

Wikipedia will tell you that the main purpose of $$\star$$ is to define, uh, de Rham cohomology on differential manifolds. Wikipedia is silly and that was clearly written by a differential geometer. The main purpose of $$\star$$ is to be able to do geometry and physics for 150 years while pretending that multivectors don't exist. Anything that comes from a cross product (areas, magnetic fields, angular momenta) is bivector-valued.

----------

### A few identities

$$\star^2$$ can be computed by considering how many terms each element of $$\star^2 \alpha$$ must move past to be rearranged into the starting order. The general form turns out to be:

$$\boxed{\star^2 \alpha = (-1)^{|\alpha||{\star \alpha}|} \alpha} \tag{2}$$

Mostly I prefer to define things without minus signs everywhere, but this one is, unfortunately, unavoidable. In manipulations this particular negative-sign factor $$(-1)^{\|\alpha\|\|{\star \alpha}\|} $$ comes up a lot, and we'll often just write $$\star^2 \alpha$$ to abbreviate it. 

Clearly $$\star^2 = I$$ is always true for $$n$$ odd, and for $$n$$ even is true if $$k$$ is also even. So $$\star^2 = - I$$ if and only if ($$n$$ even, $$k$$ odd). Most commonly it comes up for $$(n=4, k=1)$$, ie, the complement of a vector in spacetime.

We can also write down the Hodge Star's inverse:

$$\star^{-1} \alpha = (-1)^{|\alpha|| \star \alpha |} (\star \alpha) = \star^3 \alpha$$

Here's the inner product in terms of the Hodge star:

$$\< \alpha, \beta \> = \star (\star \alpha \v \beta) $$

The complement operation doesn't change the inner product of two $$k$$-vectors:

$$\begin{aligned} \< \star \alpha, \star \beta \> \omega &= \star^2 \alpha \v \star \beta \\
&= (-1)^{\| \star \beta \| \| \alpha \|} (-1)^{\| \alpha \| \| \star \alpha \|} {\star \beta \v \alpha} \\
&= (-1)^{2(n-k)k}  \< \alpha, \beta \>\omega  \\
&= \< \alpha, \beta \> \omega \\
\< \star \alpha, \star \beta \> &= \boxed{\< \alpha, \beta \>}
\tag{3}
\end{aligned} $$

-------

## 2. The Cross Product

In $$\bb{R}^3$$, the vector cross product takes two vectors and produces a third vector, orthogonal to them. This is better understood as taking their wedge product, then mapping that to a vector:

$$\b{a \times b} = \star(\b{a \v b})$$

I say 'better understood' because this understanding elucidates properties such as how as a cross product [transforms](https://en.wikipedia.org/wiki/Pseudovector) under coordinate transformations. And if you just stick with the bivector, you don't have to worry about the right hand rule either.

We can quickly compute the inner product of two cross products using (3):

$$\begin{aligned}
\< \b{a \times b}, \b{c \times d} \> &= \< \star (\b{a \v b}), \star (\b{c \v d}) \> \\
&= \< \b{a \v b}, \b{c \v d} \> \\
&= (\b{a} \cdot \b{c}) (\b{b \cdot d}) - (\b{a \cdot d}) (\b{b \cdot c})
\end{aligned}$$

But now that we have better tools, we can do the more complicated ones: 

Here's the [scalar triple product](https://en.wikipedia.org/wiki/Triple_product#Scalar_triple_product), using identity (3):

$$\begin{aligned}
\b{a \cdot (b \times c)} &=  \b{a} \cdot \star (\b{b \v c}) \\
&= \star(\b{b \v c \v a}) \\
&= \star(\b{a \v b \v c})
\end{aligned}$$

The [vector triple product](https://en.wikipedia.org/wiki/Triple_product#Vector_triple_product), using (3):

$$\begin{aligned}
\tag{shit i need interiors}
\b{a \times (b \times c)} &= \star(\b{a} \v \star(\b{b \v c})) \\
&= (-1)^{| \b{a}| |\star(\b{b \v c})|}  \b{a} \cdot \star^2 (\b{b \v c}) \\
&= - \b{a} \cdot (\b{b \v c}) \\
&= (\b{a} \cdot \b{c}) \b{b} - (\b{a} \cdot \b{b}) \b{c}
\end{aligned}$$

The [quadruple product](https://en.wikipedia.org/wiki/Quadruple_product), via the previous two:

$$\begin{aligned}
(\b{a \times b}) \times (\b{c \times d}) &= ((\b{a \times b}) \cdot \b{d}) \b{c} - ((\b{a \times b}) \cdot \b{c}) \b{d} \\
&= \star(\b{a \v b \v d}) \b{c} - \star(\b{a \v b \v c}) \b{d}
\end{aligned}$$

The [Jacobi Identity](https://en.wikipedia.org/wiki/Jacobi_identity):

$$\begin{aligned}
0 &= \b{a \times (b \times c)} + \b{b \times (c \times a)} + \b{c \times (a \times b)} \\

&= -{\star( \b{a} \cdot (\b{b \v c}) + \b{b} \cdot (\b{c \v a}) + \b{c} \cdot (\b{a \v b}))} \\
&= -{\star( (\b{a \cdot b} - \b{b \cdot a}) \b{c} + (\b{b \cdot c - c \cdot b}) \b{a} + (\b{c \cdot a - a \cdot c}) \b{b})} \\
&= 0
\end{aligned}$$

The Jacobi Identity can also be rearranged into the following intriguing form, which we will contend with in a future article (the second line is our previous expansion of $$\b{a} \cdot (\b{b \v c})$$. How could these be equal?):

$$\begin{aligned}
\b{a} \cdot (\b{b \v c}) &= \b{b} \cdot (\b{a \v c}) - \b{c} \cdot (\b{a \v b}) \\
 &\stackrel{??}{=} (\b{b \cdot a}) \b{c} - (\b{c \cdot a}) \b{b}
\end{aligned}$$

---------

## 3. Application: Matrix Inversion

I mentioned that you can define the complement and the inner product in terms of each other, via:

$$\un{k}{\alpha} \cdot \un{k}{\beta} = \star(\alpha \v \star \beta) = \star(\beta \v \star \alpha)$$

This just says that the wedge product between grade-$$1$$ and grade-$$(n-1)$$ vectors looks like a dot product:

$$(a_x \b{x} + a_y \b{y} + a_z \b{z}) \v (b_x \b{y \v z} + b_y \b{z \v x} + b_z \b{x \v y}) = (a_x b_x + a_y b_y + a_z b_z) \omega$$

And in fact, there's a pretty good argument that this is the more natural definition, because it's the only good way to invert matrices.

Let $$A$$ be a square invertible matrix on $$\bb{R}^n$$. We know that the wedge product of all the columns $$\bigwedge A_i = \det(A) \omega$$ is not $$0$$. For each column $$A_i$$, the wedge product of all the _other_ columns $$A^{\v n-1}_j =  (-1)^{j-1} A_1 \v A_2 \v \ldots \cancel{A_j} \ldots \v A_n$$ will have:[^sign]

$$A_i \v A^{\v n-1}_j  = \det(A) 1_{i = j} \omega $$

[^sign]: The sign here is related to the sign of $$\star \b{x}_i$$.



$$A^{\v n-1}_j$$ is an element of $$\v^{n-1} \bb{R}^n$$, but we can write the wedge product with it as a dot product (using $$\alpha \v \beta = \star^{-1} \beta \cdot (\star \alpha))$$ on scalars) to get:

$$\begin{aligned}
\star A_i \v A^{\v n-1}_j  &=  (A_i \cdot \star A^{\v n-1}_j) \\
&= A_i \cdot \star A^{\v n-1}_j \\
&= \star A^{\v n-1}_j \cdot A_i\\
&= \det(A) 1_{i = j}
\end{aligned}$$

Transposing so that the $$\cdot$$ becomes matrix multiplication (which is clunky, but we need to turn our $$i,j$$ labeled vector equation into a matrix equation somehow):

$$(\star A^{\v n-1})^T A = A (\star A^{\v n-1})^T = \det(A) I$$

We recognize $$(\star A^{\v n-1})^T$$ as the adjugate matrix of $$A$$. The inverse is, as usual:

$$A^{-1} = \frac{1}{\det(A)} (\star A^{\v n-1})^T = \frac{\text{adj}(A)}{\det(A)} $$

-------

I think that just about covers basic tools of exteior algebra.

Next time, we zoom out and try to figure out why there is so much structure here, and why it's so complicated despite that.

{% include ea.html %}
