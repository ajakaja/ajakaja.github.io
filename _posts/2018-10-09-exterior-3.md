---
layout: blog
title: "Exterior Algebra 2: the inner product"
footnotes: true
math: true
aside: true
description: Some uses of the inner product on the exterior algebra
---

*(All vector spaces in this article are assumed to be finite-dimensional and over $$\bb{R}$$)*

*(See my [previous post]({{ site.baseurl }}{% post_url 2018-10-08-exterior-1 %}) for some of the notations used here.)*

*(Not for any particular audience. Mostly I just wanted to write down these derivations in a presentable way because I haven't seen them from this direction before.)*

Exterior algebra is useful any time you're anywhere near a cross product or determinant. I want to show how the inner product on the exterior algebra can make certain formulas in the world of vectors and amtrices vastly easier to prove.

<!--more-->

Here's the [standard way](https://en.wikipedia.org/wiki/Gramian_matrix#Gram_determinant) to define inner products on the exterior algebra  $$\^ V$$, extending the inner product defined on the underlying vector space $$V$$:

$$\< \bigwedge_i \b{a}_i , \bigwedge_j \b{b}_j \> = \det \< \b{a}_i, \b{b}_j \>$$

The left side is two $$k$$-vectors (the wedge product of $$k$$ factors together); the right side is the determinant of a $$k \times k$$ matrix.

If we label the basis $$k$$-vectors using multi-indexes $$I = (i_1, i_2, \ldots i_k)$$, where no two $$I$$ contain the same set of elements up to permutation, then this amounts to saying that basis multivectors are orthonormal:[^id]

[^id]: I prefer $$1_{ij}$$ to $$\delta_{ij}$$ because, why not? It makes perfect sense.

$$\< \b{x}_I , \b{x}_J \> = 1_{IJ}$$

And then extending this linearly to all elements of $$\^^k V$$.

For instance, in $$\bb{R}^3$$, this just means that:

$$| \b{x \^ y} | = | \b{y \^ z} | = | \b{z \^ x} | = 1$$

So we've got an orthonormal basis on $$\^^k V$$. This turns out to be handy.

## 1. Inner Product identities

----------

In practice, $$\< \bigwedge_i \b{a}_i , \bigwedge_j \b{b}_j \>$$ can be computed by expanding one side into a tensor product and the other into an antisymmetrized tensor product:

$$\begin{aligned}
\< \bigwedge_i \b{a}_i , \bigwedge_j \b{b}_j \> &= \det \< \b{a}_i , \b{b}_j \> \\
&= \sum_{\sigma \in S_k} \sgn(\sigma) \prod_i \< \b{a}_i, \b{b}_{\sigma(i)} \> \\
&= \< \bigotimes_i \b{a}_i ,  \sum_{\sigma \in S_k} \sgn(\sigma) \bigotimes_j \b{b}_{\sigma(j)}\>\end{aligned}$$

(It can be either side, as long as its one. If you expand both you get an extra factor of $$n!$$ and a lot of extraneous operations.)

Now that's a lot of symbols. All it means is: generate every antisymmetric permutation of the right argument and contract them with the left, using the _tensor_ inner product.[^tensor]

[^tensor]: The tensor inner product applies the inner product on $$V$$ term-by-term: $$\< \b{a \o b}, \b{c \o d} \> = \< \b{a , c} \> \< \b{b, d } \>$$.

When we fully-antisymmetrize one side, we get:

$$\begin{aligned}
\< \b{a \^ b }, \b{c \^ d} \> &= \< \b{a \o b}, \b{c \o d} - \b{d \o c} \> \\
&= \< \b{a , c} \> \< \b{b, d } \> - \< \b{a , d} \> \< \b{b, c } \> \tag{1}
\end{aligned}$$


-------

The cross product on vectors $$\b{a} \times \b{b}$$ is the wedge product $$\b{a} \^ \b{b}$$ followed by the $$\star$$ [map](https://en.wikipedia.org/wiki/Hodge_star_operator), which sends $$(\b{y \^ z}, \b{z \^ x}, \b{x \^ y}) \ra (\b{x,y,z})$$. $$\star$$ transforms multivectors into other multivectors, but does not change the inner products of its arguments: $$\< \star \b{a}, \star \b{b} \> = \< \b{a}, \b{b} \>$$. 

Therefore we see from (1) that:

$$(\b{a \times b}) \cdot (\b{c \times d}) = \b{(a \cdot c) (b \cdot d) - (a \cdot d) (b \cdot c)}$$

This is called the [Binet-Cauchy identity](https://en.wikipedia.org/wiki/Binet%E2%80%93Cauchy_identity). 

... Set $$\b{a = c}$$, $$\b{b = d}$$ and relabel to get [Lagrange's Identity](https://en.wikipedia.org/wiki/Lagrange%27s_identity): 

$$| \b{a} \^ \b{b} |^2 = | \b{a} |^2 | \b{b} |^2 - (\b{a} \cdot \b{b})^2$$

Which is true in any dimension, but in $$n=3$$ it's also:

$$| \b{a} \times \b{b} |^2 = | \b{a} |^2 | \b{b} |^2 - (\b{a} \cdot \b{b})^2$$

... Drop the wedge product term to get [Cauchy-Schwarz](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality):

$$(\b{a} \cdot \b{b})^2 \leq | \b{a} |^2 | \b{b} |^2 $$

--------

I thought that was neat. Turns out Cauchy-Schwarz, which shows up everywhere, can be made into an equality with not much more work, as long as we're not afraid of doing math with cross / wedge products.

## 2. Generalizing determinant multiplicativity

----------

Let $$B: U \ra V$$ and $$A: V \ra W$$ be linear transformations, so that $$AB : U \ra W$$. It's well known that $$\det (AB) = \det (A) \det (B)$$. 

... Turns out it applies to any wedge power:

$$\wedge^k (AB) = \wedge^k (A) \wedge^k (B) \tag{2}$$

even if $$A$$ and $$B$$ are different sizes (as long as you can multiply them), and for any $$k$$.

Note that the right side is also matrix-multiplication (or composition of linear transformations, if you like), because $$\^^k A$$ and $$\^^k B$$ are both matrices.

$$\^k A$$ is a linear transformation which tells how $$A$$ transforms $$k$$-volumes (areas, volumes, etc in higher dimensions) into each other. This amounts to saying (unsurprisingly) that $$AB$$ transforms $$k$$-volumes the same way as $$B$$ followed by $$A$$.

------

**Explanation**:

Regular matrix multiplication is contraction along one index, and can be expressed an inner product of rows of $$A$$ with columns of $$B$$, labeled by the other indexes, like this (writing inner products as dot products for simplicity's sake):

$$(AB)^i_l = \sum_{j \in V} A^i_j B^j_l = A^i \cdot B_l$$


In the same way, the expression $$\wedge^k (A) \wedge^k (B)$$ is a sum over basis multivectors of $$\^^k V$$, indexed by basis multivectors of $$\^^k U$$ and $$\^^k W$$:

$$[\wedge^k (A) \wedge^k (B)]^{\^ I}_{\^ L} = \sum_{\^J \in \^^k V} A^{\^ I}_{\^ J} B^{\^ J}_{\^L} = \< A^{\^I}, B_{\^ L} \>$$

Meanwhile, $$\^^k AB$$ does the matrix multiplication first, then expands into the multivector basis:

$$\^^k (AB) = (A^i \cdot B_l)_{\^L \in \^^k U}^{\^I \in \^^k W} $$

Forgive the clunky notation: I'm trying to show that $$AB$$ has two indexes $$(i,l)$$, which are then used repeatedly in the expansion over $$\^I, \^L$$.

By writing this out as an antisymmetric summation, we can recover the form of our inner product on multivectors:

$$\begin{aligned}
(AB)_{\^L}^{\^I} &= (A^i \cdot B_l)^{\^I}_{\^L} \\
&=\sum_{\sigma \in S_k} \sgn(\sigma) \prod_{h = 1}^k A^{i_h} \cdot B_{l_{\sigma(h)}} \\
&=  (\bigotimes_h A^{i_h}) \cdot \sum_{\sigma \in S_k} \sgn(\sigma) \bigotimes_h B_{l_{\sigma(h)}} \\
&= \< \bigwedge_h A^{i_h} , \bigwedge_h B_{l_h} \> \\
&= \< A^{\^ I} , B_{\^ L} \>\\
&= (\^^k A) (\^^k B)
\end{aligned}$$

(I'm sure there's a way to justify this without going through the summation formula -- but this works, I guess. Actually I think some steps are pretty sketchy, but I'm going with it.)

Notice that $$k$$ can be less than $$m, n, p$$, in which case this is an equality of matrices (elements of $$\^^k U \ra \^^k W$$), not scalars. Of course if $$k$$ is greater than any of those, both sides are just 0.

This is called the Generalized [Cauchy-Binet formula](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Binet_formula), though I've only seen it expressed in terms of matrix minors before (but I'm pretty sure this sort of identity is obvious to people who actually have studied abstract algebra).

----

**Corollaries of Cauchy-Binet**:

If $$\dim U = \dim W = k$$, then the left side is a determinant of a square matrix.

$$\det (AB) = \^^k A \cdot \^^k B$$

If $$\dim V = k$$ and $$U = W$$, then it's just the regular "Cauchy-Binet formula".

If you write $$A$$ as a list of $$k$$ row vectors $$A^i$$ and $$B$$ as a list of $$k$$ column vectors $$B_l$$, then it says that:

$$\< \bigwedge A^i, \bigwedge_k B_l \> = \det \< A^i \cdot B_l \>$$

Which is just the definition of the dot product for multivectors in the first place, and immediately implies the identities in the first section.

If everything is in $$\bb{R}^3$$, then this says that:

$$\det \begin{vmatrix} \b{a \cdot x} & \b{a \cdot y} & \b{a \cdot z} \\
\b{b \cdot x} & \b{b \cdot y} & \b{b \cdot z} \\
\b{c \cdot x} & \b{c \cdot y} & \b{c \cdot z} \end{vmatrix} = (\b{a} \cdot (\b{b \times c})) (\b{x} \cdot (\b{y \times z}))$$

If $$B = A^T$$, then we already knew that $$\det (A A^T) = \det(A)^2$$, but this tells us that $$\^^k A A^T = (\^^k A)^2$$ also, which creates a sort of $$k$$-volume $$\^^k A A^T$$ of the $$N$$-parallelepiped created by the vectors in $$A$$, given by the sums of squares of $$A$$ projected onto every $$k$$-surface. Basically, an analogy of the distance formula for $$k$$-surfaces. The figure created by $$A$$ has projections onto each $$k$$-surface, and all of their squares add up to the total squared '$$k$$-volume' of $$A$$.

... Which is a lot of words. Basically it's a generalization of the distance formula, and if $$A$$ is just a single vector in $$\bb{R}^2$$ then this reduces to the Pythagorean theorem:

$$a^2 + b^2 = c^2$$

Let's agree to be cute and call that a corollary.

----

I was pleased to find a way to show this identity that didn't directly invoke determinants, minors, or characteristic polynomials. It seems nicer that way. I guess I'm happy just going around purging determinants from equations wherever I can.

