---
layout: blog
title: "Exterior Algebra Notes: the Inner Product"
footnotes: true
math: true
aside: true
tags: notes
---

*(See this [previous post]({{ site.baseurl }}{% post_url 2018-10-08-exterior-1 %}) for some of the notations used here.)*

*(Not intended for any particular audience. Mostly I just wanted to write down these derivations in a presentable way because I haven't seen them from this direction before.)*

*(Vector spaces are assumed to be finite-dimensional and over $$\bb{R}$$)*

*For reasons which will be explained a few posts in the future, I use the symbol $$\v$$ for the exterior product instead of the more common $$\^$$.*

Exterior algebra is obviously useful any time you're anywhere near a cross product or determinant. I want to show how it also comes with an inner product which can make certain formulas in the world of vectors and matrices vastly easier to prove.

<!--more-->

-----

## 1. The Inner Product

Euclidean vectors have an inner product that we use all the time. Multivectors are just vectors. What's theirs?

However we define the inner product (or 'dot product'; I tend to use both names) on multivectors over $$\bb{R}^N$$, we're going to want it to act a lot like it does on vectors. Particularly, it seems like it would be nice if

$$\| a \b{x \v y} + b \b{y \v z} + c \b{z \v x} \|^2 = a^2 + b^2 + c^2$$

ought to hold. More generally, for two multivectors in the same space, we should be able to sum over their components the same way we do for vectors with $$\< \b{u} , \b{v} \> \equiv \sum_{i \in V} u_i v_i$$:

$$\boxed{\< \b{u} , \b{v} \> \equiv \sum_{\v I \in \v^k V} u_{\v I} v_{\v I}} \tag{1}$$

If it doesn't look sorta like that, we're not going to have much intuition for it. Fortunately, this does turn out to be possible, although the usual presentation looks... a little different.

Here's the [standard way](https://en.wikipedia.org/wiki/Exterior_algebra#Inner_product) to define inner products on the exterior algebra $$\v^k V$$, extending the inner product defined on the underlying vector space $$V$$:

$$\< \bigvee_{i=1}^k \b{a}_i , \bigvee_{i=1}^k \b{b}_i \> = \det \< \b{a}_i, \b{b}_j \>$$

This is then extended linearly if either argument is a sum of multivectors. I find this expression pretty confusing. It turns out to be right, but it takes a while to see why.

The left side of this is the inner product of two $$k$$-vectors (each are the wedge product of $$k$$ factors together); the right side is the determinant of a $$k \times k$$ matrix. For instance:

$$\< \b{a}_1 \v \b{a}_2 , \b{b}_{1} \v \b{b}_{2} \> = 
\begin{vmatrix} 
\< \b{a}_1 , \b{b}_1 \> & \< \b{a}_1 , \b{b}_2 \> \\
\< \b{a}_2 , \b{b}_1 \> & \< \b{a}_2 , \b{b}_2 \> \end{vmatrix}$$

Simple calculations yield:

$$
\begin{aligned} \< \b{x\v y} , \b{x \v y} \> &= 1 \\
\< \b{x\v y} , \b{y \v x} \> &= -1 \\
\< \b{x\v y} , \b{y \v z} \> &= 0 \\
\end{aligned}
$$

If we label the basis $$k$$-vectors using multi-indices $$I = (i_1, i_2, \ldots i_k)$$, where no two $$I$$ contain the same set of elements up to permutation, then this amounts to saying that basis multivectors are orthonormal:[^id]

[^id]: I prefer $$1_{ij}$$ to $$\delta_{ij}$$ because, well, it makes perfect sense.

$$\boxed{\< \b{x}_{\v I} , \b{x}_{\v J} \> = 1_{IJ}}$$

And then extending this linearly to all elements of $$\v^k V$$.[^sign] This gives an orthonormal basis on $$\v^k V$$, and the first thing we'll do is define the '$$k$$-lengths' of multivectors, in the same way that we compute the length of a vector $$\| \b{v} \| = \< \b{v} , \b{v} \>$$:

[^sign]: If we don't specify that all of our multi-indices are unique up to permutation, then we would have to write something like $$\< \b{x}_{\v I}, \b{x}_{\v J} \> = \sgn(I, J)$$, since for instance $$\< \b{x \v y} , \b{y \v x} \> = -1$$.

$$\| \bigvee_i \b{a}_i \| = \< \bigvee_i \b{a}_i , \bigvee_i \b{a}_i \> = \det \< \b{a}_i , \b{a}_j \>$$

This is called the [Gram determinant](https://en.wikipedia.org/wiki/Gramian_matrix) of the matrix formed by the columns of $$\b{a}$$. It's non-zero if the vectors are linearly independent, which clearly corresponds to the wedge product $$\bigvee_i \b{a}_i$$ not being $$=0$$ in the first place.

In $$\bb{R}^3$$ this gives

$$\| a \b{x \v y} + b \b{y \v z} + c \b{z \v x} \| ^2 = a^2 + b^2 + c^2$$

It turns out that multivector inner products show up in disguise in a bunch of vector identities.

---

## 2. Computation of Identities

Let's get some practice computing with (1).

In these expressions, I'm going to be juggling multiple inner products at once. I'll denote them with subscripts: $$\<,\>_{\v}$$, $$\<,\>_{\o}$$, $$\<,\>_{V}$$. (I apologise for the similarity between $$\v$$ and $$V$$ -- hopefully they're different enough to distinguish.) 

The types are:

* the underlying inner product on $$V$$, which only acts on vectors: $$\< \b{u}, \b{v} \>_V = \sum_i u_i v_i$$.
* the induced inner product on $$\o V$$, which acts on tensors of the same grade term-by-term: $$\< \b{a \o b}, \b{c \o d} \>_{\o} = \< \b{a , c} \>_V \< \b{b, d } \>_V$$
* the induced inner product on $$\v V$$, which we described above: $$\< \b{a \o b}, \b{c \o d} \>_{\v} = \< \b{a , c} \>_V \< \b{b, d } \>_V - \< \b{a , d} \>_V \< \b{b, c } \>$$.


Let $$\text{Alt}$$ be the _Alternation Operator_, which takes a tensor product to its total antisymmetrization, e.g. $$\text{Alt}(\b{a \o b}) = \b{a \o b - b \o a}$$. For a tensor with $$N$$ factors, there are $$N!$$ components in the result.[^factorial]

[^factorial]: There are several conventions for defining $$\text{Alt}$$; often it comes with a $$\frac{1}{N!}$$. If you wanted it to preserve vector magnitudes, you might have it divide by $$\frac{1}{\sqrt{N!}}$$. I don't like either of those though, and prefer to leave it without factorials, because it makes other definitions much easier.

$$\< \bigvee_i \b{a}_i , \bigvee_j \b{b}_j \>_\v$$ can be computed by hand by expanding one side into a tensor product and the other into an antisymmetrized tensor product.

$$\begin{aligned}
\< \bigvee_i \b{a}_i , \bigvee_j \b{b}_j \>_{\v} &= \det \< \b{a}_i , \b{b}_j \>_V \\
&= \sum_{\sigma \in S_k} \sgn(\sigma) \prod_i \< \b{a}_i, \b{b}_{\sigma(i)} \>_V \\
&= \< \bigotimes_i \b{a}_i ,  \sum_{\sigma \in S_k} \sgn(\sigma) \bigotimes_j \b{b}_{\sigma(j)}\>_{\o} \\
&= \< \bigotimes_i \b{a}_i, \text{Alt}(\bigotimes_j \b{b}_j) \>_{\o} \\
&= \< \text{Alt}(\bigotimes_i \b{a}_i), \bigotimes_j \b{b}_j \>_{\o} \end{aligned}$$

The $$\text{Alt}$$ operator can be applied on either side; usually I put it on the right. If you put it on both sides, you would need to divide the whole expression by $$\frac{1}{N!}$$, which is annoying (but some people do it).

Here's an example of this on bivectors:

$$\begin{aligned}
\< \b{a \v b }, \b{c \v d} \>_\v &= \< \b{a \o b}, \b{c \o d} - \b{d \o c} \>_{\o} \\
&= \< \b{a , c} \>_V \< \b{b, d } \>_V - \< \b{a , d} \>_V \< \b{b, c } \>_V \tag{2}
\end{aligned}$$


-------

Now, some formulas which turn out to be the multivector inner product in disguise.

Set $$\b{a = c}$$, $$\b{b = d}$$ in (2) and relabel to get [Lagrange's Identity](https://en.wikipedia.org/wiki/Lagrange%27s_identity): 

$$| \b{a} \v \b{b} |^2 = | \b{a} |^2 | \b{b} |^2 - (\b{a} \cdot \b{b})^2$$

If you're working in $$\bb{R}^3$$, use the [Hodge Star map](https://en.wikipedia.org/wiki/Hodge_star_operator) $$\star$$ (to be discussed in a future post) to turn wedge products into cross products (preserving their magnitudes) to get the [Binet-Cauchy identity](https://en.wikipedia.org/wiki/Binet%E2%80%93Cauchy_identity):

$$(\b{a \times b}) \cdot (\b{c \times d}) = \b{(a \cdot c) (b \cdot d) - (a \cdot d) (b \cdot c)}$$

Or, if you have three terms on each side, you can turn $$\b{a \v b \v c}$$ into a [scalar triple product](https://en.wikipedia.org/wiki/Triple_product):

$$ \< \b{a \v b \v c}, \b{x \v y \v z} \> = (\b{a} \cdot (\b{b \times c})) (\b{x} \cdot (\b{y \times z})) \\
= \det \begin{pmatrix} \b{a \cdot x} & \b{a \cdot y} & \b{a \cdot z} \\
\b{b \cdot x} & \b{b \cdot y} & \b{b \cdot z} \\
\b{c \cdot x} & \b{c \cdot y} & \b{c \cdot z} \end{pmatrix}$$

Set $$\b{a=c}$$, $$\b{b = d}$$ in the two-vector version to get:

$$| \b{a} \times \b{b} |^2 = | \b{a} |^2 | \b{b} |^2 - (\b{a} \cdot \b{b})^2$$

Drop the cross product term to get [Cauchy-Schwarz](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality):

$$(\b{a} \cdot \b{b})^2 \leq | \b{a} |^2 | \b{b} |^2 $$

I thought that was neat. Maybe there are places where Cauchy-Schwarz is used where in fact Lagrange's identity would be more useful?

On vectors in $$\bb{R}^2$$ (or any dimension), of course, the vector magnitude of course gives the Pythagorean theorem:

$$| a \b{x} + b \b{y} |^2 = a^2 + b^2 = c^2$$

This generalizes to the bivector areas of an orthogonal tetrahedron (or $$(n-1)$$-vector surface areas of a $$n$$-simplex in any dimension), which is called [De Gua's Theorem](https://en.wikipedia.org/wiki/De_Gua%27s_theorem):

$$\| a \b{x \v y} + b \b{y \v z} + c \b{z \v x} \| ^2 = a^2 + b^2 + c^2$$

This is because the total surface area bivector for a closed figure in $$\bb{R}^3$$ is $$0$$, so the surface area of the opposing face is exactly $$-(a \b{x \v y} + b \b{y \v z} + c \b{z \v x} )$$. 

There is naturally a version of the law of cosines for any tetrahedron/ $$n$$-simplex with non-orthogonal sides as well. If $$\vec{c} = \vec{a} + \vec{b}$$ then (though it's often stated with $$c = b-a$$ instead):

$$\begin{aligned}
\| \vec{c} \|^2  &= \|\vec{a}\|^2 + \|\vec{b}\|^2 + 2 \vec{a} \cdot \vec{b}
\\ &=\|\vec{a}\|^2 + \|\vec{b}\|^2 + 2 \| \vec{a} \| \| \vec{b}\| \cos \theta_{ab}
\end{aligned}$$

We can easily expand $$\| \b{a} + \b{b} + \b{c} \|^2$$ linearly when $$a,b,c$$ are bivectors or anything else; the angles in the cosines become [angles between planes](https://en.wikipedia.org/wiki/Dihedral_angle), or something fancier, but the formula is otherwise the same:

$$\| \b{a} + \b{b} + \b{c} \|^2 = |\b{a}|^2 + |\b{b}|^2 + |\b{c}|^2 + 2(\b{a} \cdot \b{b} + \b{a} \cdot \b{c} + \b{b} \cdot \b{c})$$

Which is kinda cool.

------

## 3. Matrix Multiplication

This is one of the more enlightening things I've come across using $$\<,\>_\v$$.


Let $$A: U \ra V$$ and $$B: V \ra W$$ be linear transformations. Their composition $$B\circ A$$ has matrix representation:

$$(BA)_i^k = \sum_{j \in V} A_i^j B_j^k = \<A_i, B^k \> $$

The latter form expresses the fact that each matrix entry in $$BA$$ is an inner product of a column of $$A$$ with a row of $$B$$.

Because $$A^{\v q} : \v^q U \ra \v^q V$$ and $$B^{\v q} : \v^q V \ra \v^q W$$ are also linear transformations, their composition $$B^{\v q} \circ A^{\v q} : \v^q U \ra \v^W$$ also has a matrix representation:

$$(B^{\v q} A^{\v q})_I^K = \sum_{J \in \v^q V} (A^{\v q})_I^J (B^{\v q})_J^K = \< A^{\v q}_I, (B^{\v q})^K \>$$

Where $$I,J,K$$ are indexes over the appropriate $$\v^q$$ spaces.

$$A^{\v q}_I$$ is the wedge product of the $$I = (i_1, i_2, \ldots i_q)$$ columns of $$A$$, and $$(B^{\v q})^K$$ is the wedge product of $$q$$ rows of $$B$$ from $$K$$, which means this is just the inner product we discussed above.

$$(B^{\v q} A^{\v q})_I^K = \det_{i \in I, k \in K} \< A_{i}, B^{k} \> $$

But this is just the determinant of a minor of $$(BA)_i^k$$ -- the one indexed by $$(I,K)$$. This means that:

$$(B^{\v q} A^{\v q})_I^K = ((BA)^{\v q})_I^K$$

And thus:

$$\boxed{B^{\v q} A^{\v q} = (BA)^{\v q}} \tag{3}$$

This is called the [Generalized Cauchy-Binet formula](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Binet_formula). Note that $$(3)$$ does not require that the matrices be the square.

Which is neat. I think this version is _way_ easier to remember or use than the version in Wikipedia, which is expressed in terms of matrix minors and determinants everywhere.

-------

Corollaries:

When $$A$$ and $$B$$ are in the same space and $$q = \dim V$$, then all of the wedge powers turn into determinants, giving something familiar:

$$\det(BA) = \det(B) \det(A)$$

When $$B = A^T$$, it says that the determinant of the square matrix $$A^T A$$ is the sum of squared determinants of minors of the (not necessarily square) $$A$$. If $$A$$ is $$n \times k$$, this is a sum over all $$k \times k$$ minors of $$A$$:

$$\begin{aligned}
\det (A^T A) &= \v^k (A^T A) \\
&= \v^k A^T \v^k A \\
&= \sum_{I \in \v^k U} \sum_{J \in \v^k V} (A_I^J)^2
\end{aligned}$$


{% include ea.html %}
