---
layout: blog
title: "Exterior Algebra Notes 2: the inner product"
footnotes: true
math: true
aside: true
tags: notes
---

*(See this [previous post]({{ site.baseurl }}{% post_url 2018-10-08-exterior-1 %}) for some of the notations used here.)*

*(Not intended for any particular audience. Mostly I just wanted to write down these derivations in a presentable way because I haven't seen them from this direction before.)*

*(Vector spaces are assumed to be finite-dimensional and over $$\bb{R}$$)*

Exterior algebra is useful any time you're anywhere near a cross product or determinant. I want to show how the inner product on the exterior algebra can make certain formulas in the world of vectors and matrices vastly easier to prove.

<!--more-->

-----

## 1. The Inner Product

However we define the inner product (or dot product; I'll use both interchangeably) on multivectors over $$\bb{R}^N$$, we're going to want it to act a lot like it does on vectors. Particularly, it seems very true that

$$\| a \b{x \^ y} + b \b{y \^ z} + c \b{z \^ x} \|^2 = a^2 + b^2 + c^2$$

ought to hold, since it's true for $$\| a \b{x} + b \b{y} + c \b{z} \|$$. More generally, for two multivectors in the same space, we should be able to sum over their components the same way we do for vectors with $$\< \b{u} , \b{v} \> \equiv \sum_{i \in V} u^i v_i$$:

$$\boxed{\< \b{u} , \b{v} \> \equiv \sum_{\^ I \in \^^k V} u^{\^I} v_{\^ I}} \tag{1}$$


If it doesn't look sorta like that, we're not going to have much intuition for it. Fortunately, this does turn out to be possible, though the usual presentation looks... different.

Here's the [standard way](https://en.wikipedia.org/wiki/Exterior_algebra#Inner_product) to define inner products on the exterior algebra  $$\^^k V$$, extending the inner product defined on the underlying vector space $$V$$:

$$\< \bigwedge_{i=1}^k \b{a}_i , \bigwedge_{i=1}^k \b{b}_i \> = \det \< \b{a}_i, \b{b}_j \>$$

This is then extended linearly if either argument is a sum of multivectors. The left side of this is the inner product of two $$k$$-vectors (each are the wedge product of $$k$$ factors together); the right side is the determinant of a $$k \times k$$ matrix. For instance:

$$\< \b{a}_1 \^ \b{a}_2 , \b{b}_{1} \^ \b{b}_{2} \> = 
\begin{vmatrix} 
\< \b{a}_1 , \b{b}_1 \> & \< \b{a}_1 , \b{b}_2 \> \\
\< \b{a}_2 , \b{b}_1 \> & \< \b{a}_2 , \b{b}_2 \> \end{vmatrix}$$

This looks strange at first, but it turns out to be what we want. Simple calculations yield:

$$
\begin{aligned} \< \b{x\^ y} , \b{x \^ y} \> &= 1 \\
\< \b{x\^ y} , \b{y \^ x} \> &= -1 \\
\< \b{x\^ y} , \b{y \^ z} \> &= 0 \\
\end{aligned}
$$

If we label the basis $$k$$-vectors using multi-indices $$I = (i_1, i_2, \ldots i_k)$$, where no two $$I$$ contain the same set of elements up to permutation, then this amounts to saying that basis multivectors are orthonormal:[^id]

[^id]: I prefer $$1_{ij}$$ to $$\delta_{ij}$$ because it makes perfect sense.

$$\boxed{\< \b{x}_{\^I} , \b{x}_{\^J} \> = 1_{IJ}}$$

And then extending this linearly to all elements of $$\^^k V$$. This gives an orthonormal basis on $$\^^k V$$, and the first thing we'll do is define the '$$k$$-lengths' of multivectors, in the same way that we compute the length of a vector $$\| \b{v} \| = \< \b{v} , \b{v} \>$$:

$$\| \bigwedge_i \b{a}_i \| = \< \bigwedge_i \b{a}_i , \bigwedge_i \b{a}_i \> = \det \< \b{a}_i , \b{a}_j \>$$

This is called the [Gram determinant](https://en.wikipedia.org/wiki/Gramian_matrix) of the matrix formed by the columns of $$\b{a}$$. It's non-zero if the vectors are linearly independent, which clearly corresponds to the wedge product $$\bigwedge_i \b{a}_i$$ not being $$=0$$ in the first place.

In $$\bb{R}^3$$:

$$\| a \b{x \^ y} + b \b{y \^ z} + c \b{z \^ x} \| ^2 = a^2 + b^2 + c^2$$

It turns out that multivector inner products show up in disguise in a lot of places.

---

## 2. Computation of Identities

Let's get some practice computing with (1).

In these expressions, I'm going to be juggling multiple inner products at once. The tensor inner product applies the inner product on $$V$$ term-by-term: $$\< \b{a \o b}, \b{c \o d} \>_{\o} = \< \b{a , c} \> \< \b{b, d } \>$$, and the term-by-term inner products are the ones from the underlying vector space $$V$$. When it's helpful I'll try to note each type of tensor product with a subscript: $$\<,\>_{\^}$$, $$\<,\>_{\o}$$, $$\<,\>_{V}$$.

The tensor product space $$\o^k V$$ has a natural inner product induced by $$\< , \>_V$$, which just takes the product of each term that's in the same position, and multiplies them together. EG:

$$\< \b{a \o b} , \b{c \o d} \>_{\o} = \< \b{a}, \b{c} \>_V \< \b{b}, \b{d} \>_V$$

Let $$\text{Alt}$$ be the _Alternation Operator_, which takes a tensor product to its _total antisymmetrization_, eg $$\text{Alt}(\b{a \o b}) = \b{a \o b - b \o a}$$. For a tensor with $$N$$ factors, there are $$N!$$ components in the result.[^factorial]

[^factorial]: There are several conventions for defining $$\text{Alt}$$; often it comes with a $$\frac{1}{N!}$$. If you wanted it to preserve vector magnitudes, you might divide by $$\frac{1}{\sqrt{N!}}$$. I don't like either of those though, and prefer to leave it without factorials, because it makes other definitions much easier.

$$\< \bigwedge_i \b{a}_i , \bigwedge_j \b{b}_j \>_\^$$ can be computed by hand by expanding one side into a tensor product and the other into an antisymmetrized tensor product.

$$\begin{aligned}
\< \bigwedge_i \b{a}_i , \bigwedge_j \b{b}_j \>_{\^} &= \det \< \b{a}_i , \b{b}_j \>_V \\
&= \sum_{\sigma \in S_k} \sgn(\sigma) \prod_i \< \b{a}_i, \b{b}_{\sigma(i)} \>_V \\
&= \< \bigotimes_i \b{a}_i ,  \sum_{\sigma \in S_k} \sgn(\sigma) \bigotimes_j \b{b}_{\sigma(j)}\>_{\o} \\
&= \< \bigotimes_i \b{a}_i, \text{Alt}(\bigotimes_j \b{b}_j) \>_{\o} \\
&= \< \text{Alt}(\bigotimes_i \b{a}_i), \bigotimes_j \b{b}_j \>_{\o} \end{aligned}$$

The $$\text{Alt}$$ operator can be applied on either side; usually I put it on the right. If you put it on both sides, you would need to divide the whole expression by $$\frac{1}{N!}$$, which is annoying (but some people do it).

Here's an example of this on bivectors:

$$\begin{aligned}
\< \b{a \^ b }, \b{c \^ d} \>_\^ &= \< \b{a \o b}, \b{c \o d} - \b{d \o c} \>_{\o} \\
&= \< \b{a , c} \>_V \< \b{b, d } \>_V - \< \b{a , d} \>_V \< \b{b, c } \>_V \tag{2}
\end{aligned}$$


-------

Now, some formulas which turn out to be the multivector inner product in disguise.

Set $$\b{a = c}$$, $$\b{b = d}$$ in (2) and relabel to get [Lagrange's Identity](https://en.wikipedia.org/wiki/Lagrange%27s_identity): 

$$| \b{a} \^ \b{b} |^2 = | \b{a} |^2 | \b{b} |^2 - (\b{a} \cdot \b{b})^2$$

If you're working in $$\bb{R}^3$$, use the [Hodge Star map](https://en.wikipedia.org/wiki/Hodge_star_operator) $$\star$$ to turn wedge products into cross products (preserving their magnitudes) to get the [Binet-Cauchy identity](https://en.wikipedia.org/wiki/Binet%E2%80%93Cauchy_identity):

$$(\b{a \times b}) \cdot (\b{c \times d}) = \b{(a \cdot c) (b \cdot d) - (a \cdot d) (b \cdot c)}$$

Or, if you have three terms on each side, you can turn $$\b{a \^ b \^ c}$$ into a [scalar triple product](https://en.wikipedia.org/wiki/Triple_product):

$$ \< \b{a \^ b \^ c}, \b{x \^ y \^ z} \> = (\b{a} \cdot (\b{b \times c})) (\b{x} \cdot (\b{y \times z})) \\
= \det \begin{pmatrix} \b{a \cdot x} & \b{a \cdot y} & \b{a \cdot z} \\
\b{b \cdot x} & \b{b \cdot y} & \b{b \cdot z} \\
\b{c \cdot x} & \b{c \cdot y} & \b{c \cdot z} \end{pmatrix}$$

Set $$\b{a=c}$$, $$\b{b = d}$$ in the two-vector version to get:

$$| \b{a} \times \b{b} |^2 = | \b{a} |^2 | \b{b} |^2 - (\b{a} \cdot \b{b})^2$$

Drop the cross product term to get [Cauchy-Schwarz](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality):

$$(\b{a} \cdot \b{b})^2 \leq | \b{a} |^2 | \b{b} |^2 $$

I thought that was neat. Maybe there are places where Cauchy-Schwarz is used where in fact the wedge-product equality would be more useful?

On vectors in $$\bb{R}^2$$ (or any dimension), of course, the vector magnitude of course gives the Pythagorean theorem:

$$| a \b{x} + b \b{y} |^2 = c^2$$

This generalizes to the bivector areas of an orthogonal tetrahedron (or $$n-1$$-vector surface areas of a $$n$$-simplex in any dimension), which is called [De Gua's Theorem](https://en.wikipedia.org/wiki/De_Gua%27s_theorem):

$$\| a \b{x \^ y} + b \b{y \^ z} + c \b{z \^ x} \| ^2 = a^2 + b^2 + c^2$$

This is because the total surface area bivector for a closed figure in $$\bb{R}^3$$ is $$0$$, so the surface area of the opposing face is exactly $$-(a \b{x \^ y} + b \b{y \^ z} + c \b{z \^ x} )$$. 

There is naturally a version of the law of cosines for any tetrahedron/ $$n$$-simplex with non-orthogonal sides as well. If $$\vec{c} = \vec{a} + \vec{b}$$ then (though it's often stated with $$c = b-a$$ instead):

$$\begin{aligned}
\| \vec{c} \|^2  &= \|\vec{a}\|^2 + \|\vec{b}\|^2 + 2 \vec{a} \cdot \vec{b}
\\ &=\|\vec{a}\|^2 + \|\vec{b}\|^2 + 2 \vec{a} \cdot \vec{b} \cos \theta_{ab}
\end{aligned}$$

We can easily expand $$\| \b{a} + \b{b} + \b{c} \|^2$$ linearly when $$a,b,c$$ are bivectors or anything else; the angles in the cosines become [angles between planes](https://en.wikipedia.org/wiki/Dihedral_angle) but the formula is otherwise the same:

$$\| \b{a} + \b{b} + \b{c} \|^2 = |\b{a}|^2 + |\b{b}|^2 + |\b{c}|^2 + 2(\b{a} \cdot \b{b} + \b{a} \cdot \b{c} + \b{b} \cdot \b{c})$$

------

## 3. Matrix Multiplication

This is one of the more enlightening things I've come across using $$\<,\>_\^$$.


Let $$A: U \ra V$$ and $$B: V \ra W$$ be linear transformations. Their composition $$B\circ A$$ has matrix representation:

$$(BA)_i^k = \sum_{j \in V} A_i^j B_j^k = \<A_i, B^k \> $$

The latter form expresses the fact that each matrix entry in $$BA$$ is an inner product of a column of $$A$$ with a row of $$B$$.

Because $$A^{\^q} : \^^q U \ra \^^q V$$ and $$B^{\^q} : \^^q V \ra \^^q W$$ are also linear transformations, their composition $$B^{\^q} \circ A^{\^q} : \^^q U \ra \^^W$$ also has a matrix representation:

$$(B^{\^q} A^{\^q})_I^K = \sum_{J \in \^^q V} (A^{\^q})_I^J (B^{\^q})_J^K = \< A^{\^q}_I, (B^{\^q})^K \>$$

Where $$I,J,K$$ are indexes over the appropriate $$\^^q$$ spaces.

$$A^{\^q}_I$$ is the wedge product of the $$I = (i_1, i_2, \ldots i_q)$$ columns of $$A$$, and $$(B^{\^q})^K$$ is the wedge product of $$q$$ rows of $$B$$ from $$K$$, which means this is just the inner product we discussed above.

$$(B^{\^q} A^{\^q})_I^K = \det_{i \in I, k \in K} \< A_{i}, B^{k} \> $$

But this is just the determinant of a minor of $$(BA)_i^k$$ -- the one indexed by $$(I,K)$$. This means that:

$$(B^{\^q} A^{\^q})_I^K = ((BA)^{\^q})_I^K$$

And thus:

$$\boxed{B^{\^q} A^{\^q} = (BA)^{\^q}} \tag{3}$$

This is called the [Generalized Cauchy-Binet formula](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Binet_formula). 

-------

Corollaries:

When $$A$$ and $$B$$ are in the same space and $$q = \dim V$$, then all of the wedge powers turn into determinants, giving something familiar:

$$\det(BA) = \det(B) \det(A)$$

Which is neat. I think this version is _way_ easier to remember or use than the version in Wikipedia, which is expressed in terms of matrix minors and determinants everywhere.

When $$B = A^T$$, it says that the determinant of the square matrix $$A^T A$$ is the sum of squared determinants of minors of the (not necessarily square) $$A$$. If $$A$$ is $$n \times k$$, this is a sum over all $$k \times k$$ minors of $$A$$:

$$\begin{aligned}
\det (A^T A) &= \^^k (A^T A) \\
&= \^^k A^T \^^k A \\
&= \sum_{I \in \^^k U} \sum_{J \in \^^k V} (A_I^J)^2
\end{aligned}$$