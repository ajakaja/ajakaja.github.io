---
layout: blog
title: "Exterior Algebra Notes 2: the inner product"
footnotes: true
math: true
aside: true
tags: notes
---

*(See this [previous post]({{ site.baseurl }}{% post_url 2018-10-08-exterior-1 %}) for some of the notations used here.)*

*(Not intended for any particular audience. Mostly I just wanted to write down these derivations in a presentable way because I haven't seen them from this direction before.)*

*(Vector spaces are assumed to be finite-dimensional and over $$\bb{R}$$)*

Exterior algebra is useful any time you're anywhere near a cross product or determinant. I want to show how the inner product on the exterior algebra can make certain formulas in the world of vectors and matrices vastly easier to prove.

<!--more-->

Here's the [standard way](https://en.wikipedia.org/wiki/Exterior_algebra#Inner_product) to define inner products on the exterior algebra  $$\^^k V$$, extending the inner product defined on the underlying vector space $$V$$:

$$\< \bigwedge_{i=1}^k \b{a}_i , \bigwedge_{i=1}^k \b{b}_i \> = \det \< \b{a}_i, \b{b}_j \>$$

The left side is two $$k$$-vectors (the wedge product of $$k$$ factors together); the right side is the determinant of a $$k \times k$$ matrix. 

If we label the basis $$k$$-vectors using multi-indices $$I = (i_1, i_2, \ldots i_k)$$, where no two $$I$$ contain the same set of elements up to permutation, then this amounts to saying that basis multivectors are orthonormal:[^id]

[^id]: I prefer $$1_{ij}$$ to $$\delta_{ij}$$ because, why not? It makes perfect sense.

$$\< \b{x}_I , \b{x}_J \> = 1_{IJ}$$

And then extending this linearly to all elements of $$\^^k V$$.
For instance, in $$\bb{R}^3$$, this just means that:

$$| \b{x \^ y} | = | \b{y \^ z} | = | \b{z \^ x} | = 1$$

So we've got an orthonormal basis on $$\^^k V$$. This turns out to be handy. For one thing, we can compute '$$k$$-lengths' of multivectors, in the same way that we compute the length of a vector $$\| \b{v} \| = \< \b{v} , \b{v} \>$$:

$$\| a \b{x \^ y} + b \b{y \^ z} + c \b{z \^ x} \| ^2 = a^2 + b^2 + c^2$$

Besides this, it turns out that multivector inner products show up in disguise in a lot of places.

---

## 1 Inner Product identities

In these expressions, I'm going to be juggling multiple inner products at once. The tensor inner product applies the inner product on $$V$$ term-by-term: $$\< \b{a \o b}, \b{c \o d} \>_{\o} = \< \b{a , c} \> \< \b{b, d } \>$$, and the term-by-term inner products are the ones from the underlying vector space $$V$$. When it's helpful I'll try to note each type of tensor product with a subscript: $$\<,\>_{\^}$$, $$\<,\>_{\o}$$, $$\<,\>_{V}$$.

$$\< \bigwedge_i \b{a}_i , \bigwedge_j \b{b}_j \>_\^$$ can be computed by expanding one side into a tensor product and the other into an antisymmetrized tensor product. This antisymmetrization procedure can be written with the $$\text{Alt}$$ operator:

$$\begin{aligned}
\< \bigwedge_i \b{a}_i , \bigwedge_j \b{b}_j \>_{\^} &= \det \< \b{a}_i , \b{b}_j \>_V \\
&= \sum_{\sigma \in S_k} \sgn(\sigma) \prod_i \< \b{a}_i, \b{b}_{\sigma(i)} \>_V \\
&= \< \bigotimes_i \b{a}_i ,  \sum_{\sigma \in S_k} \sgn(\sigma) \bigotimes_j \b{b}_{\sigma(j)}\>_{\o} \\
= \< \bigotimes_i \b{a}_i, &\text{ Alt}(\bigotimes_j \b{b}_j) \>_{\o} \end{aligned}$$

This expression might seem strange, because the operator is only applied on one side. Actually it can be either side, as long as it's one, and we could do both but we would end up with $$n!$$ of the resulting term. We could define the inner product to use $$\text{Alt}$$ on both sides and divide by $$n!$$ -- but I have other reasons for not doing that that will matter in a future article. This one is the easiest to compute, anyway.

Now that's a lot of symbols. All it means is: generate every antisymmetric permutation of the right argument and contract them with the left, using the _tensor_ inner product, like this:

$$\begin{aligned}
\< \b{a \^ b }, \b{c \^ d} \>_\^ &= \< \b{a \o b}, \b{c \o d} - \b{d \o c} \>_{\o} \\
&= \< \b{a , c} \>_V \< \b{b, d } \>_V - \< \b{a , d} \>_V \< \b{b, c } \>_V \tag{1}
\end{aligned}$$


-------

I haven't talked about the [Hodge Star map](https://en.wikipedia.org/wiki/Hodge_star_operator) $$\star$$ yet, but this might still be readable without it. $$\star$$ transforms multivectors into other multivectors ($$\^^k V\ra \^^{n-k}V$$), but, crucially not change the inner products of its arguments: $$\< \star \b{a}, \star \b{b} \> = \< \b{a}, \b{b} \>$$ -- at least, not when we define the inner product the way we have.

The cross product on vectors $$\b{a} \times \b{b}$$ is the wedge product $$\b{a} \^ \b{b}$$ followed by the $$\star$$ operator, which sends $$(\b{y \^ z}, \b{z \^ x}, \b{x \^ y}) \ra (\b{x,y,z})$$. 

Therefore we see from (1) that:

$$(\b{a \times b}) \cdot (\b{c \times d}) = \b{(a \cdot c) (b \cdot d) - (a \cdot d) (b \cdot c)}$$

This is called the [Binet-Cauchy identity](https://en.wikipedia.org/wiki/Binet%E2%80%93Cauchy_identity). 

Another way we could transform (1): set $$\b{a = c}$$, $$\b{b = d}$$ in (1) and relabel to get [Lagrange's Identity](https://en.wikipedia.org/wiki/Lagrange%27s_identity): 

$$| \b{a} \^ \b{b} |^2 = | \b{a} |^2 | \b{b} |^2 - (\b{a} \cdot \b{b})^2$$

Which is true in any dimension, but in $$n=3$$ it also corresponds to:

$$| \b{a} \times \b{b} |^2 = | \b{a} |^2 | \b{b} |^2 - (\b{a} \cdot \b{b})^2$$

If we drop the wedge product term we get the [Cauchy-Schwarz Inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality):

$$(\b{a} \cdot \b{b})^2 \leq | \b{a} |^2 | \b{b} |^2 $$

I thought that was neat. Turns out Cauchy-Schwarz, which shows up everywhere, can be made into an _equality_ with not much more work, as long as we're not afraid of doing math with cross / wedge products.

My interest in this subject is partly because I believe manipulations like those above should be standard and comfortable the way derivatives and basic algebra are. The only way that will ever be the case is if we have a simple model for doing calculations with vectors that doesn't require special cases and theory to make confident calculations with.

The basic rule to remember here is that:

> To compute the wedge inner product $$\< a \^ b, c \^ d \>$$, compute their tensor inner product with one side antisymmetrized: $$\< a \o b, c \o d - d \o c\>$$.

It's a bit strange, but it works.

Eventually we would like to intuit all of the grad/div/curl identities also -- but they require some more proficiency with the Hodge star operator, which I will save for another article.

------

## 2 Generalizing determinant multiplicativity


Let $$B: U \ra V$$ and $$A: V \ra W$$ be linear transformations, so that $$AB : U \ra W$$. It's well known that $$\det (AB) = \det (A) \det (B)$$. 

It turns out that this applies to any wedge power of $$AB$$ as well:

$$(AB)^{\^ k} = A^{\^ k} B^{\^ k}\tag{2}$$

even if $$A$$ and $$B$$ are different sizes (as long as you can multiply them), and for any $$k$$.

Note that the right side is also matrix-multiplication (or composition of linear transformations, if you like), because $$A^{\^ k}$$ and $$B^{\^ k}$$ are both matrices.

$$A^{\^ k}$$ is a linear transformation which tells how $$A$$ transforms $$k$$-volumes (areas, volumes, etc in higher dimensions) into each other. This amounts to saying (unsurprisingly) that $$AB$$ transforms $$k$$-volumes the same way as $$B$$ followed by $$A$$.

------

**Explanation**:

Regular matrix multiplication is given by contraction along one index, and can be expressed an inner product of rows of $$A$$ with columns of $$B$$, labeled by the other indices, like this (writing inner products as dot products for simplicity's sake):

$$(AB)^i_l = \sum_{j \in V} A^i_j B^j_l = A^i \cdot B_l$$


In the same way, the expression $$A^{\^k} B^{\^k}$$ is a sum over basis multivectors of $$\^^k V$$, indexed by basis multivectors of $$\^^k U$$ and $$\^^k W$$:

$$[A^{\^k} B^{\^k}]^{\^ I}_{\^ L} = \sum_{\^J \in \^^k V} A^{\^ I}_{\^ J} B^{\^ J}_{\^L} = \< A^{\^I}, B_{\^ L} \>$$

Meanwhile, $$(AB)^{\^k}$$ does the matrix multiplication first, then expands into the multivector basis:

$$(AB)^{\^k}= (A^i \cdot B_l)_{\^L \in \^^k U}^{\^I \in \^^k W} $$

Forgive the clunky notation: I'm trying to show that $$AB$$ has two indices $$(i,l)$$, which are then used repeatedly in the expansion over $$\^I, \^L$$.

By writing this out as an antisymmetric summation, we can recover the form of our inner product on multivectors:

$$\begin{aligned}
(AB)_{\^L}^{\^I} &= (A^i \cdot B_l)^{\^I}_{\^L} \\
&=\sum_{\sigma \in S_k} \sgn(\sigma) \prod_{h = 1}^k A^{i_h} \cdot B_{l_{\sigma(h)}} \\
&=  (\bigotimes_h A^{i_h}) \cdot \sum_{\sigma \in S_k} \sgn(\sigma) \bigotimes_h B_{l_{\sigma(h)}} \\
&= \< \bigwedge_h A^{i_h} , \bigwedge_h B_{l_h} \> \\
&= \< A^{\^ I} , B_{\^ L} \>\\
&= A^{\^k} B^{\^k}
\end{aligned}$$

Notice that $$k$$ can be less than $$\dim U, \dim V$$ and , $$\dim W$$, in which case this is an equality of matrices (elements of $$\^^k U \ra \^^k W$$), not scalars. Of course if $$k$$ is greater than any of those, both sides are just 0.

This is called the Generalized [Cauchy-Binet formula](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Binet_formula). It's normally expressed with a lot of determinants and references to matrix minors.

----

Some special-cases:

If $$\dim U = \dim W = k$$, then the left side is a determinant of a square matrix, and 

$$\det (AB) = A^{\^k} \cdot B^{\^k}$$

If $$\dim V = k$$ and $$U = W$$, then it's just the regular "Cauchy-Binet formula".

If you write $$A$$ as a list of $$k$$ row vectors $$A^i$$ and $$B$$ as a list of $$k$$ column vectors $$B_l$$, then it says that:

$$\< \bigwedge A^i, \bigwedge_k B_l \> = \det \< A^i \cdot B_l \>$$

Which is just the definition of the dot product for multivectors in the first place, and immediately implies the identities in the first section.

If everything is in $$\bb{R}^3$$, then Cauchy-Binet says that:

$$ (\b{a} \cdot (\b{b \times c})) (\b{x} \cdot (\b{y \times z})) = \det \begin{vmatrix} \b{a \cdot x} & \b{a \cdot y} & \b{a \cdot z} \\
\b{b \cdot x} & \b{b \cdot y} & \b{b \cdot z} \\
\b{c \cdot x} & \b{c \cdot y} & \b{c \cdot z} \end{vmatrix}$$

If $$A = B^T$$, then Cauchy-Binet says that:

$$(A^T A)^{\^ k} = (A^T)^{\^k} A^{\^ k}$$

In $$n$$-dimensional space, the $$n \times n$$ matrix $$A^T A$$ (called the [Gram matrix](https://en.wikipedia.org/wiki/Gramian_matrix#Gram_determinant) of the vectors in $$A$$) tells us how a transformation by $$A$$ modifies the inner products of vectors:

$$\< A \b{u}, A \b{v} \> = \b{u} A^T A \b{v}$$

For example, in $$\bb{R}^2$$;

$$\< A \b{u}, A \b{v} \> = \begin{pmatrix} u_x & u_y \end{pmatrix} \begin{pmatrix} A_x \cdot A_x & A_x \cdot A_y \\ A_y \cdot A_x & A_y \cdot A_y \end{pmatrix} \begin{pmatrix} v_x \\ v_y \end{pmatrix}$$

Cauchy-Binet then tells us that $$A$$'s behavior on multivector inner products can be computed from wedge powers of $$A^T A$$:

$$\< (A^{\^ k}) \alpha, (A^{\^ k}) \beta \> = \alpha (A^{\^ k})^T (A^{\^ k}) \beta = \alpha (A^T A)^{\^ k} \beta $$

Which might be useful somewhere; I dunno. 

Notably, $$(A^T A)^{\^ n}$$ is called the Gram determinant, and gives the square of the volume of the figure formed from the columns of $$A$$. As such it's used to test for the linear independence of the columns of $$A$$. (In fact, you could just check that $$A^{\^^n} = 0$$ for that, though if $$A$$ is $$m \times n$$ then it will probably be more efficient to compute $$(A^T A)^{\^ n}$$.)

