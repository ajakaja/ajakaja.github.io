---
layout: blog
title: "Exterior Algebra 2: matrices and determinants"
footnotes: true
math: true
aside: true
description: Some notes on exterior powers of matrices
---

*(There is not a Part 1.)*

*(All vector spaces in this article are assumed to be finite-dimensional and over $$\bb{R}$$)*

*(I don't have an audience in mind for this. It's notes that have been dressed up as a blog post, which may be useful if you're trying to understand this stuff.)*

[Exterior algebra](https://en.wikipedia.org/wiki/Exterior_algebra) is a pretty technical subject. It gets discussed in abstract algebra and mostly used in differential geometry.

I'm interested in it because I think it takes a lot of the mysteriousness out of the otherwise technical and tedious subject of linear algebra, and turns out to be used in disguise all over the place. I'm not entirely sure what my goals with studying mathematics are, but I'm drawn to whatever makes computation and intuition easiest, and this is it. In college I learned about determinants and matrix inverses and never really understood how they work: they were impressive constructions that I don't think I would have found myself. Exterior algebra makes them things you can casually rederive whenever it's useful.

<!--more-->

## 1. An example problem

--------

Suppose: you're writing a computer graphics library. You want to have everything be lit up by the sun, which is in a fixed direction in the sky. To do this, each surface will be drawn with a brightness based on the angle it makes with the light -- because being pointed directly at the sun makes surfaces brighter than being at an $$89 \degree$$ angle to them, right?

For each object surface in your world, you store the _surface unit normal vector_ $$\b{n}$$, which points out of the surface, and compute the brightness of that object in the sun. The formula for the brightness scaling factor $$c$$:

$$c = \b{n} \cdot \b{v}_{light}$$

But every vector in your space is transformed many times before it ends up being in its final coordinates -- because it moves around over time and you just store the transformations required to move it. When it comes time to compute how the _normals_ transform, you [write](https://webgl2fundamentals.org/webgl/lessons/webgl-3d-lighting-directional.html):

> There is one problem which I don't know how to show directly so I'm going to show it in a diagram. We're multiplying the normal by the u_world matrix to re-orient the normals. What happens if we scale the world matrix? It turns out we get the wrong normals. 
> ...
> I've never bothered to understand the solution but it turns out you can get the inverse of the world matrix, transpose it, which means swap the columns for rows, and use that instead and you'll get the right answer.

Turns out that normal vectors are _bivectors_ and have to transform like them or you get the wrong answer.

Specifically: a normal vector $$\b{n} = n_x \b{x} + n_y \b{y} + n_z \b{z}$$ is 'really' a bivector:

$$\b{n} = n_x \b{y \^ z} + n_y \b{z \^ x} + n_z \b{x \^ y}$$

If vectors transform according to $$(v_x, v_y, v_z) \ra R(v_x, v_y, v_z)$$, then bivectors transform like this:

$$(n_x, n_y, n_z) \ra \^^2 R(n_x, n_y, n_z)$$

It [_so happens_]({{ site.baseurl }}{% post_url 2018-05-27-how-to-invert-a-matrix %}) that in $$\bb{R}^3$$, $$\^^2 R= (R^{-1})^T$$, so you can get away with transforming $$\b{n}$$ using $$(R^{-1})^T$$ without really knowing what you're doing.

--------

## 2 Multivector Matrices


Now here are some notes about how linear transformations work on multivectors, because it took me a while to understand this and I had to write it out.

In this section I use _multi-indexes_ to refer to multivectors, which are labeled with capital letters like $$I$$ and refer to tuples $$I = (i_1, i_2, \ldots i_k)$$. (Usually it's obvious what $$k$$ is.) $$\b{x}^{\^ I}$$ means:

$$\b{x}_{\^ I} = \b{x}_{i_1} \^ \b{x}_{i_2} \^ \ldots \b{x}_{i_k} \in \^^k V$$ 

Multiple tuples can refer to the same multivector, since transposing elements just multiplies by $$-1$$:

$$\b{x}_{i_1} \^ \b{x}_{i_2} \^ \ldots \b{x}_{i_k} = - \b{x}_{i_2} \^ \b{x}_{i_1} \^ \ldots \b{x}^{i_k}$$

To get around this, we assume that whenever we talk about multivectors using multi-indexes, we never include two which are the same components in a different permutation. We pick one of the two and stick with it -- so we might consider $$\b{x \^ y}$$ or $$\b{y \^ x}$$, but never both, because that would be redundant and confusing.

Also, I'm using the [convention](https://en.wikipedia.org/wiki/Einstein_notation) of labeling matrices with superscripts for rows and subscripts for columns: $$A^i_j$$. A lower index only $$A_j$$ means the $$j$$'th column vector; an upper index only $$A^i$$ means the $$i$$'th row vector. Basis vectors will get lower indexes $$\b{x}_i$$ if they're acting like columns and upper for rows.

--------


A matrix $$A$$ may be seen as writing a linear transformation $$A : V \ra V$$ out in terms of its actions on each basis vector $$\b{x}_n \in V$$:

$$A = \begin{pmatrix} A_1 & A_2 & \ldots & A_n \end{pmatrix} = (A(\b{x}_1), A(\b{x}_2), \ldots A(\b{x}_n))$$

For any basis _multivector_ $$\b{x}_{\^I} \in \wedge^k V$$, $$A$$ also has an action:

$$A(\b{x}_{\^ I}) = A(\b{x}_{i_1}) \^ A(\b{x}_{i_2}) \^ \ldots A(\b{x}_{i_k})$$

The action on all of the basis multivectors is itself a linear transformation, called $$\wedge^k A : \wedge^k V \ra \wedge^k V$$, and we can write _it_ out as a matrix. If $$n = \dim V$$, then it's $${n \choose k} \times {n \choose k}$$.


<!--more-->

Each column of $$\wedge^k V$$ is the action of $$A$$ on a particular $$k$$-vector. For instance, suppose $$V = \bb{R}^3$$. Then:

$$\wedge^2 A = (A(\b{x} \^ \b{y}), A(\b{y} \^ \b{z}) , A(\b{z} \^ \b{x}) )$$

The rows' data is the coordinates of each of these transformations. For instance:

$$
A(\b{x}) \^ A(\b{y})
= \begin{pmatrix} A_x^x A_y^y - A_x^y A_y^x \\[3pt] A_x^y A_y^z - A_x^z A_y^y \\[3pt] A_x^z A_y^x - A_x^x A_y^z \end{pmatrix} = 
\begin{pmatrix} A(\b{x \^ y})_{\b{x \^ y}} \\[3pt] A(\b{x \^ y})_{\b{y \^ z}} \\[3pt] A(\b{x \^ y})_{\b{z \^ x}}  \end{pmatrix}$$

I find it useful to label things like this:

<aside>

**Proposed notation**: $$A_{x \^ y}$$ is defined to mean $$A(\b{x}) \^ A(\b{y})$$. As before, we can refer to only rows or columns by omitting one index. $$A_{x \^ y}^{u \^ v}$$ is defined to extract to the $$u \^ v$$ coordinate of $$A_{x \^ y}$$.

Then:

$$\^^2 A = 
\begin{pmatrix} 
A_{x \^ y}^{x \^ y} & A_{y \^ z}^{x \^ y} & A_{z \^ x}^{x \^ y}  \\[5pt]
A_{x \^ y}^{y \^ z} & A_{y \^ z}^{y \^ z} & A_{z \^ x}^{y \^ z}  \\[5pt]
A_{x \^ y}^{z \^ x} & A_{y \^ z}^{z \^ x} & A_{z \^ x}^{z \^ x} \end{pmatrix} = 
\begin{pmatrix} A_{x \^ y} & A_{y \^ z} & A_{z \^ x} \end{pmatrix}
= \begin{pmatrix} A^{x \^ y} \\[5pt] A^{y \^ z} \\[5pt] A^{z \^ x} \end{pmatrix}$$

(nb: of course it doesn't matter what order we write the columns or rows in; they don't even have to be in the same order. It's just a choice of basis.)

Higher powers of $$\^^k A$$ are the same idea, but indexed by the basis multivectors $$\b{x}_{\^ I} \in \^^k V$$.

If referring to multivectors with multi-indexes, we can write them this way also:

$$A_{\^^I}^{\^^J} \equiv A_{x^{\^^I}}^{x^{\^^J}}$$

</aside>


If $$A$$ is a map between two _different_ vector spaces, we can still talk about $$\^^k A$$. It just has indexes from the two different spaces. If $$\dim U = n$$ and $$\dim V = m$$ and $$A : U \ra V$$, then $$\^^k A$$ is a $${m \choose k} \times {n \choose k}$$ matrix (reminder: that means it has $${ m \choose k} = \dim \^^k V$$ rows and $${ n \choose k} = \dim \^^k U$$ columns).

Suppose $$A : \bb{R}^2 \ra \bb{R}^3$$, and we label the $$\bb{R}^2$$ by basis vectors $$\b{u,v}$$. Then:

$$\^^2 A = \begin{pmatrix}
A_{u \^ v}^{x \^ y} \\[5pt]
A_{u \^ v}^{y \^ z} \\[5pt]
A_{u \^ v}^{z \^ x}
\end{pmatrix}$$

Also:

$$\^^2 A^T = \begin{pmatrix}
(A^T)^{u \^ v}_{x \^ y} &
(A^T)^{u \^ v}_{y \^ z} &
(A^T)^{u \^ v}_{z \^ x}
\end{pmatrix}$$

Concretely:


$$A = \begin{pmatrix} a & b \\ c & d \\ e & f \end{pmatrix}$$

$$\^^2 A = \begin{pmatrix} ad-bc \\ cf-de \\ eb - fa \end{pmatrix}$$

$$\^^2 A^T = \begin{pmatrix} ad - bc & cf - de & eb - fa \end{pmatrix}$$


But no matter what $$\^^k A$$ is a map between $$\^^k U \ra \^^k V$$, and if $$k > \dim U$$ or $$k > \dim V$$ then one of those spaces is zero-dimensional and $$A$$ is just 0.

## 3. Determinants (briefly)

The determinant of a matrix $$A : V \ra V$$ is its action on the one-dimensional space of volumes $$\^^V$$. If $$\omega = \b{x}^1 \^ \b{x}^2 \^ \ldots \b{x}^n$$ is the unique volume basis element of $$\^^n V$$, then:

$$A(\omega) = \det (A) \omega$$

Which we may write as:

$$A_{\omega}^{\omega} = \det A$$

Most of the properties of $$\det$$ follow directly from this interpretation. (eg: non-linearly-independent matrix -> zero volume; composition of transformations -> multiplication of determinants.)

If $$B: U \ra V$$ transforms between different spaces of the same dimension, then the determinant is a ratio of volume elements:

$$B_{\omega_U}^{\omega_V} = \det B$$

Each component $$A_{\^ I}^{\^ J}$$ in $$\^^K A$$ is the determinant of a $$k \times k$$ [minor](https://en.wikipedia.org/wiki/Minor_(linear_algebra)) of $$A$$: the minor created by the columns $$I$$ and the rows $$J$$ (possible with a minus sign, depending on their permutations). I prefer this notation because it emphasizes exactly the meaning of that value: that it is a component of the matrix $$\^^k A$$. The diagonal elements $$A_{x^{\^ I}}^{x^{\^ I}}$$ are the determinants of the principal minors. 

A non-square matrix doesn't have a determinant, but it does have minors (as we in the $$2 \times 3$$ example above). If $$k$$ exceeds its smallest dimension, then all the values are $$0$$.

We could also proceed by taking 'minors' of tensors in the same way (they're called [hyperdeterminants](https://en.wikipedia.org/wiki/Hyperdeterminant)), which could be... fun... I guess. But I'll leave that for another ~day~ month.
