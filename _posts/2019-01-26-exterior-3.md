---
layout: blog
title: "EA Notes #3: The Interior Product"
footnotes: true
math: true
aside: true
tags: notes
---

*Previously: [matrices]({{ site.baseurl }}{% post_url 2018-10-08-exterior-1 %}) and [inner products]({{ site.baseurl }}{% post_url 2018-10-09-exterior-2 %}) on exterior algebras.*

*Vector spaces are assumed to be finite-dimensional and over $$\bb{R}$$. The grade of a multivector $$\alpha$$ will be written $$\| \alpha \|$$, while its magnitude will be written $$\Vert \alpha \Vert$$. Bold letters like $$\b{u}$$ will refer to (grade-1) vectors, while Greek letters like $$\alpha$$ refer to arbitrary multivectors with grade $$\| \alpha \|$$.*

------

Previously we looked at how the inner product on an exterior power of a vector space $$\^^k V$$ is actually exactly what you'd expect (unit multivectors like $$\b{x \^ y}$$ are orthonormal), and gives rise to some common identities. The inner product of two $$k$$-vectors was computed by summing over the orthonormal basis vectors, like so:

$$\< \b{u} , \b{v} \> \equiv \sum_{\^ I \in \^^k V} u^{\^I} v_{\^ I}$$

In practice the way we compute this for two arbitrary multivectors is by antisymmetrizing one of them (I usually choose the right), then contracting term-by-term:

$$\begin{aligned}
\< \b{a \^ b }, \b{c \^ d} \>_\^ &= \< \b{a \o b}, \b{c \o d} - \b{d \o c} \>_{\o} \\
&= \< \b{a , c} \>_V \< \b{b, d } \>_V - \< \b{a , d} \>_V \< \b{b, c } \>_V
\end{aligned}$$


In this article we cover the two other common operations of exterior algebra: a far-reaching generalization of the inner product called the **interior product** and the **Hodge Star** (which I prefer to call the **complement**).

<!--more-->

----------

## 1. The Interior Product

The [Interior Product](https://en.wikipedia.org/wiki/Interior_product), also called the _contraction_ or _insertion_ operator, lets us 'partially apply' the inner product. The left argument is now allowed to be of lower grade than the right, in which case it deletes _some_ dimensions, but not all, from the right. I will write with a dot product $$\cdot$$ symbol.

It is most efficiently defined as the _adjoint_ of the wedge product with respect to the inner product, meaning that an interior product on one side of an inner product is the same as a wedge on the other:

$$\boxed{\< \gamma \^ \alpha, \beta \> = \< \alpha, \gamma \cdot \beta \>} 
\tag{1}$$

This fully defines the product, and shows that an interior product lowers grades in symmetry with how a wedge product raises grades.

Inspection of the adjoint relationship above reveals that the interior product _subtracts_ grades, and specifically it subtracts the _left_ argument from the _right_ argument, so $$\alpha \cdot \beta \in \^^{\|\beta\|-\|\alpha\|}$$ (in symmetry with how the wedge product adds grades). The rough intuition for the two operations, side by side, is:


1. The exterior (wedge) product $$\alpha \^ \beta$$ creates the subspace spanned by its two arguments, or $$0$$ if have any overlap.
2. The interior product $$\alpha \cdot \beta$$ removes the subspace $$\alpha$$ from $$\beta$$, or gives $$0$$ if $$\alpha$$ is entirely overlapped by $$\beta$$.

<aside class="toggleable" id="twosided" placeholder="Aside: Two-sided interior products <em>(click to expand)</em>">

Why left from right? It's basically arbitrary, and it would be reasonable to define another operation which does right-from-left. Some sources do it this way, and some do both and call them left-contraction and right-contraction, respectively.

And in fact, we could define a version which works both ways: $$\cancel{\b{x}} \cdot (\cancel{\b{x}} \^ \b{y}) = \b{y} = (\cancel{\b{x}} \^ \b{y}) \cdot \cancel{\b{x}} $$, canceling out as many common terms as it can and being symmetric in its arguments. This gets a bit weird, though, because it is useful for there to be a symmetry with the way that the wedge product gives $$\alpha \^ \beta = 0$$ if $$\| \alpha \| + \| \beta \| > n$$. When we define $$\cdot$$ as left-from-right only we have $$\alpha \cdot \beta = 0$$ if $$\| \beta \| - \| \alpha \| < 0$$, and some of the useful identities require this.

Sometimes I think a two-sided product would make more sense. But it turns out that one-sided contraction corresponds to a set operation (subtraction) and two-sided... doesn't, so I suspect there are a lot more identities for the one-sided version. We'll look at it more in a future article, I bet.

</aside>

The interior product of two $$k$$-vectors with each other is just their inner product (recalling that the wedge product with a scalar is defined to just be scalar multiplication):

$$\begin{aligned}
\<\un{\^^k}{\alpha}, \un{\^^k}{\beta}\> &= \< \alpha \^ 1, \beta \> \\
&= \< 1, \alpha \cdot \beta \> \\
&\boxed{= \alpha \cdot \beta} \in \bb{R}
\end{aligned}$$


Here are some other examples:

$$\begin{aligned}
\b{x} \cdot (\b{x \^ y}) &= \b{y} \\
\b{x} \cdot (\b{y \^ z}) &= 0 \\
(\b{x \^ y}) \cdot (\b{x \^ y}) &= 1 \\
(\b{x \^ y}) \cdot (\b{y \^ x}) &= -1 \\
(\b{x \^ y}) \cdot (\b{y \^ z}) &= 0 \\
(\b{x \^ y}) \cdot (\b{x \^ y \^ z}) &= \b{z} \\
(a_{xy} \b{x\^y} + a_{yz} \b{y\^z}) \cdot (b_{yz} \b{y \^ z}) &= a_{yz} b_{yz}
\end{aligned}$$


In general, you can compute interior products via a slight generalization of the method for inner products (now the antisymmetrization _must_ be performed on the right, though):

1. Write the right side (which has the higher grade) as an antisymmetric tensor product using $$\text{Alt}$$.
2. Contract term-by-term until you run out of terms.

For example:

$$\begin{aligned}
\b{a } \cdot (\b{b \^ c}) &= \b{a} \cdot \text{Alt}(\b{b \o c}) \\
&= \b{a} \cdot (\b{b \o c} - \b{c \o b}) \\
&= (\b{a \cdot b}) \b{c} - (\b{a \cdot c}) \b{b}
\end{aligned}$$

Applying this to a general product of multivectors shows that the interior product distributes over the wedge product like this:

$$\b{x} \cdot (\alpha \^ \beta) = (\b{x} \cdot \alpha) \^ \beta + (-1)^{|\alpha|} \alpha \^ (\b{x} \cdot \beta)$$

This property makes it a _graded antiderivation_, in the language of abstract algebra (so called because the above is a generalization of the product rule for derivatives), which is probably important to someone other than me. 

--------

### Note on Names and Definitions

The interior product, which I pronounce "dot", is so named for symmetry with the 'exterior' product, which I pronounce and write "wedge" because it's easier. I've defined it in terms of the inner product, but it would have totally been possible to define the interior product first and then just define the inner product as the case where both arguments have the same grade. This way just seemed easier.

On Wikipedia the product is written as $$\iota_{\b{x}}$$ (that's an 'iota'), and some other sources use the symbols $$\llcorner$$ or $$\lrcorner$$. I prefer to just use a dot product because things are a lot more compact that way. This does require some caution, though, because we might be used to dot products on vectors being reflexive ($$\alpha \cdot \beta \stackrel{??}{=} \beta \cdot \alpha$$), but that's not defined here except when both arguments have the same grade.

The reason it's sometimes called _contraction_ is because it is contraction. Specifically, it's the operation of contraction of tensors in the tensor algebra, transferred to the exterior algebra (which is usually modeled as quotient- or sub- algebra of the tensor algebra). On a tensor like $$T \in A \o A^* \o B$$, the contraction operation folds a copy of a vector space and its dual space to a scalar, such as $$A \o A^* \ra k$$. In practice, this looks like $$T = T_{ik}^j a^i \o a_j \o a^k \ra \sum_i T_{ik}^i a^k \in B$$, and corresponds to the contraction operation $$T_{ik}^i$$ in index notation, and to the trace $$\tr S_k$$ of the matrices given by $$S_k = T_{i k}^j$$. In index notation, a wedge product looks like an antisymmetrization, such as $$F_{ij} = A_{ij} - A_{ji}$$, and the contraction looks like it does on any other tensor: $$x \cdot F = x^i A_{ij} - x^i A_{ji}$$.

The reason it's sometimes called _insertion_ is from differential forms, which are the most well-known application of exterior algebra in math[^forms]. Differential $$k$$-forms are elements of "the exterior algebra on the cotangent space of a differential manifold", ie things like $$dx$$ and $$dx \^ dy$$ which you can integrate against. They are understandable as (_only_ as, to some people) linear operations on vectors: $$(dx \^ dy)(\b{a}, \b{b}) = dx(\b{a}) dy(\b{b}) - dy(\b{a}) dx(\b{b})$$. Then 'insertion' refers to inserting only one argument into a function which has more than one argument: $$\b{a} \cdot (dx \^ dy) = dx(\b{a}) dy - dy(\b{a}) dx$$. Incidentally, this is exactly the opration of [currying](https://en.wikipedia.org/wiki/Currying) in computer science; I find it useful to think of interior products as curried inner products.

[^forms]: One of the reasons I'm motivated to exposit about exterior algebra independent of discussion of differential forms is a lingering bitterness over having to (struggle to) learn both at once in college.

A lot of sources insist that the interior product have for its left argument an element of $$\^(V^*)$$, the "exterior algebra over the dual space of $$V$$", writing $$\b{x}^* \cdot \b{x} = 1$$ or $$\iota_{\b{x}^*}\b{x} = 1$$. As long as we're in a Euclidean space with $$\b{x}_i \cdot \b{x}_j = 1_{i=j}$$, this distinction is verbose, unnecessary, and distracting, so I'm not doing that.


-----------

### Properties of the Interior Product

Because the interior product is translatable via adjointness into a wedge product, it is independent of choice of coordinate system (for free!), and it obeys very similar properties. Let $$\iota_{\alpha}(\beta) \equiv \alpha \cdot \beta$$ and $$L_{\alpha}(\beta) \equiv \alpha \^ \beta$$ (this is the only time that the $$\iota$$ notation is useful). Then:[^interiorsource]

[^interiorsource]: A helpful source for this section: "Linear Algebra via Exterior Products" by Sergei Winitzki.


$$\begin{aligned}
L_{\alpha} &: \^^k V \ra \^^{k + |\alpha|} V \\
L_{c} \alpha &= c \alpha \\
L_{\b{u}} \circ L_{\b{v}} &= - L_{\b{v}} \circ L_{\b{u}} \\
L_{\b{u}} \circ L_{\b{u}} &= 0 \\
L_{\alpha \^ \beta} = L_{\alpha} \circ L_{\beta} &= (-1)^{|\alpha| |\beta|} L_{\beta} \circ L_{\alpha} \\
&\\
\iota_{\alpha} &: \^^k V \ra \^^{k - |\alpha|} V \\
\iota_{c} \alpha &= c \alpha \\
\iota_{\b{u}} \circ \iota_{\b{v}} &= -  \iota_{\b{v}} \circ \iota_{\b{u}} \\
\iota_{\b{u}} \circ \iota_{\b{u}} &= 0 \\
\iota_{\beta \^ \alpha} = \iota_{\alpha} \circ \iota_{\beta} &= (-1)^{|\alpha| |\beta|}  \iota_{\beta} \circ \iota_{\alpha} \\
\end{aligned}$$

Clearly the two operations have very similar structure. The one difference is in how they decompose into component operations -- the interior product reverses the order of its factors (which you can check via the adjoint relationship):

$$\begin{aligned}
(\alpha \^ \beta) \^ \gamma &= L_{\alpha \^ \beta} (\gamma) \\
&= L_{\alpha} L_{\beta} (\gamma) \\
&= \alpha \^ (\beta \^ \gamma) \\
&= (-1)^{|\alpha| |\beta|} \beta \cdot (\alpha \^ \gamma) \\
(\alpha \^ \beta) \cdot \gamma &= \iota_{\alpha \^ \beta} (\gamma) \\
&\stackrel{!!}{=} \iota_{\beta} \iota_{\alpha} (\gamma) \\
&= \beta \cdot (\alpha \cdot \gamma) \\
&\boxed{= (-1)^{|\alpha| |\beta|} \alpha \cdot (\beta \cdot \gamma)}
\end{aligned}$$

It is tempting to try to combine the two operations into one, since the interior product seems to act like a wedge product with an element of grade $$-1$$: something like $$\b{x}^* \^ \alpha \equiv \b{x} \cdot \alpha$$. But this would immediately break the associativity of $$\^$$ on scalars: $$\b{x} \cdot (\b{x} \^ \b{x}) = 0 \neq (\b{x} \cdot \b{x}) \^ \b{x} = \b{x}$$, and I'm not sure how to go about repairing that.

Of the identities above, we should particularly note that:

1. The interior product is not associative; rather, $$\alpha \cdot (\beta \cdot \gamma) = (\beta \^ \alpha) \cdot \gamma$$
2. The interior product of a constant on a vector is scalar multiplication: $$c \cdot \alpha = c \alpha$$
3. But the interior product _on_ a constant is 0: $$\alpha \cdot c = 0$$, unless $$\alpha$$ itself is constant: $$a \cdot c = ac$$.

These rules take some getting used to, but they're not arbitrary; they basically follow from considering the interior product as 'subtracting' or 'removing' the left argument from the right.

On any non-constant basis multivector $$\beta$$, one of $$\b{v} \^ \beta$$ and $$\b{v} \cdot \beta$$ will always be 0 and the other will not, and we have:

$$\boxed{L_{\b{v}} \circ \iota_{\b{v}} + \iota_{\b{v}} \circ L_{\b{v}} = I}$$

(This is not true for arbitrary multivectors, though, because both could be $$0$$. More on this someday.)

------

### Example: Projections and Rejections

We have seen that $$\b{a} \cdot (\b{u \^ v})$$ somehow 'removes' the $$\b{a}$$ part of $$\b{u \^ v}$$. How is that useful?

One example is to implement a sort of _division_ operation. We've all gotten used to the idea that there's no sense of division on vectors that works, such that $$\frac{\b{a \^ b}}{\b{a}}$$ would make sense. We can't quite get this, partly because our concept of multiplication isn't commutative, but we can do something similar. I claim that contraction by $$\b{a}^{-1} = \frac{\b{a}}{\Vert \b{a} \Vert^2}$$ (for $$\b{a} \neq 0$$) acts sort-of like division by $$\b{a}$$, in the following sense:

$$\begin{aligned}
\b{a}^{-1} \cdot (\b{a \^ b}) &= \frac{1}{\Vert \b{a} \Vert^2} ((\b{a \cdot a}) \b{b} - (\b{a \cdot b}) \b{a}) \\
&= \b{b} - \frac{\b{a \cdot b}}{\b{a \cdot a}} \b{a} \\
&= \b{b} - \b{b}_{\parallel \b{a}} \\
&= \b{b}_{\perp \b{a}}
\end{aligned}$$

Here $$\b{b}_{\parallel \b{a}}$$ refers to the projection of $$\b{b}$$ onto $$\b{a}$$, and $$\b{b}_{\perp \b{a}}$$ to the remainder, which is sometimes called the 'rejection'. This is also easily seen from writing $$\b{b}$$ this way in the initial product:

$$\begin{aligned}

\b{a}  \cdot (\b{a \^ b}) &= \b{a} \cdot (\b{a} \^ (\b{b}_{\perp \b{a}} + \b{b}_{\parallel \b{a}} )) \\
&= \b{a} \cdot (\b{a} \^ \b{b}_{\perp \b{a}} ) \\
&= \Vert \b{a} \Vert^2 \b{b}_{\perp \b{a}}
\end{aligned}$$

You can see how this is 'sort of' a division operation: in $$\frac{\b{c}}{\b{a}}$$, factor $$\b{c}$$ into parallel- and perpendicular- components, and the division removes the perpendicular part, in essence 'dividing out the factor of $$\b{a}$$'. If $$\b{c}$$ was constructed from the wedging ('multiplication') of orthogonal components, _then_ the interior product would act like 'division'.

This metaphor also explains why we can't just write $$\alpha \cdot \beta \cdot \gamma$$ without parentheses: it's basically the same ambiguity in writing division as $$c/b/a$$.

This projections continue to work for higher-dimensional objects. In the same way that we can factor, say, a vector $$\b{a}$$ into $$a_x$$ and $$a_y$$ components, we can factor a trivector $$\b{a \^ b \^ c}$$ into its projections onto a set of lines or planes. The same intuition suggests that we interpret the volume of an object as its 'projection onto a point'.

---------

## 2. The Complement (Hodge Star)

Let $$\omega \in \^^n V$$ be a unit pseudoscalar in $$\^ V$$.[^integral] For examples we'll use $$\omega = \b{x \^ y \^ z} \in \^^3 \bb{R}^3$$. The selection of $$\omega$$ (particularly, the order of its factors) determines a global orientation on $$V$$. Once we have an $$\omega$$, we define the **Complement** or [Hodge Star](https://en.wikipedia.org/wiki/Hodge_star_operator) or _Hodge Dual_ operation as the interior product with $$\omega$$:

[^integral]: In some texts, particularly those of Gian-Carlo Rota, this is called the _Integral_. I think that's a terrible name. He also calls multivectors 'extensors'. See for instance his "On the Exterior Calculus of Invariant Theory", which is very useful except for all the bad names.

$$\star \alpha \equiv \alpha \cdot \omega\tag{2}$$

I personally prefer the name 'complement', for reasons that will become clear in my next post, but the name "Hodge Star" is much more common. And I would prefer the overbar notation $$\bar{\alpha}$$, but I have to concede that it's often very useful to write it as a prefix operator when it has complicated arguments. So I guess we're sticking with using the star notation at least. Some people also write $$\ast \alpha$$ or $$\vert \alpha$$ or even $$\text{\textasciitilde} \alpha$$ instead of $$\star \alpha$$.

The complement operation takes a wedge product of basis vectors to the wedge product of all the _other_ basis vectors, with an appropriate sign such that $$\alpha \^ \star \alpha = \Vert \alpha \Vert^2 \omega$$. This is then extended linearly to all multivectors.

Examples in $$\bb{R}^3$$:

$$\begin{aligned}
\star 1 &= \b{x \^ y \^ z} \\
\star \b{x} &= \b{y \^ z} \\
\star \b{y} &= \b{z \^ x} \\
\star \b{z} &= \b{x \^ y} \\
\star (\b{x \^ y}) &= \b{z} \\
\star (\b{y \^ x}) &= -\b{z} \\
\star(a \b{z} + b\b{x}) &= a \b{x \^ y} + b\b{y \^ z} \\
\star^2 &= I
\end{aligned}$$

Examples in $$\bb{R}^2$$:

$$\begin{aligned}
\star \b{x} &= \b{y} \\
\star \b{y} &= -\b{x} \\
\star^2 &= -I
\end{aligned}$$

In $$\bb{R}^n$$:

$$\begin{aligned}
\star \b{x}_i &= (-1)^{i-1} \b{x}_1 \^ \ldots \cancel{\b{x}_i} \ldots \^ \b{x}_n \\
\star^2 \b{x}_i &=  (-1)^{i-1} (-1)^{n-i} \b{x}_i \\
&= (-1)^{n-1} \b{x}_i
\end{aligned}$$

$$\star^2$$ can be computed by considering how many terms each element of $$\star^2 \alpha$$ must move past to be rearranged into the starting order. The general form turns out to be:

$$\tag{3} \boxed{\star^2 \alpha = (-1)^{|\alpha||{\star \alpha}|} \alpha}$$

Mostly I prefer to define things without minus signs everywhere, but this one is, unfortunately, unavoidable.[^omega] We can also write the coefficient as $$(-1)^{\| {\star \alpha} \|} = (-1)^{n - \| \alpha \|}$$, but I prefer it with the star for lucidity.

[^omega]: However, I do think it's a bit unfortunate. If we had defined $$\star \alpha$$ such that $$\alpha \^ (\star \alpha) = (-1)^{\|\alpha\|} \omega$$ in the first place, then we would have had $$\star^2 = I$$. Maybe that would have been the right move. But this way is ubiquitous.

Clearly $$\star^2 = I$$ is always true for $$n$$ odd, and for $$n$$ even is true if $$k$$ is also even. It turns out that this particular negative-sign factor $$(-1)^{\|\alpha\|\|{\star \alpha}\|} $$ comes up a lot, and we'll often just write $$\star^2 \alpha$$ to abbreviate it. We can also write down the Hodge Star's inverse:

$$\star^{-1} \alpha = (-1)^{|\alpha|| \star \alpha |} (\star \alpha) = \star^3 \alpha$$


Because $$\star$$ is the complement _with respect to $$\omega$$_, it is specifically _not_ basis-independent, but if a linear transformation $$A$$ doesn't change $$\omega$$ (ie $$\det (A) = 1$$, ie it is orthogonal) then $$\star$$ is also unchanged. We think of a choice of $$\omega$$ not as a basis-dependent object but as a _geometric_ object, akin to saying "this is the scale I am using to measure volume", which then transforms according to whatever basis we are working in. This seems fine; anyway, a non-orthogonal change of basis would change our inner products also.


------
### A few identities


The inner product and complement operations can be defined in terms of each other, because of the following relationship for $$\alpha, \beta$$ of the same grade:

$$\un{k}{\alpha} \^ (\star \un{k}{\beta}) = \< \alpha, \beta \> \omega$$



It's also useful in the other direction:

$$\begin{aligned}
\un{k}{\alpha} \cdot \un{k}{\beta} &= \star (\alpha \^ (\star \beta)) \\
&= \star(\beta \^ (\star \alpha))
\end{aligned}$$

Written differently (using the fact that $$\star^2 \omega = \omega$$):

$$\star(\un{k}{\alpha} \cdot \un{k}{\beta}) = \star^2 (\alpha \^ (\star \beta)) = \alpha \^ (\star \beta)$$

This makes sense: if $$\< \alpha, \beta\> \neq 0$$, then they are 'parallel' multivectors, and thus $$\star \beta$$ and $$\alpha$$ have no overlap. That the signs and scalar factors work out is indicative of the usefulness of the $$\star$$ operation.

We can use this to show that the complement operation doesn't change the inner product of two $$k$$-vectors:

$$\begin{aligned} \< \star \un{k}{\alpha}, \star \un{k}{\beta} \>  &= \< \star \alpha, \beta \cdot \omega \> \\
&= \< \beta \^ \star \alpha, \omega \> \\
&= (-1)^{|\beta| |\star \alpha|} \< \star \alpha \^ \beta, \omega \> \\
&=  (-1)^{|\beta| |\star \alpha|} \< \beta, \star^2 \alpha \> \\
&= (-1)^{|\beta| |\star \alpha|} (-1)^{|\alpha| |\star \alpha|}  \< \beta, \alpha \> \\
&= (-1)^{2k |\star \alpha|} \< \beta, \alpha \>\\
&\boxed{= \< \alpha, \beta \>}
\tag{4}
\end{aligned} $$

We can generalize to arbitrary interior products, but it's a bit messy (I hope I computed these right!):

$$\begin{aligned} 
\alpha \cdot \beta &= \boxed{ (-1)^{(| \beta | - |\alpha |) |\star \beta|} (\star \beta) \cdot (\star \alpha)} \\
&= \boxed{\star((\star^{-1} \beta) \^ \alpha)} \\
&= (-1)^{| \star \beta| |\alpha|} {\star(\alpha \^ (\star^{-1} \beta))} \\
&= (-1)^{| \star \beta| (|\alpha| + |\beta|)} {\star(\alpha \^ \star \beta) }
\end{aligned} $$

<aside class="toggleable" id="derivation" placeholder="Derivation <em>(click to expand)</em>">

Many identities of this form are most easily derived by considering their behavior on pure multivectors in a space which is factored into the most convenient possible form. Since we're interested in the components along $$\alpha$$ and $$\beta$$ and their complements, we pick our unit pseudoscalar so that it factors into those parts:

$$\omega = \b{x}_{\alpha \beta} \^ \b{x}_{\bar{\alpha} \beta} \^ \b{x}_{\alpha \bar{\beta}} \^ \b{x}_{\bar{\alpha} \bar{\beta}}$$

We're interested only in cases where $$\alpha \cdot \beta \neq 0$$, so we can simplify this by removing the $$\b{x}_{\alpha \bar{\beta}}$$ and abbreviating some other terms (because all the terms of $$\alpha$$ are also in $$\beta$$):

$$\omega = \b{x}_{\alpha } \^ \b{x}_{\bar{\alpha} \beta} \^ \b{x}_{\bar{\beta}}$$

This gives $$\alpha = \b{x}_{\alpha}$$, $$\beta = \b{x}_{\alpha} \^ \b{x}_{\bar{\alpha} \beta}$$, $$\star \beta = \b{x}_{\bar{\beta}}$$, and $$\alpha \cdot \beta = \b{x}_{\bar{\alpha} \beta}$$. We can immediately guess that any other way of getting to this component is a valid identity, up to sign that we have to figure out:

$$\begin{aligned}
\alpha \cdot \beta &= \pm (\star \beta) \cdot (\star \alpha) \\
&= \pm \star((\star \beta) \^ \alpha)
\end{aligned}$$

To figure out what the $$\pm$$ should be, we compute in our convenient basis:

$$\begin{aligned}
(\star \beta) \cdot (\star \alpha) 
&= (\b{x}_{\bar \beta}) \cdot (\b{x}_{\bar{\alpha} \beta}  \^ \b{x}_{\bar \beta}) \\
&= (-1)^{| \bar{\alpha} \beta | |\bar \beta|} (\b{x}_{\bar \beta}) \cdot (\b{x}_{\bar \beta} \^ \b{x}_{\bar{\alpha} \beta}) \\
&= (-1)^{| \bar{\alpha} \beta | |\bar \beta|} \b{x}_{\bar{\alpha} \beta} \\
&= (-1)^{(| \beta | - |\alpha |) |\star \beta|} \b{x}_{\bar{\alpha} \beta} \\
&= (-1)^{(| \beta | - |\alpha |) |\star \beta|} (\alpha \cdot \beta)
\end{aligned}$$

And:

$$\begin{aligned}
\star((\star \beta) \^ \alpha)
&= \star( \b{x}_{\bar \beta} \^ \b{x}_{\alpha})\\
&= (-1)^{| \beta | |\star \beta|} \b{x}_{\bar{\alpha} \beta} \\
&= \alpha \cdot \star^2 \beta \\
&= \star((\star^{-1} \beta) \^ \alpha)
\end{aligned}$$

(In the $$\star$$ step, we can figure out the sign by requiring that $$X \^ (\star X) = \omega$$, which requires reordering $$\b{x}_{\bar{\beta}} \^ \b{x}_{\alpha } \^ \b{x}_{\bar{\alpha} \beta} $$ into $$\omega$$. We recognize the resulting factor as the coefficient of $$\star^2 \beta$$.)


</aside>

The complement of a wedge product turns out to be quite useful:

$$\begin{aligned} 
\star(\alpha \^ \beta) &= (\alpha \^ \beta) \cdot \omega \\
&= \beta \cdot (\alpha \cdot \omega) \\
&= \boxed{\beta \cdot (\star \alpha)} \\
&= (-1)^{|\alpha| |\beta| } \alpha \cdot (\star \beta) \\ 
\alpha \^ \beta &= \star^{-1} \beta \cdot (\star \alpha)

\tag{5}
\end{aligned}$$


There are many more identities that can be made by combining $$\^$$, $$\cdot$$, and $$\star$$, but they get really confusing, really fast. The next article will go over them all, promise, and in a much more general way. For now, let's prove some new stuff, because that's fun.

-------

## 3. The Cross Product

In $$\bb{R}^3$$, the familiar vector cross product can be defined as:

$$\b{a \times b} = \star(\b{a \^ b})$$

As we saw last time:

$$\begin{aligned}
\< \b{a \times b}, \b{c \times d} \> &= \< \star (\b{a \^ b}), \star (\b{c \^ d}) \> \\
&= \< \b{a \^ b}, \b{c \^ d} \> \\
&= (\b{a} \cdot \b{c}) (\b{b \cdot d}) - (\b{a \cdot d}) (\b{b \cdot c})
\end{aligned}$$

But now that we have better tools, we can do the more complicated ones: 

Here's the [scalar triple product](https://en.wikipedia.org/wiki/Triple_product#Scalar_triple_product), using identity (5):

$$\begin{aligned}
\b{a \cdot (b \times c)} &=  \b{a} \cdot \star (\b{b \^ c}) \\
&= \star(\b{b \^ c \^ a}) \\
&= \star(\b{a \^ b \^ c})
\end{aligned}$$

The [vector triple product](https://en.wikipedia.org/wiki/Triple_product#Vector_triple_product), using (5):

$$\begin{aligned}
\b{a \times (b \times c)} &= \star(\b{a} \^ \star(\b{b \^ c})) \\
&= (-1)^{| \b{a}| |\star(\b{b \^ c})|}  \b{a} \cdot \star^2 (\b{b \^ c}) \\
&= - \b{a} \cdot (\b{b \^ c}) \\
&= (\b{a} \cdot \b{c}) \b{b} - (\b{a} \cdot \b{b}) \b{c}
\end{aligned}$$

The [quadruple product](https://en.wikipedia.org/wiki/Quadruple_product), via the previous two:

$$\begin{aligned}
(\b{a \times b}) \times (\b{c \times d}) &= ((\b{a \times b}) \cdot \b{d}) \b{c} - ((\b{a \times b}) \cdot \b{c}) \b{d} \\
&= \star(\b{a \^ b \^ d}) \b{c} - \star(\b{a \^ b \^ c}) \b{d}
\end{aligned}$$

The [Jacobi Identity](https://en.wikipedia.org/wiki/Jacobi_identity):

$$\begin{aligned}
0 &= \b{a \times (b \times c)} + \b{b \times (c \times a)} + \b{c \times (a \times b)} \\

&= -{\star( \b{a} \cdot (\b{b \^ c}) + \b{b} \cdot (\b{c \^ a}) + \b{c} \cdot (\b{a \^ b}))} \\
&= -{\star( (\b{a \cdot b} - \b{b \cdot a}) \b{c} + (\b{b \cdot c - c \cdot b}) \b{a} + (\b{c \cdot a - a \cdot c}) \b{b})} \\
&= 0
\end{aligned}$$

The Jacobi Identity can also be rearranged into the following intriguing form, which we will contend with in a future article (the second line is our previous expansion of $$\b{a} \cdot (\b{b \^ c})$$. How could these be equal?):

$$\begin{aligned}
\b{a} \cdot (\b{b \^ c}) &= \b{b} \cdot (\b{a \^ c}) - \b{c} \cdot (\b{a \^ b}) \\
 &\stackrel{??}{=} (\b{b \cdot a}) \b{c} - (\b{c \cdot a}) \b{b}
\end{aligned}$$

---------

## 4. Application: Matrix Inversion

I mentioned that you can define the complement and the inner product in terms of each other, via:

$$\un{k}{\alpha} \cdot \un{k}{\beta} = \star(\alpha \^ \star \beta)$$

This works because the wedge product between grade-$$1$$ and grade-$$(n-1)$$ vectors looks like a dot product:

$$(a_x \b{x} + a_y \b{y} + a_z \b{z}) \^ (b_x \b{y \^ z} + b_y \b{z \^ x} + b_z \b{x \^ y}) = (a_x b_x + a_y b_y + a_z b_z) \omega$$

And in fact, there's a pretty good argument that this is the more natural definition, because it's the only good way to invert matrices.

Let $$A$$ be a square invertible matrix on $$\bb{R}^n$$. We know that the wedge product of all the columns $$\bigwedge A_i = \det(A) \omega$$ is not $$0$$. For each column $$A_i$$, the wedge product of all the _other_ columns $$A^{\^ n-1}_j =  (-1)^{j-1} A_1 \^ A_2 \^ \ldots \cancel{A_j} \ldots \^ A_n$$ will have:[^sign]

$$A_i \^ A^{\^ n-1}_j  = \det(A) 1_{i = j} \omega $$

[^sign]: The sign here is related to the sign of $$\star \b{x}_i$$.



$$A^{\^ n-1}_j$$ is an element of $$\^^{n-1} \bb{R}^n$$, but we can write the wedge product with it as a dot product, using $$\alpha \^ \beta = \star^{-1} \beta \cdot (\star \alpha)$$, which gives:

$$\begin{aligned}
A_i \^ A^{\^ n-1}_j  &= \star^{-1} (A_i \cdot \star A^{\^ n-1}_j) \\
&= A_i \cdot \star A^{\^ n-1}_j \\
&= \det(A) 1_{i = j}
\end{aligned}$$

Transposing so that the $$\cdot$$ becomes matrix multiplication:

$$(\star A^{\^ n-1})^T A = A (\star A^{\^ n-1})^T = \det(A) I$$

We recognize $$(\star A^{\^ n-1})^T$$ as the adjugate matrix of $$A$$. The inverse is, as usual:

$$A^{-1} = \frac{1}{\det(A)} (\star A^{\^ n-1})^T = \frac{\text{adj}(A)}{\det(A)} $$

-------

I think that just about covers the interior product. It's still definitely more confusing than it should be, but probably not so esoteric that we should only hear about it in the context of differential forms. It does at least make cross-product identities (and their differential analogs) easier to reason about.

Next time, we zoom out and try to figure out why there is so much structure here, and why it's so complicated despite that.