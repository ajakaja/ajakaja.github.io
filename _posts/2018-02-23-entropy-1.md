---
layout: blog
title: "Some Intuition Around Entropy"
math: true
aside: true
footnotes: true
---

*(Only interesting if you already know about information theory, maybe.)*
*(Disclaimer: these are just my notes from reviewing this. Don't trust me, I'm not a mathematician.)*

I have been reviewing concepts from [Information Theory]() this week, and I've realized that I never quite understood what (Shannon) [Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)) was all about.

Namely: Entropy is *not* a property of probability distributions per se; it's a property of streams of information. When we talk about 'the entropy of a probability distribution', we're implicitly talking about the stream of information produced by sampling from that distribution. Some equations make a lot more sense when you keep this in mind.

<!--more-->

## 1 Entropy

Recall that **Information** measures 'the number of bits (or some other unit) required to store data'. For example, an 8-bit number takes 8 bits of data. A binary digit, or the outcome of a coin flip encoded as a binary digit, takes 1 bit of data.

Meanwhile, **Entropy** is the *average amount of Information* learned by learning an unknown value, such as the outcome of a random variable that's selected from a probability distribution. It's a functional[^functional] which maps a probability distribution to a quantity of Information. It usually takes the symbol $$H$$ in information theory or $$S$$ in physics. I'll use $$H$$.

[^functional]: A function that acts on functions. I'll write it as $$H[]$$ instead of $$H()$$ to remind of this. The expectation value $$\bb{E}[]$$ is also a functional, sorta, but I also like to write it in blackboard type to make it stand out specially.

The formula for the entropy of a discrete probability distribution is:

$$H[p] = -\sum_{x} p(x) \log p(x)$$

Which can be better thought of as the 'expected value of $$-\log p(x)$$':

$$H[p] = \bb{E}[- \log p(x)]$$

Since entropy measures average information, $$-\log p(x)$$ must somehow capture that concept: it is the 'average information learned upon observing $$x$$".

This equation for entropy is *always positive*. The negative sign is misleading: since $$p(x)$$ is always less than 1, $$\log p(x) < 0$$. It would probably be better to always write $$\log \frac{1}{p(x)}$$ to emphasize this.

(It should also be mentioned that $$0 \log 0 = 0$$, because $$\lim_{p\ra 0^{+}} p \log p = \lim_{N \ra \infty} \frac{-\log N}{N} = 0$$, so 0-probability events contribute nothing to entropy.)

Some examples: the entropy of a fair coin flip is, as expected, 1 bit:

$$H = - p(\tt{H}) \log p(\tt{H})- p(\tt{T}) \log p(\tt{T}) = -\frac{1}{2} \log \frac{1}{2} -\frac{1}{2} \log \frac{1}{2} = 1$$

The entropy of a biased coin that has $$p(\tt{H}) = \frac{3}{4}$$ is:

$$H = - \frac{3}{4} \log \frac{3}{4} - \frac{1}{4} \log \frac{1}{4} = 2 - \frac{3}{4} \log 3 \approx 1.642$$

This means, intuitively, that as we make a sequence of $$N \ra \infty$$ outcomes from this unfair coin (with $$\ra \frac{3}{4}N$$ `H`s), it will be possible to compress that sequence in $$B \ra 1.642N$$ bits.

## 2 Microstates vs Macrostates

The discrete entropy formula $$H[p] = \bb{E}[- \log p]$$ is usually derived from some axioms about how it should behave, which I find useless for intuition. Instead, here's the easiest way to see way to see why it's defined that way.

Imagine a message space, like "the results of $$X$$ fair coin flips", with two descriptions, one *macro-* which describes how many $$\tt{H}$$ and $$\tt{T}$$ were seen, and one *micro-*, which describes the actual underlying sequence of results.

* At the microstate level, the system has $$M = 2^{X}$$ equiprobable outcomes, which takes $$X$$ bits of data: the exact value of each coin flip.
* At the macrostate level, there is a probability distribution that captures the chance $$p(x \tt{ Heads})$$ of getting exactly $$x$$ heads, because getting $$\frac{X}{2}$$ `Heads` is very likely and getting 0 isn't.

Specifically, the chance of $$x$$ `Heads` is:

$$p(x \tt{ Heads}) = \frac{\langle \tt{number of ways to get exactly x Heads} \rangle}{\langle \tt{possible outcomes}\rangle}$$

$$p(x) = \frac{\binom{X}{x}}{M} = \frac{m_{x}}{M}$$

So each macrostate "$$x$$ Heads" corresponds to $$m_{x}$$ microstates, out of the $$M$$ total possible microstates, with $$p(x \; \tt{Heads}) = \frac{m_{x}}{M}$$. (It's clear that $$\sum m_{x} = \sum \binom{X}{x}$$ has to equal $$M = 2^{X}$$ for this to be a probability distribution, and it does.)

The entropy of this distribution is:

$$H[p(x)] = - \sum \frac{m_{x}}{M} \log \frac{m_{x}}{M} = \log M - \frac{m_{x}}{M} \log m_{x}$$

$$= \log M - \bb{E}[\log m_{x}]$$

Since $$\log M = X$$ and $$\log m_{x}$$ is the number of bits required to specify a microstate *within* macrostate $$x$$, we can interpret this as:

$$H[p(x)] = \bb{E}[\langle \tt{information learned} \rangle] $$

$$= \langle \tt{total entropy} \rangle - \bb{E}[\langle \tt{remaining entropy} \rangle]$$

We see that the quantity $$I(x) = - \log p(x) = \log \frac{1}{p(x)}$$ term can be understood as: "if you tell me we're in state `x`, I still have to learn $$\log m_{x}$$ bits of data to get to the exact microstate, meaning that you just told me $$\log M - \log m_{x}$$ bits of data." (This quantity is called the [self-entropy](https://en.wikipedia.org/wiki/Self-information) or "surprisal" of outcome $$x$$.)

This shows that the two $$p$$s in the usual expression of entropy mean different things. Really, it's just an expression for $$\bb{E}[I(x)]$$, where $$x \in X$$ is an event that can happen with probability $$p$$. I think it should be viewed as incidental that the expression for $$I(x)$$ happens to include $$p(x)$$ also: there's nothing about that $$H = \bb{E}[\tt{information}]$$ that requires the information come in the form of probabilistic events. There's a probability distribution for *how much information we learn*, but it doesn't matter where that information *comes* from.

For instance: if I tell you 1 bit of information with probability $$\frac{1}{2}$$ and 100 bits with probability $$\frac{1}{2}$$", you can still calculate an entropy $$H = \frac{1}{2}(1 + 100) = 50.5 \; \tt{bits}$$. It doesn't matter that those bits themselves come from occurences of events with probabilities. It's just *any* information.

For a more interesting example: there's no requirement that the macrostate descriptions be exclusive. If I flip 100 coins, maybe I'll tell you "whether I got more than 50 heads" (1 bit of information) and "the exact sequence of results" (100 bits of information) with equal probability. That's still `50.5 bits` of data, but it's not a probability distribution over the set of possible macrostate descriptions -- some of the macrostates overlap.

(Yeah, okay, it's well-known that entropy really refers to information rates over channels, and that was Shannon's [model](https://en.wikipedia.org/wiki/Channel_capacity) anyway. But I didn't really internalize it until seeing this, and I think that it sheds a lot of light on what's going on with the definition.)

We imagine receiving an stream of events from $$P$$: "something happened", and breaking it down into *which* event happened.  The values from $$P$$ can only take on a single value, with $$\log 1 = 0$$ bits of entropy -- meaning that, as the number of events goes to infinity, the data required per bit goes to 0, because $$\lim_{N \ra \infty} \frac{\log N}{N} = 0$$. The stream of values from $$i$$ would naively be representable in $$\log \vert i \vert$$ information, but, knowing the probabilities lets us compress them into $$H[p] = - \sum p_{i} \log p_{i}$$ information. Wait

We can also hypothetically imagine further decomposing each $$i$$ into $$m_{i}$$ microstates, which take $$\log m_{i}$$ data to distinguish between. If the probabilities are irrational numbers, maybe we just do this to some reasonable fidelity. Or maybe it's just a tool for understanding. It doesn't really matter.

And, of course, the number of microstates we used dropped out of the calculation, letting entropy be defined purely in terms of probabilities.

Nevertheless I think it is appropriate to view entropy *not* as "the data required to encode a stream of events", but "the data required to re-encode a stream into another description language":
* The $$H[p]$$ we've been dealing with is the data per message for re-encoding of $$P$$ events into $$I$$ events.
* The $$\bb{E}[ \log m_{i} ]$$ is the data for re-encoding $$I$$ events into $$M$$ events.
* And, in fact, to say that data takes "N bits" to encode is really to say that that is the minimum average number bits required to re-encode it into binary.

This interpretation becomes important when we try to generalize our equation for entropy to continuous distributions, because we *can't* encode a stream of infinite-precision events perfectly. That's the topic of the next section.

## 3 The Continuous Analog of Entropy

The interpretation of entropy as 'average information to encode events' shows that the 'naive' approach to defining entropy on a continuous probability distribution $$p(x)$$ by just converting the integral sign to a summation:

$$H[p] = - \int_{\bb{X}} p(x) \log p(x) dx$$

must have something weird going on.

How? Well, what is that even trying to do? We want to be able to write the information learned as an expectation:

$$H[p(x)] = \bb{E}[I(x)]$$

that is, "Entropy is the average information learned through a channel of random values drawn from $$p(x)$$". 

But it's a probability density function: the probability of the variable $$x$$ to equal an exact value $$A$$ is 0, with $$I(x) = - \log 0 = \infty$$! And it makes that specifying the exact value would take 'infinite' information.

---

On the other hand, everything works fine if we're only talking about gradually refining the range of data. Since $$P(x \in (a,b)) = \int_{a}^{b} p(x) dx$$, the information learned by learning that $$x \in (a,b)$$ is:

$$I(x \in (a,b)) = - \log \int_{a}^{b} p(x) dx$$

If $$\; \cal{U}(a, b)$$ is a uniform distribution on $$(a,b)$$, with density function $$\frac{1}{b-a}1_{x \in (a,b)}$$, it makes sense that "dividing the range in half" takes exactly 1 bit of data (to specify which half):

$$H[\cal{U}(0, \frac{1}{2})] = H[\cal{U}(0, 1)] - \log \frac{1}{2} = H[\cal{U}(0, 1)] + 1$$

And formulas like this one work even if specifying the *exact* value would take infinite data.

Since we can't write down a finite value for $$U = H[\cal{U}(0, 1))$$, perhaps we can instead just reduce the distribution for $$P$$ to a function of $$U$$. We measure "how different $$p(x]$$ is from uniform", without fully reducing it the information required to specify exact values for $$x$$.

This is actually very similiar to the macro/micro-state idea above. Imagine partitioning $$p(x)$$ into tiny buckets of width $$\D x$$, such that $$p(x) \approx p(x + \D x)$$, so that the distribution over each bucket is basically uniform. Now we have a discrete distribution that we can calculate a finite entropy for, as long as we're okay with only speciying values of $$x$$ to the granularity of $$\D x$$.

We will end up with something like:

$$H[p(x)] = H_{\D x}[p(x)] + H[\cal{U}]$$

which we interpret as:

$$= \langle \tt{entropy from non-uniformity} \rangle + \langle \tt{entropy of continuum}\rangle$$

This is, like in the previous section, the entropy associated with 'changing languages' (or, in this case, 'changing variables', to one that makes $$p(x)$$ totally flat by distorting the $$x$$ axis!)

(Perhaps our variable isn't continuous, but is granular at a much smaller length scale than we're interested in going. Then we could find a meaningful value for the second term, but we'll still want to discard it.)

This resulting formula is called [differential entropy](https://en.wikipedia.org/wiki/Differential_entropy), and is given by, as expected:

$$H[p] = - \int_{X} p(x) \log p(x) dx$$

But it's *not* the limit of the discrete formula on infinitesimal partitions; that's this:

$$H_{N}[p] = \log N + H[p]$$

And while $$\lim_{N \ra \infty} H_{N}[P]$$, the limit of the discrete expression, goes to infinity, the partial entropy relative to a uniform distribution $$\lim_{N\ra \infty} H[p] = H_{N}[p] - \log N$$ does not.

Click below if you want to see a long derivation:

<aside class="toggleable" placeholder="<b>Aside</b>: (most of) A Derivation of Continuous Entropy <em>(click to expand)</em>">

*(I say "most of" because there is an analytically questionable step. You'll see.*)

First we need to calculate the expression for $$U$$, the 'entropy of the continuum'.

Let's imagine that our space has a minimum granularity $$M$$, so the uniform distribution $$\cal{U}(0, 1)$$ is actually picking a value from $$M$$ equiprobable values. We won't specify what $$M$$ is, so we're free to let it be as small as we need. Then:

$$U_{M} = H[\cal{U}_{M}(0,1)] = \log M$$

And:

$$H[\cal{U}_{M}(a, b)] = \log M  + \log (b-a)$$

That is, it takes $$\log (b-a)$$ bits to reduce $$\cal{U}(a, b)$$ to $$\cal{U}(0,1)$$, and $$\log M$$ more to specify a value exactly. Note that $$b-a$$ might be *less* than 1 (making the logarithm negative), or even less than $$-\log M$$ for any particular value of $$M$$, but the formula still works: since we can pick $$M$$ to be as high as we want, we can always pick it to make $$\log M + \log (b-a) > 0$$.

Now, to avoid our expression for $$H[p]$$ blowing up as $$M \ra \infty$$, we can compute the limit $$\lim_{M \ra \infty} H[p(x)] - \log M$$ instead, and the $$M$$s *should* cancel and leave a finite result. So let's do it!

<aside class="toggleable" placeholder="<b>Aside</b>: Units in logarithms <em>(click to expand)</em>">
By the way: what do we make of a logarithm of a quantity that units, like $$\log p(x)$$? Since $$p(x) dx$$ is unitless, $$p(x)$$ itself has units of $$\tt{length}^{-1}$$. 

I've never seen it mentioned explicitly, but the logarithm identity $$\log \frac{a}{b} = \log a - \log b$$ implicitly changes division of units into subtraction, whatever that means! If $$a$$ and $$b$$ have units $$L$$, would $$\log a$$ has units $$\log L$$? I really don't know. Regardless, we can skirt the issue by including the unit element from the space $$x$$, which I'll label as $$1_{x}$$:

$$\log \frac{a}{b} = \log \frac{1_{x} a}{1_{x} b} = \log \frac{a}{1_{x}} - \log \frac{b}{1_{x}}$$

If $$a$$ is a quantity with units of $$\tt{length}$$, then $$\frac{a}{1_{x}}$$ is a numerical ratio of lengths, which itself has no units and can happily be logarithm'd.

As it happens, though, the $$1_{x}$$ term doesn't matter. As far as I can tell it's implicit all over the place -- for example, a distribution $$p(x) = 2x$$ is *actually* $$p(x) = \frac{2x}{1_{x}^{2}}$$. Or maybe using the proper indicator function for the support, $$1_{x \in (0, 1)$$, should be thought of as having 'x' units on it. I don't know, but, either way, it's not worth sticking $$1_{x}$$s everywhere.
</aside>

Let's compute the 'partial entropy', relative to a granularity $$M$$, of a continuous distribution $$p(x)$$, via a limit of the discrete formula. Define $$\D x = \frac{1}{M}$$, and write:

$$H[p] \Ra \lim_{M \ra \infty} H[p(x)] - \log M $$

$$= \lim_{M \ra \infty} -\sum_{X} \big[ \int_{x}^{x + \D x} p(x') dx' \big] \log \int_{x}^{x + \D x} p(x') dx' - \log M $$

Using the fact (er, assumption) of smoothness that for small enough $$\D x$$, $$\int_{x}^{x + \D x} p(x') dx' \approx p(x) \D x $$:

$$= \lim_{M \ra \infty} \big[ - \sum_{X} \big[ p(x) \D x  \big] \log (p(x) \D x ) - \log M \big]$$

$$= \lim_{M \ra \infty}  \big[ - \sum_{X} (p(x) \D x \log p(x) +  \log M \sum_{X} p(x) \D x - \log M \big] $$

The second half is the limit: $$\lim_{M \ra \infty} \log M (\sum_{X} \frac{p(x)}{M} - 1)$$. It should equal $$\log M (\int_{x} p(x) dx - 1) = 0$$, but... I'm not aware of how to be analytically correct about it so I'll have to wave my hands and say it seems to cancel out. Try it, it's tricky. Maybe it doesn't work!

Moving on:

$$H[p(x)] - \log M = \lim_{M \ra \infty}  - \big[ \sum_{X} (p(x) \D x \log p(x) \big] $$

$$= - \int_{X} p(x) \log p(x) dx$$

So that's 'the entropy of a continuous distribution $$p(x)$$, relative to the uniform distribution $$\cal{U}(0, 1)$$'. I'm not an analysis person, and I definitely waved my mathematical hands a few times in there, but I'm pretty happy with this as an explanation. It's basically the same conclusion [here](https://en.wikipedia.org/wiki/Limiting_density_of_discrete_points), but written out the way I figured it out for myself.

</aside>

Note that differential entropy *can* be negative, because it's easy to write down a probability distribution on a continuous space that has *less* entropy than $$\cal{U}(0,1)$$: for example, any $$\cal{U}(a,b)$$ with $$(b-a) < 1$$.

(I assume that saying distribution $$A$$ has negative entropy relative to distribution $$B$$ means that you encode $$B$$ into $$A$$ efficiently, rather than the other way around!)

Also, it does not survive changes-of-variables in the integral. Why? Because it's relative to a uniform distribution *on the variable $$x$$*. The differential entropy on another variable $$u = f(x)$$ may rescale $$\cal{U}(0,1)$$. (I guess it should be the same relative to the entropy of $$f(\cal{U}(0,1))$$?)

Maybe we should be more explicit by writing:

$$H[p] = \lim_{N \ra \infty} H_{N}[p] - H_{N}[\cal{U}(0,1)]$$

By the way, if this wasn't so long, I'd go on and talk about [relative entropy](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), which generalizes this idea of the entropy of a distribution relative to another. I also need to learn it first. Maybe another time!

## 4 Final Thoughts

I think it's funny that entropy seems so special. Really, the entire theory is just, like, 'the logarithm of probability theory'. If you have two probability spaces that take $$A$$ and $$B$$ possible states, you could write that the compression ratio is $$R = \frac{A}{B}$$, or you could write that the entropy is $$\log R = \log A - \log B$$. It really doesn't matter.

Well, there's a way it could matter: it might matter if take the average of information, vs taking the information of the average sized state. Right? For example, in the micro/macro-state example:

$$H = \bb{E}[-\log p] =  -\sum_{i} p_{i} \log p_{i}$$

We can't just take an average first; it will give a different value: $$\bb{E}[\log p_{i}] \neq \log \bb{E}[p_{i}] $$.

But, really, the problem is that we'd like some sort of 'multiplicative' expectation value, but that doesn't exist so we go around taking logarithms to make it easier to write! I mean, suppose it was called $$\bb{G}[p]$$:

$$-\log \bb{G}[p] = -\log \prod_{i} p_{i}^{p_{i}} = - \sum_{i} p_{i} \log p_{i} = \bb{E}[-\log p]$$

This basically generalizes the geometric mean. I assume there are similar versions for any other [generalized mean](https://en.wikipedia.org/wiki/Generalized_mean).

Point being, all of these things are just ratios that have been flattened out into sums because it makes them easier to think about.

-----

Also, I wonder if more can be said about exactly *how* we map languages into each other. When we say a space $$P$$ can be represented by $$H[P]$$ bits, we literally mean it: there exists a function with the property of mapping strings of bits to elements with an average ratio of $$H[P]$$, and no function can exist that does better than that. But I'm curious about trying to implement these mappings on, say, different kinds of formal languages. For instance -- the space of sequences of coin flips until tails $$\{ \tt{T}, \tt{HT}, \tt{HHT}, \ldots\}$$ clearly is represented by a regular language `H*T`. Maybe we can make some kind of more correspondence between a state machine of binary strings to a state machine of this sequence as the encoding. Maybe that'd be interesting.

-----

Also, I'm annoyed that the seemingly coherent concept of differential entropy makes _yet another example_ where it seems that our fear of infinities is a real problem. Like, can't we find a better way to handle them than carefully trodding through limits and trying to cancel them out?

-----

Also, I wonder if the quantum mechanical [version of entropy](https://en.wikipedia.org/wiki/Von_Neumann_entropy) is more easily understood in terms of being a description 'relative to a uniform distribution', like I did above.

Because, uh, everyone seems pretty happy just throwing their hands up when they see negative values for differential entropy without trying to interpret them. Ah well.
