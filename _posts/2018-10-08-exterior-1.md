---
layout: blog
title: "Exterior Algebra Notes 1: matrices and determinants"
footnotes: true
math: true
aside: true
tags: notes
---

*(This is not an intro to the subject. I don't have an audience in mind for this. It's notes that have been dressed up as a blog post, which may be useful if you're trying to understand this stuff.)*

*(Vector spaces are assumed to be finite-dimensional and over $$\bb{R}$$ with the standard inner product)*

[Exterior algebra](https://en.wikipedia.org/wiki/Exterior_algebra) (also known as 'multilinear algebra') is a pretty technical subject. It gets discussed in abstract algebra and mostly used in differential geometry.

I happen to think it deserves to be _more_ commonplace, but the reasons why aren't exactly obvious. I think it takes a lot of the mysteriousness out of the otherwise technical and tedious subject of linear algebra, and turns out to be used in disguise all over the place.

I'm not entirely sure what my goals with studying mathematics are, but I'm drawn to whatever makes computation and intuition simple, and this is that. In college I learned about determinants and matrix inverses and never really understood how they work; they were impressive constructions that I memorized and then mostly forgot. Exterior Algebra turns out to make them into simple constructions that you could rederive whenever you wanted.

This stuff took me a long time to understand, so here's a lot of text explaining what I figured out. Maybe it will save you time.

<!--more-->

----

## 1. An example problem

Suppose: you're writing a computer graphics library. You want to have everything be lit up by the sun, which is in a fixed direction in the sky. To do this, each surface will be drawn with a brightness based on the angle it makes with the light -- because being pointed directly at the sun makes surfaces brighter than being at an $$89 \degree$$ angle to them, right?

For each object surface in your world, you store the _surface unit normal vector_ $$\b{n}$$, which points out of the surface, and compute the brightness of that object in the sun. The formula for the brightness scaling factor $$c$$ is just the dot product of the normal with the direction of the sun:

$$c = \b{n} \cdot \b{v}_{light}$$

But every vector in your space is transformed many times before it ends up being in its final, on-screen coordinates -- because they move around over time, and your perspective changes, etc. You just store the transformation to apply to the vectors as a matrix called, say, $$R$$. When it comes time to compute how the _normals_ transform in the lighting calculation, you go with the obvious choice: $$\b{n} \ra R\b{n}$$. But nothing looks right. You give up and search online and read [this](https://webgl2fundamentals.org/webgl/lessons/webgl-3d-lighting-directional.html):

> There is one problem which I don't know how to show directly so I'm going to show it in a diagram. We're multiplying the normal by the u_world matrix to re-orient the normals. What happens if we scale the world matrix? It turns out we get the wrong normals. 
> ...
> I've never bothered to understand the solution but it turns out you can get the inverse of the world matrix, transpose it, which means swap the columns for rows, and use that instead and you'll get the right answer.

Turns out that normal vectors aren't vectors at all. They're _bivectors_ and have to transform like them or you get the wrong answer.

There are other sources where you can learn all about bivectors and multivectors and everything else. I mention this to say that this stuff isn't just abstract elegant nonsense; it's got actual implications if you vector math in real life. Actually, I think it's more far-reaching and important than, well, anyone seems to think it is, which is why I'm writing about it, but we'll get back to that later.

The answer, by the way, is that a normal vector that we store as $$\b{n} = n_x \b{x} + n_y \b{y} + n_z \b{z}$$ is really behaving as a bivector:

$$\b{n} = n_x \b{y \^ z} + n_y \b{z \^ x} + n_z \b{x \^ y}$$

Bivectors created from elements of $$\bb{R}^3$$ are elements of a vector space called $$\^^2 \bb{R}^3$$, which is spanned by $$\{ \b{ x \^ y, y \^ z, z \^ x }\}$$. If vectors are transformed by a matrix $$R$$, then bivectors transform according to a matrix called $$\^^2 R$$, which is defined by the property:

$$\^^2 R (\b{z}) = R(\b{x}) \^ R(\b{y})$$

and likewise for $$\b{x} \lra \b{y} \^ \b{z}$$ and $$\b{y} \lra \b{z} \^ \b{x}$$.

For a visual example of how this works, suppose our $$R$$ is the matrix $$\text{diag}(1,1,-1)$$ which takes $$\b{z} \ra -\b{z}$$ and leaves the other directions unchanged. The lighting vector of an $$xy$$ plane would point in the $$\b{z}$$ direction and so be rotated by this, but it shouldn't be; after the application of $$R$$ the surface normal should _still_ be $$+ \b{z}$$. The vector is actually $$\b{n}_z \b{x \^ y}$$, which $$R$$ doesn't affect.

It _so happens_ that in $$\bb{R}^3$$, $$\^^2 R= (R^{-1})^T$$, so you can get away with transforming $$\b{n}$$ using $$(R^{-1})^T$$ without really knowing what you're doing, which is what the computer graphics people are doing.

--------

## 2 Multivector Matrices

Here are some notes about how linear transformations work on multivectors, because it took me a while to understand this and I had to write it out.

### Notation

The first problem with getting deep into exterior / multilinear algebra is that the equations quickly become unwieldy. Determinants were already enough of a pain to deal with, and now we want to generalize them? So we're going to just look at matrices for a while and develop a smoother notation for them.

We'll use index notation here for matrices, with **subscripts for columns** and **superscripts for rows**, so that $$A^j_i$$ is the $$j$$'th row and $$i$$'th column. 

$$A = \begin{pmatrix} A_1^1 & \ldots & A_n^1 \\ \vdots & \ddots & \vdots \\ A_1^n & \dots & A_n^n \end{pmatrix}$$

A matrix like $$A$$ represents a linear transformation between two spaces, $$U \ra V$$, which is isomorphic to an element of $$U^* \otimes V$$: $$A = A_i^j \b{u}^i \o \b{v}_j$$.

By convention we write elements of $$U^*$$ as _row vectors_, so they have multiple _columns_, indexed by subscript: $$\b{u} = \{ u_i \} = \{ u_1, u_2, \ldots u_{\| U \|} \}$$. Similarly elements of $$V$$ are column vectors, which have multiple _rows_ indexed by superscript: $$\b{v} = \{ v^j \} = (v^1, v^2, \ldots v^{\| V \|})^T$$. We use the transpose operation $$\b{u} \ra \b{u}^T$$ to map rows to columns, which means it both raises/lower indices and it takes $$U \lra U^*$$. This is not that elegant; we should really use one symbol instead of both the transpose and star, but oh well.

When we write $$A_i^j$$, we are picking the $$i$$'th column (selecting a coordinate in $$U^*$$) and the $$j$$'th row (a coordinate in $$V$$). The symbol $$A_i^j$$ refers to the _scalar_ value of this matrix entry, with no basis vectors attached.

It's useful to refer to the rows and columns of a matrix in isolation, by omitting one of the indexes. When we do this I like to write a vector symbol, $$\vec{A}$$, to remind ourselves that we're dealing with a vector. A lower index only $$A_j$$ means the $$j$$'th column vector; an upper index only $$A^i$$ means the $$i$$'th row vector. Therefore:

$$A = \begin{pmatrix} \vec{A}_1 & \ldots \vec{A}_n \end{pmatrix} = \begin{pmatrix} \vec{A}^1 \\ \vdots \\ \vec{A}^n \end{pmatrix}$$


We can think of a particular column of a matrix like $$\vec{A}_1$$ as its action on a particular basis vector from $$U$$: $$\vec{A}_1 = A(\b{u}^1)$$. Similarly a particular row is given by the action of $$A$$ on a particular basis covector from $$V^*$$: $$\vec{A}^1 = \b{v}^T_1 A$$.


-----

### Linear Transformations on $$\^^k V$$

A multivector's components are indexed by the basis elements of the space it lives in, so, just as a vector $$\b{v}$$ has a $$v_x$$ component, a bivector $$\b{b}$$ has a $$\b{b}_{x \^ y}$$ component. You could ask for the $$\b{b}_{y \^ x}$$ component, but that would be like asking for $$\b{v}_{-x}$$: redundant.

I like to use _multi-indexes_ to refer to multivector components, which are labeled with capital letters like $$I$$ and refer to tuples: $$I = (i_1, i_2, \ldots i_k)$$ (usually it's obvious what $$k$$ is). $$\b{v}_{\^ I}$$ means:

$$\b{v}_{\^ I} = \b{v}_{i_1} \^ \b{v}_{i_2} \^ \ldots \^ \b{v}_{i_k} \in \^^k V$$ 

For any basis _multivector_ $$\b{v}_{\^I} \in \^^k V$$, $$A$$ also generates as a linear transformation, which we write as $$A^{\^k}$$ (some people write it as $$\^^k A$$):

$$A^{\^ k} \in \^^k U \ra \^^k V$$

It's defined by its action on any vector in $$\^^K U$$:

$$A^{\^k} (\b{u}_{\^ I}) \equiv A(\b{u}_{i_1}) \^ A(\b{u}_{i_2}) \^ \ldots A(\b{u}_{i_k})$$

Since $$A$$ is just a linear transformation, we can write it out as a matrix. If $$\dim U = m$$ and $$\dim V = n$$, then $$\dim \^^k V = {n \choose k}$$ and $$\dim \^^k U = {m \choose k}$$, so $$A^{\^ k}$$ is an $${m \choose k} \times {n \choose k}$$ matrix.

The key insight for understanding matrices such as $$A^{\^ k}$$ is that they are _just matrices_, albeit in a different vector space ($$\^^k V$$), and all of the usual matrix knowledge applies.


Each column of $$\^^k V$$ is the action of $$A$$ on a particular $$k$$-vector, which results in a linear combination of $$k$$-vectors. It does not really matter what order we write the columns in, as long as we use the same ordering for anything the matrix acts on. This is true for regular matrices also, of course -- you're always free to start writing $$\bb{R}^3$$ as $$(\b{x, z, y})$$ if you really want to; you just need to swap all the coordinates appropriately.

Important special cases.

* $$A^{\^ 1}$$ is just $$A$$. 
* If $$\dim U = \dim V = n$$, then $$A^{\^ n} = \det A \b{u}_{\^ I}^T \o \b{v}_{\^ I}$$. That is, it's the **determinant** of $$A$$, except considered as a linear transformation from $$\^^N V \ra \^^n V$$, meaning that it has basis vectors attached rather than being a dimensionless scalar.
	* This is a (scalar) mapping between _volumes_ of $$U$$ to volumes of $$V$$.
	* Most of the properties of $$\det$$ follow directly from this interpretation. (eg: non-linearly-independent matrix -> degenerate volume -> 0 determinant; composition of transformations -> multiplicativity of determinants.)
* If $$U=V$$ then $$A^{\^ n - 1}$$ is the _adjoint_ of $$A$$, except written in the wrong basis, which you might [have to fix]({{ site.baseurl }}{% post_url 2018-05-27-how-to-invert-a-matrix %}). 
* For completeness it's useful to define $$\^^0 A$$ as $$1 \in \bb{R} = \^^0 V$$.


**Example:**

Consider the sloped rectangle in $$\bb{R}^3$$ formed by the points $$(a,0,0), (a, 1, 0), (0,1,b), (0,0,b)$$. Its area is given by the bivector:

$$\b{b} = a \b{x \^ y} + b \b{y \^ z} \in \^^2 \bb{R}^3$$

It's a linear combination of some area on the $$\b{xy}$$ and $$\b{yz}$$ planes. Its total scalar area can be computed by taking the magnitude of this bivector ($$\| \b{b} \| =\sqrt{a^2 + b^2}$$), which shows how areas and $$k$$-volumes in general obey a version of the Pythagorean theorem.

Suppose $$A = \begin{pmatrix} 0 & 0 & 3 \\ 2 & 0 & 0 \\ 0 & -1 & 0 \end{pmatrix}$$. Its action on $$\b{b}$$ is:

$$\begin{aligned} \alpha &\ra a (A \b{x}) \^ (A \b{y}) + b (A \b{y} \^ A \b{z}) \\
&= a (2 \b{y} \^ - \b{z}) + b(- \b{z} \^ \b{x}) \\
&= -2a \b{y \^ z} - b \b{z \^ x}
\end{aligned}$$

We see that it $$A$$ does not multiply $$\b{b}$$ by a scalar, but rather takes it to $$\| \b{b} \| \ra \| A^{\^2} \b{b} \| = \sqrt{4a^2 + b^2}$$.



------

### More Multivector Matrices

In this section let $$U = V = \bb{R}^3$$. Then:

$$\begin{aligned}
A^{\^1} &= A = (A \b{x}, A \b{y}, A \b{z}) \\
A^{\^2} &= (A^{\^2}(\b{x} \^ \b{y}), A^{\^2}(\b{y} \^ \b{z}) , A^{\^2}(\b{z} \^ \b{x}) ) \\
A^{\^3} &= A^{\^3}(\b{x \^ y \^ z}) = A \b{x} \^ A \b{y} \^ A \b{z} = \det (A) \; \b{x \^ y \^ z} \end{aligned}$$

Let's write out $$A^{\^2}$$ componentwise. It's a $$3 \times 3$$ matrix: it has three columns, which are shown, and each of those produces a bivector with three rows. The rows' data is the coordinates of each of these transformations, for instance:

$$
A^{\^2} (\b{x \^ y}) = A(\b{x}) \^ A(\b{y})
= \begin{pmatrix} A_x^x A_y^y - A_x^y A_y^x \\[3pt] A_x^y A_y^z - A_x^z A_y^y \\[3pt] A_x^z A_y^x - A_x^x A_y^z \end{pmatrix} = 
\begin{pmatrix} A^{\^2}(\b{x \^ y})_{\b{x \^ y}} \\[3pt] A^{\^2}(\b{x \^ y})_{\b{y \^ z}} \\[3pt] A^{\^2}(\b{x \^ y})_{\b{z \^ x}}  \end{pmatrix}$$

We can also extract a row, the same way we can extract a row from a regular matrix via $$\b{x}_i^T A$$:

$$\begin{aligned}
(\b{x}^T \^ \b{y}^T) (A^{\^2}) &= (\b{x}^T A) \^ (\b{y}^T A) \\
&= \big(
(A_x^x A_y^y - A_x^y A_y^x) , 
(A_y^x A_z^y - A_y^y A_z^x) ,
(A_z^x A_y^y - A_z^y A_x^x) \big)
\end{aligned}$$


Each component $$A_{\^ I}^{\^ J}$$ of $$A^{\^k}$$ is the determinant of a $$k \times k$$ [minor](https://en.wikipedia.org/wiki/Minor_(linear_algebra)) of $$A$$: the minor created by the columns $$I$$ and the rows $$J$$ (possible with a minus sign, depending on their permutations). I prefer this notation because it emphasizes exactly the meaning of that value: that it is a component of the matrix $$A^{\^ k}$$. Here's an example:

$$(\b{x \^ y})^T A (\b{x \^ y}) = \begin{vmatrix} A_{xx} & A_{xy} \\ A_{yx} & A_{yy} \end{vmatrix}$$

Minors on the diagonal are called _principle minors_. The diagonal elements of $$A^{\^ k}$$ are the determinants of these.

Note that we can take expand a matrix in its columns or rows first and get the same answer. Also note that in general, $$(\^^k V)^T \simeq \^^k V^T$$, at least in finite dimensional spaces, so we can compute either of $$(A^{\^ k})^T = (A^T)^{\^k}$$. Either way you end up with coordinates on each of the pairs of the $$\^^k V$$ basis vectors.

This notation gets pretty clunky. I find it useful to label things like this:

<aside>

**Proposed notation**: 

$$A_{x \^ y} = A \b{x} \^ A \b{y}$$ by definition, extracting the $$\b{x} \^ \b{y}$$ column of $$A^{\^ 2}$$. 

$$A^{x \^ y} = (\b{x}^T A) \^ (\b{y}^T A)$$ extracts the $$\b{x} \^ \b{y}$$ row of $$A^{\^ 2}$$. 

$$A_{x \^ y}^{u \^ v}$$ extracts the (scalar) value of the $$\b{x \^ y}$$ column and $$\b{u \^ v}$$ row.

Using these we can expand $$A^{\^ 2}$$ like this:

$$A^{\^ 2} = 
\begin{pmatrix} 
A_{x \^ y}^{x \^ y} & A_{y \^ z}^{x \^ y} & A_{z \^ x}^{x \^ y}  \\[5pt]
A_{x \^ y}^{y \^ z} & A_{y \^ z}^{y \^ z} & A_{z \^ x}^{y \^ z}  \\[5pt]
A_{x \^ y}^{z \^ x} & A_{y \^ z}^{z \^ x} & A_{z \^ x}^{z \^ x} \end{pmatrix} = 
\begin{pmatrix} A_{x \^ y} & A_{y \^ z} & A_{z \^ x} \end{pmatrix}
= \begin{pmatrix} A^{x \^ y} \\[5pt] A^{y \^ z} \\[5pt] A^{z \^ x} \end{pmatrix}$$

Higher powers of $$A^{\^ k}$$ are the same idea, but indexed by the basis multivectors $$\b{x}_{\^ I} \in \^^k V$$.

As with regular matrices, whichever components we index by do not appear in the final value, so, for instance, $$A_{x \^ y}^{x \^ y}$$ is a _scalar_. For a further example: notice how these three equations say slightly different things, because each index that we use is a basis multivector which does not appear on the right side:

$$A_{x \^ y \^ z} = \det (A) \, \b{x} \^ \b{y} \^ \b{z}$$

$$A_{x \^ y \^ z}^{x \^ y \^ z} = \det (A)$$

$$A^{\^3} = \det (A) \, (\b{x} \^ \b{y} \^ \b{z})^T \otimes (\b{x} \^ \b{y} \^ \b{z}) $$

(Whenever possible, $$\det(A)$$ will refer to the _scalar_ value of $$A^{\^n}$$. Otherwise things get very confusing.)

</aside>


------

### Different dimensions

If $$A$$ is a map between two _different_ vector spaces, we can still talk about $$A^{\^k}$$. It just has indexes from the two different spaces. Intuitively, this is a linear transformation which maps areas (or $$k$$-volumes) in one space to areas ($$k$$-volumes)  in the other, and there's no requirement that they be the same space for that to make sense. They don't even need to have the same dimension.

If $$\dim U = m$$ and $$\dim V = n$$ and $$A : U \ra V$$, then $$A^{\^k}$$ can be written as a $${m \choose k} \times {n \choose k}$$ matrix.

Suppose $$A : \bb{R}^2 \ra \bb{R}^3$$, and we label the $$\bb{R}^2$$ by basis vectors $$\b{u,v}$$. Then:

$$\^^2 A = \begin{pmatrix}
A_{u \^ v}^{x \^ y} \\[5pt]
A_{u \^ v}^{y \^ z} \\[5pt]
A_{u \^ v}^{z \^ x}
\end{pmatrix}$$

Also:

$$\^^2 A^T = \begin{pmatrix}
(A^T)^{u \^ v}_{x \^ y} &
(A^T)^{u \^ v}_{y \^ z} &
(A^T)^{u \^ v}_{z \^ x}
\end{pmatrix}$$

Concretely:


$$A = \begin{pmatrix} a & b \\ c & d \\ e & f \end{pmatrix}$$

$$\^^2 A = \begin{pmatrix} ad-bc \\ cf-de \\ eb - fa \end{pmatrix}$$

$$\^^2 A^T = \begin{pmatrix} ad - bc & cf - de & eb - fa \end{pmatrix}$$


No matter what, $$A^{\^k}$$ is a map between $$\^^k U \ra \^^k V$$, and if $$k > \dim U$$ or $$k > \dim V$$ then one of those spaces is zero-dimensional and $$A$$ is just 0. In the case above, $$A^{\^ 3} = 0$$.

(Technically there is nothing stopping us from discussing a linear transformation which maps, like, areas in one space to volumes in another. But those are not going to be common.)
