---
layout: blog
title: "Exterior Algebra as Linearized Set Theory"
footnotes: true
math: true
aside: true
tags: notes
---

*Part 4 of an excessively thorough series on exterior algebra. Previously: [matrices]({{ site.baseurl }}{% post_url 2018-10-08-exterior-1 %}) and [inner products]({{ site.baseurl }}{% post_url 2018-10-09-exterior-2 %}) and [interior products]({{ site.baseurl }}{% post_url 2019-01-26-exterior-3 %}) on exterior algebras.*

*Vector spaces are assumed to be finite-dimensional and over $$\bb{R}$$. The grade of a multivector $$\alpha$$ will be written $$\| \alpha \|$$, while its magnitude will be written $$\Vert \alpha \Vert$$. Bold letters like $$\b{u}$$ will refer to (grade-1) vectors, while Greek letters like $$\alpha$$ refer to arbitrary multivectors with grade $$\| \alpha \|$$.*

*I think a disclaimer is starting to be necessary: I am not a mathematician and am basically in way over my head. I can't promise this is correct, but perhaps it is at least interesting.*

--------

## 1. Pause for intuition

We're about to add a fourth operation to our collection, and we're probably already overwhelmed. Fortunately, it turns out there's an overarching intuition that puts this all in a more manageable context.

You may have noticed that the wedge product on basis multivectors more-or-less appends them as lists (up to signs, which are 0 if they share a component): $$\b{wx} \^ \b{yz} = \b{wxyz}$$. This could also be interpreted as taking their union as sets. Either way, $$\^$$ seems to act like a union or concatenation operation combined with a bonus antisymmetrization step which has the effect of making duplicate terms like $$\b{x \^ x}$$ vanish.

<!--more-->

Suppose that some pure multivectors $$\alpha$$, $$\beta$$ are instead considered as just their underlying sets of basis vectors, in some particular basis (for example $$\b{x \^ y} \lra (\b{x, y})$$). Then:

$$\alpha \^ \beta= 
\begin{cases} 
\pm \alpha \cup \beta & \if \alpha \cap \beta = \emptyset \\
0 & \text{ otherwise} \end{cases}$$

And the interior product acts like set subtraction:

$$\alpha \cdot \beta= 
\begin{cases}
\pm \beta / \alpha & \if \alpha \subset \beta \\
0 & \text{ otherwise}
\end{cases}$$

It turns out that a lot of identities are consistent with this model; for example, $$(\alpha \^ \beta) \cdot \gamma = \beta \cdot (\alpha \cdot \gamma)$$ expresses the same thing, more or less, as the set algebra identity $$C \setminus (A \cup B) = (C \setminus A) \setminus B$$. These seem not to be coincidences: there turns out to be a sense in which exterior algebra is very nearly a "**linearized set algebra**". I have seen mentions of this in lots of places, but haven't managed to find a coherent exposition. Notably, from "On the Exterior Calculus of Invariant Theory" (Barnabei, Brini, Rota 1985):

>In the context of Peano spaces the theory of this operator acquires a crystalline simplicity, and the algebraic system obtained by meet, join and star operator is at last seen to be the much-sought-after linear analog of the Boolean algebra of sets with intersection, union and complement, an analogy which was first intuited by Grassmann and later pursued, with partial success, by Clifford, Peano, Burali-Forti and A. N. Whitehead. 

A more direct quote of Rota: "Philosophically, the exterior algebra is a “linearization” of the Boolean algebra of all subsets of $$\{1,2,...,n\}$$." - _Combinatorics: The Rota Way_". So the analogy is mentioned, but I haven't come across much more of an explanation. I thought I would investigate.

---

## 2. Set Algebra

Recall that a [boolean algebra](https://en.wikipedia.org/wiki/Boolean_algebra_(structure)) is a set $$A$$ equipped with two binary options $$\^$$ ("meet") and $$\vee$$ ("join"), a unary option $$\neg$$ ("complement") and two distinguished elements $$0$$ ("bottom") and $$1$$ ("top"), with these axioms for all $$a,b,c \in A$$:

$$\begin{aligned}
a \vee (b \vee c) &= (a \vee b) \vee c & a \^ (b \^ c) &= (a \^ b) \^ c) \\
a \vee b &= b \vee a & a \^ b &= b \^ a \\
a \vee (a \^ b) &= a & a \^ (a \vee b) &= a \\
a \^ 0 &= a & a \vee 1 &= a \\
a \vee (b \^ c) &= (a \vee b) \^ (a \vee c) & a \^ (b \vee c) &= (a \^ b) \vee (a \^ c) \\
a \vee \neg a &= 1 & a \^ \neg a &= 0 
\end{aligned}$$

The conventional algebra of $$(true, false)$$ is an example of this, where $$\^$$ is "and", $$\vee$$ is "or", $$\neg$$ is "not", and $$0 = false$$ and $$1 = true$$. The terminology of "meet" and "join" comes from [lattices](https://en.wikipedia.org/wiki/Lattice_(order)), of which this structure is an example.

[Set algebra](https://en.wikipedia.org/wiki/Algebra_of_sets) is an example of a boolean algebra on the subsets $$2^U$$ of a "universe" set $$U$$, with $$\vee$$ is $$\cup$$ (union), $$\^$$ is $$\cap$$ (intersection), $$\neg$$ is $$a \ra a^c$$ (complement with respect to $$U$$, ie $$a^c = U \setminus a$$), $$0$$ is $$\emptyset$$, and $$1$$ is $$U$$.

I claim that exterior algebra is related to set algebra. If we set $$\emptyset \ra 1$$, $$U \ra \omega$$, $$\cup \ra \^$$, and $$a^c \ra \star a$$, then these axioms are clearly _similar_:

$$\begin{aligned}
\alpha \^ (\beta \^ \gamma) &= (\alpha \^ \beta) \^ \gamma \\
\alpha \^ \beta &= - \beta \^ \gamma \\
\alpha \^ 1 &= \alpha \\
\alpha \^ \star \alpha &= \omega
\end{aligned}$$


Each of these looks a lot like an axiom up above. Here are a few other observations:

1. The wedge product of two pure multivectors $$\alpha \^ \beta$$ gives either the correct _union_ of their basis components, or $$0$$ in the case where there are duplicate vectors in each term, such as $$(\b{xy}) \cup (\b{yz}) = \b{xyz}$$.
2. The interior product of two pure multivectors $$\alpha \cdot \beta$$ gives either the correct _set subtraction_ of their basis components $$\beta \setminus \alpha$$, or $$0$$ in the case where every term in $$\alpha$$ is not present in $$\beta$$, such as in $$(\b{xy}) \setminus (\b{yz}) = (\b{x})$$.
3. The naive way of linearizing the union operation, by just enforcing bilinearity via $$a(\b{xy}) + b(\b{yz})) \cup (x) = a (\b{xy}) + b (\b{xyz})$$, is not well-behaved:
	* It is not a graded algebra (the cardinalities don't add)
	* As such, it cannot be made basis-invariant, because $$(c \b{x}) \cup (c \b{x}) = c^2 \b{x} \neq c \b{x}$$.
4. There are a few obvious ways to replace the union with a related operation that might make it into a graded algebra:
	* Concatenate _lists_, without removing duplicates (the tensor algebra)
		* This is probably inappropriate because it requires picking some ordering on the underlying sets, but it does keep all the information, which could be mapped to a union afterwards.
	* Concatenate sets, without removing duplicates (the symmetric algebra)
	* Concatenate sets, removing duplicates (the exterior algebra)


I think it is plausible that, in order to "linearize" the set algebra in a useful way, we have to adjust the operations so that they always respect grading on their arguments. The union must always add cardinalities; set subtraction must always subtract cardinalities; intersection must do... something. And any time this rule is violated, the result is $$0$$ instead of what the set operation would give. This is certainly not a homomorphism of the algebras formed by $$\cup$$ and $$\cap$$ (because $$\phi(A \cup B) \neq \phi(A) \^ \phi(B)$$ if they intersect), and shortly, when we discuss exterior algebra's version of 'intersection', it will turn out that the distributivity axioms of set algebra _do not_ carry over and must be weakened. Nevertheless, the intuition from this correspondence is helpful; usually we can think of multivectors as 'signed sets' and intuit what identities should hold from that fact, up to a sign or so.

Here's the glossary:

* Boolean algebra's **_Or_**, **_And_**, **_Not_** are
* Lattice theory's **_Join_**, **_Meet_**, **_Complement_**, which are
* Set algebra's **_Union_**, **_Intersection_**, and **_Complement_**, which are _related to_
* Exterior Algebra's **_Wedge_** or **Exterior Product**, **_Antiwedge_** (?), and **_Hodge Star_** (??)

(This is why I want to call the Hodge Star the 'complement'. Meanwhile, the exterior algebra version of intersection/meet doesn't have agood name at all -- it feels weird to call it the 'meet' without calling wedge products the 'join', but 'regressive product' (Grassmann's name) isn't great either, and "Antiwedge" sucks too. Overall "meet" and "join" seem like the best set of names. Maybe I'll use those in the future.)

In the process we realize the following confusing fact:

> The wedge product $$\^$$ has the wrong symbol, because it corresponds to $$\vee$$ ("or", "join") in boolean algebra and $$\cup$$ ("union") in the set algebra.

Plenty of authors do use $$\vee$$ for the exterior product already, particularly if they come to it from the perspective of geometry and lattice theory. Fortunately we can probably keep calling it the 'wedge' product either way.

Let's round out our theory with the exterior-algebra version of "intersection", which, for lack of a better name, we'll call the Meet.

-----------

## 3. The Meet

The **Meet**, denoted $$\vee$$ and pronounced 'vee' if you want, is the linearization of set intersection $$\cap$$. It's also occasionally called the "antiwedge" or the "regressive product" (because it reduces grades. these people also call $$\^$$ the 'progressive' product). It feels slightly strange to call it the 'meet' if we don't also call $$\^$$ the 'join', so let's try to start doing that. I'm capitalizing it because I find it hard to read otherwise, since it's such an ordinary word.

The Meet is the dual of the wedge product with respect to the complement $$\star$$:

$$\begin{aligned}
\alpha \vee \beta &= \star((\star \alpha) \^ (\star \beta)) \\
\alpha \^ \beta &= \star((\star \alpha) \vee (\star \beta)) \tag{1}
\end{aligned} $$

(Compare to De Morgan's laws in set algebra: $$(a \^ b)^c = a^c \vee b^c$$. We would write these as $$\star^{-1} (\alpha \^ \beta) = \star \alpha \vee \star \beta$$.)

This definition is quite useable. It means that any wedge product on vectors has a parallel equation on _pseudovectors_:

$$\begin{aligned}
(\star \b{x}) \vee (\star \b{y}) &= \star(\b{x \^ y})  \\
(\b{x}) \^ (\b{y}) &= \star((\star \b{x}) \vee (\star \b{y}))
\end{aligned}$$

In general, this is what the Meet does to grades:

$$\un{n-p}{\alpha} \vee \un{n-q}{\beta} = \underbrace{(\alpha \vee \beta)}_{n - p - q}$$

(It can be useful to think of these things as being 'graded modulo $$n$$': $$\un{-p}{\alpha} \vee \un{-q}{\beta} \equiv \underbrace{(\alpha \vee \beta)}_{-(p+q)}$$.[^grassmann])

[^grassmann]: Apparently Grassmann, who was the first to write about these operations, lumped $$\^$$ and $$\vee$$ together into one operation that basically acted on grades modulo $$n$$. Also-apparently, this did not work very well.

Because $$( \^ V, \vee)$$ is isomorphic (via $$\star$$) to the exterior algebra $$(\^V, \^)$$, it is itself an exterior algebra, except that the 'vectors' are the pseudovectors $$\^^{n-1} V$$ and the product is $$\vee$$. This algebra, which we call $$\vee V$$, has exactly the same relations as $$\^V$$, with $$\omega$$ as the identity and $$1 = \star \omega$$ as the 'pseudoscalar'. Taken together, the Join and Meet algebras are sometimes called the "Double Algebra" on $$V$$, which we might write as $$\^ {\vee} V$$.

When $$\| \alpha \| + \| \beta \| = n$$, the Meet is just a scalar product, and some authors use this to define the inner and interior products:[^trivia]

[^trivia]: The Barnabei/Brini/ Rota paper cited above gives a brief history: when Elie Cartan imported the exterior algebra into the theory of differential forms, he dropped the meet in favor of the interior product. They disagree with this on the grounds that it loses out on elegant structure, and prefer to define the interior and inner products in terms of $$\vee$$. Apparently this also performs better in settings where there is no canonical inner product.

$$\begin{aligned}
\un{k}{\alpha} \vee \un{n-k}{\beta} &= \star(\star \alpha \^ (\star \beta)) \\
&= \star \<\star \alpha, \beta\> \omega \\
&= \< \star \alpha, \beta \>
\end{aligned}$$

We can define the interior product and Meet in terms of each other:[^interior]

$$\begin{aligned}
\alpha \cdot \beta &= \star^2 \beta \vee (\star^{-1} \alpha) \\
\alpha \vee \beta &= (\star \beta) \cdot (\star^2 \alpha)
\end{aligned}$$

[^interior]:A more symmetric form for the first: $$\alpha \cdot (\star \beta) = \star^{-1} \beta \vee \star^{-1} \alpha$$. The duality (1) clearly means that $$\alpha \vee \beta = (-1)^{\| \star \alpha \| \| \star \beta \|} \beta \vee \alpha$$, which immediately leads to $$\alpha \cdot (\star \beta) = (-1)^{\| \star \alpha \| \| \star \beta \|} \beta \cdot (\star \alpha)$$, which is a much cleaner derivation of an identity from my previous post.

------

Unfortunately, Meets in general are... confusing. I find them to be very unintuitive compared to $$\^$$, outside of cases where the above duality is obvious. The problem is that the operation acts _almost_ like intersection, but is very dependent on the grade of the space you're working in. It's only nonzero when the grades of its arguments sum to $$\geq n$$, which is easily seen from the duality $$(1)$$:

$$\un{p}{\alpha} \vee \un{q}{\beta} = \begin{cases}
\underbrace{(\alpha \vee \beta)}_{p + q - n} & \if p + q \geq n \\
0 & \if p + q < n \end{cases}$$

As a result, Meets are mostly not useful as an intersection operation in practice, and an equation which gives a nonzero result in $$\^ \bb{R}^n$$ may give zero in $$\^ \bb{R}^{m > n}$$.

Here are some examples of Meets in $$\bb{R}^3$$:

$$\begin{aligned}
\b{x \vee y} &= \star ((\b{x \^ y}) \^ (\b{z\^x})) = 0 \\
\b{(x \^ y) \vee (y \^ z)} &= \star ( \b{z \^ x} ) = \b{y} \\
\b{x \vee (x \^ y)} &= \star(\b{y \^ z} \^ \b{z}) = 0 \\
\b{x \vee (y \^ z)} &= \star(\b{y \^ z \^ x}) = 1
\end{aligned}$$

In $$\bb{R}^4$$ with $$\omega = \b{w \^ x \^ y \^ z}$$:

$$\begin{aligned}
\b{(x \^ y) \vee (y \^ z)} &= \star(\b{z \^ w \^ w \^ x}) = 0 \\
\b{(w \^ x) \vee (y \^ z)} &= 1 \\
\b{(w \^ x \^ y) \vee (z)} &= 1
\end{aligned}$$

Notice that it's sort of computing intersections, but not very well. $$\b{(x \^ y) \vee (y \^ z)} = \b{y}$$ but $$\b{x \vee (x \^ y)} = 0$$? What are we supposed to do with that? The basis reason for this behavior is that, for the Meet to be nonzero, the result must contain an _entire copy of_ $$\omega$$. For instance, note that $$(\alpha) \vee (\star \alpha \^ \beta) \propto \beta$$: the meet removes the $$\alpha \^ (\star \alpha) \propto \omega$$ part, leaving a 'remainder' of $$\pm \beta$$.

------

## 3. Problems

Unfortunately, the 'exterior algebra as a linearization of set algebra' analogy is... not great.

A few reasons: we know that $$\alpha \^ \alpha = 0$$ unless $$\alpha \in \^^0 V$$, yet $$a \cup a = a$$. This means lots of identities don't work. We might try work around this by saying: well, identities from set algebra hold in exterior algebra _so long as both sides are nonzero_, which we could write as $$\stackrel{\neq 0}{=}$$. But it's still not true that $$\alpha \^ \alpha \stackrel{\neq 0}{=} \alpha$$ in any sense -- if it's a scalar it would be equal to $$\| \alpha \|^2 1$$; otherwise it would be equal to $$0$$.

We can sorta repair this by writing $$\alpha \^ \alpha \stackrel{\neq 0}{=} \| \alpha \|^2 \hat{\alpha}$$. But... that's not very good, aesthetically.

The main cause of failures is when the two sides of an equality have different numbers of _terms_. Because exterior algebra is coordinate-independent, any identity must have the same number of terms on both sides (because if we multiplied the whole space by $$c$$, any equation picks up a $$c^m$$ factor where $$m$$ is its grade). This immediately rules out a direct translation of the distributivity axioms of boolean algebra, and anything derived from them:

$$\begin{aligned}
\alpha \^ (\beta \vee \gamma) &\neq (\alpha \^ \beta) \vee (\alpha \^ \gamma) \\
\alpha \vee (\beta \^ \gamma) &\neq (\alpha \vee \beta) \^ (\alpha \vee \gamma)
\end{aligned}$$

Here maybe the solution is to include a second factor of $$\alpha$$ on the left (which still corresponds to a valid set-algebra identity):

$$\begin{aligned}
\alpha \^ \alpha \^ (\beta \vee \gamma) &\stackrel{\neq 0}{=} (\alpha \^ \beta) \vee (\alpha \^ \gamma) \\
\alpha \^ \alpha \vee (\beta \^ \gamma) &\stackrel{\neq 0}{=} (\alpha \vee \beta) \^ (\alpha \vee \gamma)
\end{aligned}$$

But basically overall we end up with this set of not-entirely-valid 'axioms' that don't really work, though they do look pleasantly similar to those of set-algebra (some of these are redundant and can be derived from the others; I'm just following Wikipedia's example here):

$$\begin{aligned}
(\alpha \^ \beta) \^ \gamma &= \alpha \^ (\beta \^ \gamma)  & (\alpha \vee \beta) \vee \gamma &= \alpha \vee (\beta \vee \gamma) \\
\alpha \^ \beta &= - \beta \^ \gamma & \alpha \vee \beta &= (-1)^{| \star \alpha | | \star \beta |} \beta \vee \alpha \\
\alpha \^ (\alpha \vee \beta) &\stackrel{\neq 0}{=}  \beta \alpha^2 & \alpha \vee (\alpha \^ \beta) &\stackrel{\neq 0}{=}  \beta \alpha^2 \\
\alpha \^ 1 &= \alpha & \alpha \vee \omega &= \star^2 \alpha \\
\alpha \^ \alpha \^ (\beta \vee \gamma) &\stackrel{\neq 0}{=}  (\alpha \vee \beta) \^ (\alpha \vee \gamma) 
& \alpha \vee \alpha \vee (\beta \^ \gamma) &\stackrel{\neq 0}{=} (\alpha \^ \beta) \vee (\alpha \^ \gamma)  \\
\alpha \^ (\star \alpha) &= |\alpha|^2 \omega & \alpha \vee (\star \alpha) &= | \alpha |^2
\end{aligned}$$

So this is clearly... not very good. Yet, I still find the set algebra analogy useful for intuition. It makes deducing which relationships between $$\^$$, $$\vee$$, and $$\cdot$$ should exist very easy.

-----------------

## 4. The Symmetric Difference / Geometric Product

By the way, since this is probably the right time to bring this up:

There is another subject called [Geometric Algebra](https://en.wikipedia.org/wiki/Geometric_algebra) which is closely related to exterior algebra, and probably more well-known outside of formal mathematics. It is basically the exterior algebra augmented with 1) a 'geometric product', denoted by juxtaposition $$ab$$, 2) like five other products and a dozen more operators, each more confusing than the last, and 3) a vocal community of amateurs with a tendency to long calculations of gibberish, reverence to esteemed ancient masters (Clifford, Grassmann...), and empty promises about its capabilities. Actually there is probably something good in there but it is hard to find it sifting through all the parts that aren't.[^disclaimer]

[^disclaimer]: It should be mentioned that this is just my opinion based only on reading things on the Internet and it is probably no more valid than any other opinion that comes from reading things on the Internet and that I am just an opinionated amateur myself.

The 'geometric product' can be understood in any of several ways:

1. It is like the wedge product, except that instead of like terms annihiliating the whole expression ($$\b{xy \^ yz} = 0$$), they just disappear: $$(\b{xy})(\b{yz}) = \b{xz}$$.
2. It is like the wedge product, in that it is defined via a quotient of the tensor algebra, with $$\b{x \o y} \sim - \b{y \o x}$$, but instead of $$\b{x \o x} \sim 0$$ we have $$\b{x \o x} \sim 1$$ for all basis unit vectors $$\b{x}$$, or, in non-Euclidean space $$\b{x \o x} \sim G(\b{x,x})$$.
3. It is the signed linearization of the set [symmetric difference](https://en.wikipedia.org/wiki/Symmetric_difference) operation $$A \ominus B = (A \cup B) \setminus (A \cap B)$$, although due to the annoying annihilating properties of $$\^$$ and $$\vee$$ you can't construct it via $$(\alpha \vee \beta) \cdot (\alpha \^ \beta)$$.

It is most efficiently computed similar to a wedge product, except that any adjacent copies of a basis vector disappear:

$$(\b{xyz})(\b{wxy}) = \b{xyzwxy} = - \b{xyyzwx} = - \b{zwxx} = -\b{zw}$$

Unfortunately this quickly leads to mixed-grade multivectors when you use it on vectors, which is useful if you want to generalize complex multiplication but tricky to interpret geometrically:

$$\b{uv} = (u_x \b{x} + u_y \b{y}) (v_x \b{x} + v_y \b{y}) = (u_x v_x + u_y v_y) + (u_x v_y - u_y v_x) \b{x \^ y}$$

GA also allows you to create a multiplicative inverse of any non-zero vector: $$\b{u}^{-1} = \frac{\b{u}}{\b{uu}}$$, which ends up working a lot like the inverse-like usage the interior product via $$\frac{\b{u}}{\| \b{u} \|^2} \cdot \b{v}$$. A standard way of writing identities in GA is to perform a geometric product which gives multiple terms, and then to extract (via 'grade-projection') only the part of the result that you care about -- for example, in projection, you multiply by $$\b{u}^{-1}$$, then extract, say, the scalar component. Often I think it would be easier to just use the operation you cared about in the first place but no one asked me.

Geometric Algebra's merit is that it is clearly useful because it directly generalizes complex and quaternion multiplication which are useful. What _isn't_ useful is the proliferation of notations and products that come with it. The standard way to start a paper about GA is to open with how useful and intuitive it is and how brilliant the old masters were and then to dive into calculations which are far from useful or intuitive, which doesn't make a very good impression[^ga]. I suspect the subject is seriously weakened by trying to deal with mixed-grade multivectors (elements of $$\bigoplus_k \^^k V$$) in general, which are rarely useful (they are sorta necessary for generalizing $$\bb{C}$$-multiplication... but it sure makes everything else messy. I think there must be a better way). Basically I think the geometric product is mostly not intuitive at all, while the wedge product often is.


[^ga]: One of my motivations for writing this series are to be _very concrete_ about the utility of these concepts, which means mostly avoiding the geometric product entirely, by focusing on finding clean derivations of well-known identities and not making lofty claims or jumping straight into trying to solve physics.

We'll probably talk more about geometric products later. I don't feel nearly ready to start using them for simple derivations (it would make them... not simple), but it is worth mentioning for how they fit in to the set-algebra picture. My suspicion is that they are very closed to a good idea, but not quite there, and I hope to figure out how to bridge the gap someday.

--------

I brought up analogy the set algebra to make everything simpler, but instead we got two new operations and no simplicity at all. Next time I'm going to try again.