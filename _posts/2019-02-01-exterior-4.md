---
layout: blog
title: "Exterior Algebra as Linearized Set Theory"
footnotes: true
math: true
aside: true
tags: notes
---

*Part 4 of an excessively thorough series on exterior algebra. Previously: [matrices]({{ site.baseurl }}{% post_url 2018-10-08-exterior-1 %}) and [inner products]({{ site.baseurl }}{% post_url 2018-10-09-exterior-2 %}) and [interior products]({{ site.baseurl }}{% post_url 2019-01-26-exterior-3 %}) on exterior algebras.*

*Vector spaces are assumed to be finite-dimensional and over $$\bb{R}$$. The grade of a multivector $$\alpha$$ will be written $$\| \alpha \|$$, while its magnitude will be written $$\Vert \alpha \Vert$$. Bold letters like $$\b{u}$$ will refer to (grade-1) vectors, while Greek letters like $$\alpha$$ refer to arbitrary multivectors with grade $$\| \alpha \|$$.*

*I think a disclaimer is starting to be necessary: I am not a mathematician and am basically in way over my head. I can't promise this is correct, but perhaps it is at least interesting.*

--------

## 1. Pause for intuition

We're about to add a fourth operation to our collection, and we're probably already overwhelmed. It's worth taking a second to figure out what we've been doing.

You may have noticed that the wedge product on basis multivectors more-or-less appends them as lists (up to signs, which are 0 if they share a component): $$\b{wx} \^ \b{yz} = \b{wxyz}$$. This could also be interpreted as taking their union as sets. Either way, $$\^$$ seems to act like a union or concatenation operation combined with a bonus antisymmetrization step which has the effect of making duplicate terms like $$\b{x \^ x}$$ vanish.

<!--more-->

Consider treating a single basis multivector $$\b{x} \^ \b{y}$$ as just an (unordered) set of basis vectors $$\{\b{x}, \b{y}\}$$. If $$\alpha$$, $$\beta$$ refer to their underlying sets:

$$\alpha \^ \beta= 
\begin{cases} 
\alpha \cup \beta & \if \alpha \cap \beta = \emptyset \\
0 & \text{ otherwise} \end{cases}$$

And the interior product acts like set subtraction:

$$\alpha \cdot \beta= 
\begin{cases}
\beta / \alpha & \if \alpha \subset \beta \\
0 & \text{ otherwise}
\end{cases}$$

These are not coincidences. There turns out to be a sense in which exterior algebra is literally **linearized set algebra**. I have seen mentions of this in lots of places. Notably, from "On the Exterior Calculus of Invariant Theory" (Barnabei, Brini, Rota 1985):

>In the context of Peano spaces the theory of this operator acquires a crystalline simplicity, and the algebraic system obtained by meet, join and star operator is at last seen to be the much-sought-after linear analog of the Boolean algebra of sets with intersection, union and complement, an analogy which was first intuited by Grassmann and later pursued, with partial success, by Clifford, Peano, Burali-Forti and A. N. Whitehead. 

A more direct quote of Rota: "Philosophically, the exterior algebra is a “linearization” of the Boolean algebra of all subsets of $$\{1,2,...,n\}$$." - _Combinatorics: The Rota Way_". So the analogy is mentioned, but I haven't managed to find a satisfactory elaboration of this idea. Particularly, it seems like Rota meant this _only_ philosophically, while as far as I can tell it's true _literally_. So here's my version.

---

## 2. Set Algebra


A [Set algebra](https://en.wikipedia.org/wiki/Algebra_of_sets) is a [boolean algebra](https://en.wikipedia.org/wiki/Boolean_algebra_(structure) on the subsets of a universe set $$U$$, with the operations of union, intersection, and complement (with respect to $$U$$, ie $$A^c = U \setminus A$$). This forms a boolean algebra where $$\emptyset = 0$$, $$U = 1$$, and the set operations correspond to and/or/not. Boolean algebras are themselves examples of [lattices](https://en.wikipedia.org/wiki/Lattice_(order)), and sometimes terminology is borrowed from those. I claim that exterior algebra is related to set algebra. Here's the glossary:

* Boolean algebra's **_Or_**, **_And_**, **_Not_** are
* Lattice theory's **_Join_**, **_Meet_**, **_Complement_**, which are
* Set algebra's **_Union_**, **_Intersection_**, and **_Complement_**, which are finally
* Exterior Algebra's **_Wedge_** or **Exterior Product**, **_Antiwedge_** (?), and **_Hodge Star_** (??)

(This is why I want to call the Hodge Star the 'complement'. Meanwhile, the exterior algebra version of intersection/meet doesn't have agood name at all -- it feels weird to call it the 'meet' without calling wedge products the 'join', but 'regressive product' (Grassmann's name) isn't great either, and "Antiwedge" sucks too. Overall "meet" and "join" seem like the best set of names. Maybe I'll use those in the future.)

So how do we make this correspondence?

I don't know how it to do it rigorously, but I will sketch things. Start with a few observations:

1. The wedge product of two pure multivectors $$\alpha \^ \beta$$ gives either the correct _union_ of their basis components, or $$0$$ in the case where there is are duplicate vectors in each term, such as $$(\b{xy}) \cup (\b{yz}) = \b{xyz}$$.
2. The interior product of two pure multivectors $$\alpha \cdot \beta$$ gives either the correct _set subtraction_ of their basis components $$\beta \setminus \alpha$$, or $$0$$ in the case where every term in $$\alpha$$ is not present in $$\beta$$, such as in $$(\b{xy}) \setminus (\b{yz}) = (\b{x})$$.
3. The naive way of linearizing the union operation, by just enforcing bilinearity via $$a(\b{xy}) + b(\b{yz})) \cup (x) = a (\b{xy}) + b (\b{xyz})$$, is not very well-behaved:
	* It is not a graded algebra (the cardinalities don't add)
	* As such, it cannot be made basis-independent, because $$(c \b{x}) \cup (c \b{x}) = c^2 \b{x} \neq c \b{x}$$.
4. There are a few obvious ways to replace the union with a related operation that might make it into a graded algebra:
	* Concatenate _lists_, without removing duplicates (the tensor algebra)
		* This is probably inappropriate because it requires picking some ordering on the underlying sets, but it does keep all the information, which could be mapped to a union afterwards.
	* Concatenate sets, without removing duplicates (the symmetric algebra)
	* Concatenate sets, removing duplicates (the exterior algebra)

I think it is plausible that, in order to linearize the set algebra, we have to adjust the operations so that they always respect grading on their arguments. The union must always add cardinalities; set subtraction must always subtract cardinalities; intersection must do... something. And anytime this rule is violated, the result is $$0$$ instead of what the set operation would give. This is certainly not a homomorphism of the algebras formed by $$\cup$$ and $$\cap$$ (because $$\phi(A \cup B) \neq \phi(A) \^ \phi(B)$$ if they intersect), but it's the best we can do.[^homomorphism][^zero]

[^homomorphism]: One way to make a homomorphism would be by making each term into a tuple $$(grade \in \bb{N}, scalar \in \bb{R}, set \in 2^V)$$, where $$(a, c_a, A)(b,c_b, B)$$ $$= (a+b, c_a c_b, A \cup B)$$ and $$\phi(a, c_a, A) = (c_a A) 1_{\| A \| = a}$$, ie the kernel of $$\phi$$ is any element whose cardinality doesn't match its grade. This repairs $$\phi(AB) = \phi(A) \phi(B)$$, but it's not going to work when we add other operations anyway so it's not very useful.

[^zero]:A different interpretation is that the operations always work as expected, but their coefficients just end up being $$0$$. For instance, $$\b{x} \^ \b{x} = 0 (\b{x \cup x}) = 0 \b{x}$$, and these 'phantom' elements are being dropped to us.

Consider trying to construct an associative "union algebra" on the set $$2^V$$. We start with the free algebra over $$2^V$$ (call it $$F(2^V)$$, which has a formal multiplication operation with no additional constraints (not even associativity). For example:

$$(a \b{y} + b \b{xy}) (c \b{z}) = ac (\b{yz}) + bc \b{(xy)(yz)}$$

Next, define a projection function $$\phi: F(2^V) \ra 2^V$$ which maps these abstract multiplications back into $$2^V$$, and modifies scalars accordingly. The function is essentially $$\phi(AB) = A \^ B$$, but it could be written like this:

$$\phi(AB) = \begin{cases} 
\sgn(AB) (A \cup B) & \if A \cap B = \emptyset \\
0 & \if A \cap B \neq \emptyset
\end{cases}$$

Where $$\sgn(AB)$$ is positive if $$[A_1, A_2, \ldots A_k, B_1, B_2, \ldots B_{n-k}]$$ is a positive permutation of $$[x_1, x_2, \ldots x_n]$$. This definition requires that we supply an orientation along with our vector space which gives the positive ordering of basis vectors. But, honestly, I don't particularly care about the precise definition, since it's not a homomorphism anyway. I am happy just using $$\phi(AB) = A \^ B$$ as our 'coefficient', defining things circularly.

In summary, we realize that:

**Realization 1:**

> The exterior algebra $$\^ V$$ is the linearization of set algebra in the universe of "basis vectors of $$V$$" (and this turns out be basis-independent).

**Realization 2:**

> The wedge product $$\^$$ has the wrong symbol, because it corresponds to $$\vee$$ ("or", "join") in boolean algebra and $$\cup$$ ("union") in the set algebra.

... Whoops. I'm going to keep using $$\^$$ for now, but let's agree to change that someday, okay? Plenty of authors do use $$\vee$$ for the exterior algebra product already, particularly if they come to it from the perspective of geometry and lattice theory. Fortunately we can probably keep calling it the 'wedge' product either way.

**Realization 3:**

> Any calculation on the set algebra corresponds to a calculation in the exterior algebra or equals 0. Any theorem in the set algebra either holds in the exterior algebra or has one side set to 0.

Caveat: since the exterior algebra includes, well, addition, it's always possible to add something to 0 and un-annihilate a side. The set algebra doesn't have anything that maps to this. Fortunately, linearity in every argument means that this isn't going to break the above rule. Both sides of a set algebra theorem will be a composition of operations which are each either valid or 0.

-----------


## 3. The Meet

The **Meet**, denoted $$\vee$$ and pronounced 'vee' if you want, is the linearization of set intersection $$\cap$$. It's also occasionally called the "antiwedge" or the "regressive product" (because it reduces grades. these people also call $$\^$$ the 'progressive' product). It feels slightly strange to call it the 'meet' if we don't also call $$\^$$ the 'join', so let's try to start doing that. Also, I find it hard to read about if it's not capitalized because it's such an ordinary word otherwise, so I'm going to do that:

The Meet is the dual of the wedge product with respect to the complement $$\star$$:

$$\begin{aligned}
\alpha \vee \beta &= \star((\star \alpha) \^ (\star \beta)) \\
\alpha \^ \beta &= \star((\star \alpha) \vee (\star \beta)) \tag{1}
\end{aligned} $$

I find this definition to be much more useful than the definition via set intersection. It means that parallel equations like these are possible:

$$\begin{aligned}
(\star \b{x}) \vee (\star \b{y}) &= \star(\b{x \^ y})  \\
(\b{x}) \^ (\b{y}) &= \star((\star \b{x}) \vee (\star \b{y}))
\end{aligned}$$

In general, this is what the Meet does to grades:

$$\un{n-p}{\alpha} \vee \un{n-q}{\beta} = \underbrace{(\alpha \vee \beta)}_{n - p - q}$$

It manages to be a graded algebra also, despite the fact that it does not directly add the grades of its arguments. In fact, the Meet operation creates a second exterior algebra, where the 'vectors' are the pseudovectors $$\^^{n-1} V$$ and the product is $$\vee$$. This algebra (call it $$\vee V$$) has exactly the same relations as $$\^V$$, with $$\omega$$ as the identity and $$1 = \star \omega$$ as the 'pseudoscalar'. Taken together, the Join and Meet algebras are sometimes called the "Double Algebra" on $$V$$, which we might write as $$\^ {\vee} V$$.

When $$\| \alpha \| + \| \beta \| = n$$, the Meet is just a scalar product, and some authors use this to define the inner and interior products:[^trivia]

[^trivia]: The Barnabei/Brini/ Rota paper cited above gives a brief history: when Elie Cartan imported the exterior algebra into the theory of differential forms, he dropped the meet in favor of the interior product. They disagree with this on the grounds that it loses out on elegant structure, and prefer to define the interior and inner products in terms of $$\vee$$. Apparently this also performs better in settings where there is no canonical inner product.

$$\begin{aligned}
\un{k}{\alpha} \vee \un{n-k}{\beta} &= \star(\star \alpha \^ (\star \beta)) \\
&= \star \<\star \alpha, \beta\> \omega \\
&= \< \star \alpha, \beta \>
\end{aligned}$$

Unfortunately, Meets in general are... confusing. I find them to be completely unintuitive compared to $$\^$$, outside of cases where the above duality is obvious. The problem is that it acts _almost_ like intersection, but is very dependent on the grade of the space you're working in. It's only nonzero when the grades of its arguments sum to $$\geq n$$, which is easily seen from the duality $$(1)$$:

$$\un{p}{\alpha} \vee \un{q}{\beta} = \begin{cases}
\underbrace{(\alpha \vee \beta)}_{p + q - n} & \if p + q \geq n \\
0 & \if p + q < n \end{cases}$$

As a result, Meets are mostly not useful as an intersection operation in practice, and an equation which gives a nonzero result in $$\^ \bb{R}^n$$ may give zero in $$\^ \bb{R}^{m > n}$$.

Here are some examples in $$\bb{R}^3$$:

$$\begin{aligned}
\b{x \vee y} &= \star ((\b{x \^ y}) \^ (\b{z\^x})) = 0 \\
\b{(x \^ y) \vee (y \^ z)} &= \star ( \b{z \^ x} ) = \b{y} \\
\b{x \vee (x \^ y)} &= \star(\b{y \^ z} \^ \b{z}) = 0 \\
\b{x \vee (y \^ z)} &= \star(\b{y \^ z \^ x}) = 1
\end{aligned}$$

In $$\bb{R}^4$$ with $$\omega = \b{w \^ x \^ y \^ z}$$:

$$\begin{aligned}
\b{(x \^ y) \vee (y \^ z)} &= \star(\b{z \^ w \^ w \^ x}) = 0 \\
\b{(w \^ x) \vee (y \^ z)} &= 1 \\
\b{(w \^ x \^ y) \vee (z)} &= 1
\end{aligned}$$

Notice that it's sort of computing intersections, but not very well. $$\b{(x \^ y) \vee (y \^ z)} = \b{y}$$ but $$\b{x \vee (x \^ y)} = 0$$? What are we supposed to do with that? The best explanation I can give is that it works like this (in $$\bb{R}^3$$ for example):

1. To compute $$\alpha \vee \beta$$ (for instance $$\b{(x \^ y) \vee (y \^ z)}$$),
2. First compute $$\alpha \^ \beta$$ but without dropping any duplicate terms ($$=\b{x \^ y \^ y \^ z}$$)
3. Then, if you can, delete a whole copy of $$\omega = \b{x \^ y \^ z}$$ (giving $$ \alpha \^ \beta = \b{y}$$)
4. Otherwise the result is $$0$$.

This shows that $$\^$$ and $$\vee$$ are actually pretty similar. Basically: to find the set that has the parts of both $$\alpha$$ and $$\beta$$ removed, concatenate them and then see which parts are missing from $$\omega \^ \omega$$. Apparently Grassmann, the discoverer of this stuff, used them as one combined operation that did both operations at once (and adds grades mod $$n$$). I'm not sure how I feel about that yet.

Anyway, we could carefully study its properties ($$\alpha \vee \beta = (-1)^{\| \star \alpha \| \| \star \beta \|} \beta \vee \alpha$$, that sort of thing), but I don't really like this operation, so let's move on.


-----------

## 3. Identities

Since exterior algebra is basically set algebra, set algebra theorems are exterior algebra theorems, more or less. And look, there's a giant list of those [right here](https://en.wikipedia.org/wiki/Algebra_of_sets).

| Set Algebra | Exterior Algebra | notes |
|------------|----------------|---------:|
| $$\emptyset$$ | $$1$$ |  |
| $$U$$ | $$\omega$$ | |
| $$ A \cup B$$ | $$ \alpha \^ \beta$$, or $$0$$ if $$\alpha \cap \beta \neq \empty$$ | union |
| $$ A \cap B$$ | $$\alpha \vee \beta$$, or $$0$$ if $$\alpha^c \cap \beta^c \neq \empty$$ | intersection|
| $$ A \setminus B$$ | $$\beta \cdot \alpha$$, or $$0$$ if $$\alpha^c \cap \beta \neq \empty$$ | subtraction |
| $$A^c = U \setminus C $$ | $$\star \alpha = \alpha \cdot \omega$$ | complement |

And now we just start copying them over. The way this works is: every rule of set theory has to carry over (sometimes with a coefficient) to exterior algebra, but sometimes things end up with a coefficient of $$0$$, which breaks some of the equalities. Particularly, formulas equal $$0$$ whenever the grades of the results would be different than what the operations require. For example: the wedge $$\alpha \^ \beta$$ is $$0$$ if the cardinality of the result is not $$\| \alpha \| + \| \beta \|$$. (It's possible for formulas to match grades via, say, wedging and then contracting, whereupon the result might be 0 anyway.)

Equalities which are only true when neither side is $$0$$ are marked with $$\stackrel{?}{=}$$. These took a lot of checking, so I am not sure the signs are right everywhere, and on a few of them I stopped short of filling everything out for space and my sanity. I'm sure there must be a more robust method, but I haven't figured it out yet.

| Set Algebra | Exterior Algebra | notes |
|------------|----------------|---------:|
| $$A \cup B = B \cup A$$ | $$\alpha \^ \beta = (-1)^{\| \alpha \| \|\beta \|} \beta \^ \alpha$$ | commutativity |
| $$A \cap B = B \cap A$$ | $$\alpha \vee \beta = (-1)^{\| \star \alpha \| \|\star \beta \|} \beta \vee \alpha$$ |
| $$A \cup (B \cup C) = (A \cup B) \cup C$$ | $$\alpha \^ (\beta \^ \gamma) = (\alpha \^ \beta) \^ \gamma$$ | associativity |
| $$A \cap (B \cap C) = (A \cap B) \cap C$$ | $$\alpha \vee (\beta \vee \gamma) = (\alpha \vee \beta) \vee \gamma$$ | 
| $$A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$$ | $$\alpha \^ (\beta \vee \gamma) \stackrel{?}{=} (\alpha \^ \beta) \vee (\alpha \^ \gamma)$$ | distributivity |
| $$A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$$ | $$\alpha \vee (\beta \^ \gamma) \stackrel{?}{=} (\alpha \vee \beta) \^ (\alpha \vee \gamma)$$ | 
|----
| $$(A^c)^c = A$$ | $$\star^2 \alpha = (-1)^{\| \alpha \| \| \star \alpha \|} \alpha $$ | |
| | $$\star^4 \alpha = \alpha$$ | |
| $$A \cup \emptyset = A$$ | $$\alpha \^ 1 = \alpha$$ | identities |
| $$A \cap U = A$$ | $$\alpha \vee \omega = \star^2 \alpha$$ | |
| $$A \cup A^c = U$$ | $$\alpha \^ (\star \alpha) = \Vert \alpha \Vert^2 \omega$$ | complements |
| $$A \cap A^c = \emptyset$$ | $$\alpha \vee (\star \alpha) = \Vert \alpha \Vert^2 1 $$ |
|----
| $$ A \cup A = A$$ | $$\alpha \^ \alpha = 0$$ | 
| $$A \cap A = A$$ | $$\alpha \vee \alpha = 0$$ |
| $$A \cup U = U$$ | $$\alpha \^ \omega \stackrel{?}{=} \Vert \alpha \Vert \omega$$ | $$0$$ if $$\alpha \notin \^^0 V$$ |
| $$A \cap \empty = \emptyset$$ | $$\alpha \vee 1 \stackrel{?}{=} \Vert \alpha \Vert 1$$ | $$0$$ if $$\alpha \notin \^^n V$$ |
| $$\empty^c = U, U^c = \empty$$ | $$\star 1 = \omega$$, $$\star \omega = 1$$ | |
| $$A \cup (A \cap B) = A$$ | $$\alpha \^ (\alpha \vee \beta) \stackrel{?}{=} \alpha $$| absorption|
| $$A \cap (A \cup B) = A$$ | $$\alpha \vee (\alpha \^ \beta) \stackrel{?}{=} \alpha $$| |
| $$A \cap B = A \setminus (A \setminus B)$$ | $$ \alpha \vee \beta \stackrel{?}{=} (\beta \cdot \alpha) \cdot \alpha $$ | |
|----
| $$(A \cup B)^c = A^c \cap B^c$$ | $$\star(\alpha \^ \beta) = (\star^{-1} \alpha) \vee (\star^{-1} \beta)$$ | De Morgan's |
| $$(A \cap B)^c = A^c \cup B^c$$ | $$\star(\alpha \vee \beta) = (\star^{-1} \alpha) \^ (\star^{-1} \beta)$$ |  |
| $$A \cap B = (A^c \cup B^c)^c$$ | $$\alpha \vee \beta = \star((\star \alpha) \^ (\star \beta))$$ | (cleaner way) |
| $$A \cup B = (A^c \cap B^c)^c$$ | $$\alpha \^ \beta = \star((\star \alpha) \vee (\star \beta))$$ |
|----
| $$C \setminus (A \cup B) = (C \setminus A) \cap (C \setminus B)$$ | $$(\alpha \^ \beta) \cdot \gamma \stackrel{?}{=} (\alpha \cdot \gamma) \vee (\beta \cdot \gamma)$$ | subtraction examples |
| $$C \setminus (A \cap B) = (C \setminus A) \cup (C \setminus B)$$ | $$(\alpha \vee \beta) \cdot \gamma \stackrel{?}{=} (\alpha \cdot \gamma) \^ (\beta \cdot \gamma)$$ | |
| $$C \setminus (A \setminus B) = (A \cap C) \cup (B \cap C)$$ | $$(\beta \cdot \alpha) \cdot \gamma \stackrel{?}{=} (\alpha \vee \gamma) \^ (\beta \vee \gamma) $$| |
| $$(B \setminus A) \cap C = (B \cap C) \setminus A$$  | $$(\star^2 (\alpha \cdot \beta)) \vee \gamma = {\star(\star^{-1} \beta \^ \alpha \^ \star \gamma)}$$ | (something like this) |
| $$= B \cap (C \setminus A)$$ | $$\propto \alpha \cdot (\beta \vee \gamma) \propto \beta \vee (\alpha \cdot \gamma) $$ | |
| $$(B \setminus A) \cup C  = (B \cup C) \setminus (A \setminus C)$$ | $$(\alpha \cdot \beta) \^ \gamma \stackrel{?}{=} (\gamma \cdot \alpha) \cdot (\beta \^ \gamma)$$ | |
|----
| $$A \setminus A = \emptyset$$ | $$\alpha \cdot \alpha = \Vert \alpha \Vert^2$$ | |
| $$\emptyset \setminus A = \emptyset$$ | $$\alpha \cdot 1 \stackrel{?}{=} 1$$ | only if $$\alpha \in \^^0 V$$ |
| $$A \setminus \emptyset = A $$ | $$1 \cdot \alpha = \alpha$$ | |
| $$B \setminus A = A^c \cap B$$ | $$\alpha \cdot \beta = \star^2 \beta \vee (\star^{-1} \alpha)$$ |  |
| | $$\alpha \vee \beta = (\star \beta) \cdot (\star^2 \alpha) $$ | (useful version) |
| $$(B \setminus A)^c = A \cup B^c$$ | $$\star (\alpha \cdot \beta) = \star \beta \^ \star^2 \alpha$$ |  |
| $$A^c \setminus B^c = B \setminus A$$ | $$(\star \beta) \cdot (\star \alpha) = (-1)^{\| \alpha \| \| \star \beta \|} \alpha \cdot \star^2 \beta$$ |  |
| $$U \setminus A = A^c$$ | $$\alpha \cdot \omega = \star \alpha$$ | | 
| $$A \setminus U = \empty$$ | $$\omega \cdot \alpha \stackrel{?}{=} 1$$ | only if $$\alpha \in \^^n V$$ |

--------

This is a lot to take in. Disclaimer: there were a lot of error-prone calculations in making this chart, so it might change occasionally. I have been spending a lot of time lately trying to find a more robust way of figuring these out that isn't so manual.

I want to highlight the translations between Meets and interior products:

$$\begin{aligned}
\alpha \cdot \beta &= \star^2 \beta \vee (\star^{-1} \alpha) \\
\alpha \vee \beta &= (\star \beta) \cdot (\star^2 \alpha)
\end{aligned}$$

They're clunky, but it's important to note that the two operations can be defined terms of each other. In many ways, the Meet is better behaved (being even close to symmetric in its arguments). I just wish the signs weren't so clunky.

My main takeaway from this this correspondence exists, but also kinda sucks. There must be a more precise way to make this work that removes all the difficulty of checking these by hand, and possibly eliminates or otherwise copes with the $$\stackrel{?}{=}$$s.

This is making me wish I had a mathematician's expertise in abstract algebra. I suspect I'm in way over my head.